<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Find a dataset Find a corpus of text in language you prefer.
 Such as OSCAR  Intuitively, the more data you can get to pretrain on, the better results you will get."><meta property="og:title" content="Train LLM from scratch"><meta property="og:description" content="Find a dataset Find a corpus of text in language you prefer.
 Such as OSCAR  Intuitively, the more data you can get to pretrain on, the better results you will get."><meta property="og:type" content="website"><meta property="og:image" content="https://pinktalk.online/icon.png"><meta property="og:url" content="https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/train_LLM/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="Train LLM from scratch"><meta name=twitter:description content="Find a dataset Find a corpus of text in language you prefer.
 Such as OSCAR  Intuitively, the more data you can get to pretrain on, the better results you will get."><meta name=twitter:image content="https://pinktalk.online/icon.png"><meta name=twitter:site content="PinkR1ver"><title>Train LLM from scratch</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://pinktalk.online//icon.png><link href=https://pinktalk.online/styles.f342c0370dcc71a4a5d1fac5b401c180.min.css rel=stylesheet><link href=https://pinktalk.online/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://pinktalk.online/js/darkmode.4b1d856da524d844cfa5256b4795119b.min.js></script>
<script src=https://pinktalk.online/js/util.a0ccf91e1937fe761a74da4946452710.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script async src=https://pinktalk.online/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://pinktalk.online/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://pinktalk.online/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://pinktalk.online/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://pinktalk.online/",fetchData=Promise.all([fetch("https://pinktalk.online/indices/linkIndex.69fe3d62efe7fb02e4cac601f89fd506.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://pinktalk.online/indices/contentIndex.42fbaafcb5c0902ec139cad2450b4d0e.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://pinktalk.online",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://pinktalk.online",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:2,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'‚Äô':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/pinktalk.online\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=pinktalk.online src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://pinktalk.online/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://pinktalk.online/>üé£JudeW's Digital Garden</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Train LLM from scratch</h1><p class=meta>Last updated
Sep 4, 2023
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/train_LLM.md rel=noopener>Edit Source</a></p><ul class=tags><li><a href=https://pinktalk.online/tags/LLM/>Llm</a></li><li><a href=https://pinktalk.online/tags/LLM-training-method/>Llm training method</a></li><li><a href=https://pinktalk.online/tags/deep-learning/>Deep learning</a></li></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#find-a-dataset>Find a dataset</a></li><li><a href=#train-a-tokenizer>Train a tokenizer</a><ol><li><a href=#tokenization>Tokenization</a><ol><li><a href=#subword-tokenization-algorithm>Subword Tokenization Algorithm</a></li></ol></li><li><a href=#word-embedding>Word embedding</a></li></ol></li><li><a href=#train-a-language-model-from-scratch>Train a language model from scratch</a><ol><li><a href=#language-model-definition>Language model definition</a></li><li><a href=#language-model-algorithm>Language model algorithm</a><ol><li><a href=#classical-lm>Classical LM</a></li><li><a href=#cutting-edge>Cutting-edge</a></li></ol></li><li><a href=#train-method>Train Method</a><ol><li><a href=#bert-like-model>BERT-Like model</a></li></ol></li></ol></li><li><a href=#check-lm-actually-trained>Check LM actually trained</a><ol><li><a href=#take-bert-as-example>Take BERT as example</a></li></ol></li><li><a href=#fine-tune-our-lm-on-a-downstream-task>Fine-tune our LM on a downstream task</a></li><li><a href=#example>Example</a></li><li><a href=#reference>Reference</a></li></ol></nav></details></aside><a href=#find-a-dataset><h1 id=find-a-dataset><span class=hanchor arialabel=Anchor># </span>Find a dataset</h1></a><p>Find a corpus of text in language you prefer.</p><ul><li>Such as
<a href=https://oscar-project.org/ rel=noopener>OSCAR</a></li></ul><p>Intuitively, the more data you can get to pretrain on, the better results you will get.</p><a href=#train-a-tokenizer><h1 id=train-a-tokenizer><span class=hanchor arialabel=Anchor># </span>Train a tokenizer</h1></a><p>There are something you need take into consideration when train a tokenizer</p><a href=#tokenization><h2 id=tokenization><span class=hanchor arialabel=Anchor># </span>Tokenization</h2></a><p>You can read more detailed post -
<a href=/computer_sci/Deep_Learning_And_Machine_Learning/NLP/basic/tokenization/ rel=noopener class=internal-link data-src=/computer_sci/Deep_Learning_And_Machine_Learning/NLP/basic/tokenization/>Tokenization</a></p><p>Tokenization is the process of <strong>breaking text into words of sentences</strong>. These tokens helps machine to learn context of the text. This helps in <em>interpreting the meaning behind the text</em>. Hence, tokenization is <em>the first and foremost process while working on the text</em>. Once the tokenization is performed on the corpus, the resulted tokens can be used to prepare vocabulary which can be used for further steps to train the model.</p><p>Example:</p><p>‚ÄúThe city is on the river bank‚Äù -> ‚ÄúThe‚Äù, ‚Äùcity‚Äù, ‚Äùis‚Äù, ‚Äùon‚Äù, ‚Äùthe‚Äù, ‚Äùriver‚Äù, ‚Äùbank‚Äù</p><p>Here are some typical tokenization:</p><ul><li>Word ( White Space ) Tokenization</li><li>Character Tokenization</li><li><strong>Subword Tokenization (SOTA)</strong></li></ul><p>Subword Tokenization can handle OOV(Out Of Vocabulary) problem effectively.</p><a href=#subword-tokenization-algorithm><h3 id=subword-tokenization-algorithm><span class=hanchor arialabel=Anchor># </span>Subword Tokenization Algorithm</h3></a><ul><li><strong>Byte pair encoding</strong> <em>(BPE)</em></li><li><strong>Byte-level byte pair encoding</strong></li><li><strong>WordPiece</strong></li><li><strong>unigram</strong></li><li><strong>SentencePiece</strong></li></ul><a href=#word-embedding><h2 id=word-embedding><span class=hanchor arialabel=Anchor># </span>Word embedding</h2></a><p>After tokenization, we make our text into token. We also wants to present token in math type. Here we use word embedding technique, converting word to math.</p><p>Here are some typical word embedding algorithms:</p><ul><li><strong>Word2Vec</strong><ul><li>skip-gram</li><li>continuous bag-of-words (CBOW)</li></ul></li><li><strong>GloVe</strong> (Global Vectors for Word Representations)</li><li><strong>FastText</strong></li><li><strong>ELMo</strong> (Embeddings from Language Models)</li><li><strong>BERT</strong> (Bidirectional Encoder Representations from Transformers)<ul><li>a language model rather than a traditional word embedding algorithm. <strong>While BERT does generate word embeddings as a byproduct of its training process</strong>, its primary purpose is to learn contextualized representations of words and text segments.</li></ul></li></ul><a href=#train-a-language-model-from-scratch><h1 id=train-a-language-model-from-scratch><span class=hanchor arialabel=Anchor># </span>Train a language model from scratch</h1></a><p>We need clear the definition of language model.</p><a href=#language-model-definition><h2 id=language-model-definition><span class=hanchor arialabel=Anchor># </span>Language model definition</h2></a><p>Simply to say, the language model is a computational model or algorithm that is designed to understand and generate human language. It is a type of artificial intelligence(AI) model that uses <em>statistical and probabilistic techniques to predict and generate sequences of words and sentences</em>.</p><p>It captures the statistical relationships between words or characters and <em>builds a probability distribution of the likelihood of a particular word or sequence of words appearing in a given context.</em></p><p>Language model can be used for various NLP tasks, including machine translation, speech recognition, text generation and so on&mldr;.</p><p>As usual, a language model takes a seed input or prompt and uses its <em>learned knowledge of language(model weights)</em> to predict most likely words or characters to follow.</p><p>The SOTA of language model today is GPT-4.</p><a href=#language-model-algorithm><h2 id=language-model-algorithm><span class=hanchor arialabel=Anchor># </span>Language model algorithm</h2></a><a href=#classical-lm><h3 id=classical-lm><span class=hanchor arialabel=Anchor># </span>Classical LM</h3></a><ul><li><strong>n-gram</strong><ul><li>N-gram can be used as <em>both a tokenization algorithm and a component of a language model</em>. In my searching experience, n-grams are easier to understand as a language model to predict a likelihood distribution.</li></ul></li><li><strong>HMMs</strong> (Hidden Markov Models)</li><li><strong>RNNs</strong> (Recurrent Neural Networks)</li></ul><a href=#cutting-edge><h3 id=cutting-edge><span class=hanchor arialabel=Anchor># </span>Cutting-edge</h3></a><ul><li><strong>GPT</strong> (Generative Pre-trained Transformer)</li><li><strong>BERT</strong> (Bidirectional Encoder Representations from Transformers)</li><li><strong>T5</strong> (Text-To-Text Transfer Transformer)</li><li><strong>Megatron-LM</strong></li></ul><a href=#train-method><h2 id=train-method><span class=hanchor arialabel=Anchor># </span>Train Method</h2></a><p>Different designed models usually have different training methods. Here we take BERT-like model as example.</p><a href=#bert-like-model><h3 id=bert-like-model><span class=hanchor arialabel=Anchor># </span>BERT-Like model</h3></a><p><img src=https://pinktalk.online//computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/attachments/Pasted%20image%2020230629104307.png width=auto alt></p><p>To train BERT-Like model, we&rsquo;ll train it on a task of <strong>Masked Language Modeling</strong>(MLM), i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset.</p><p>Also, we&rsquo;ll train BERT-Like model using <strong>Next Sentence Prediction</strong> (NSP). <em>MLM teaches BERT to understand relationships between words and NSP teaches BERT to understand long-term dependencies across sentences.</em> In NSP training, give BERT two sentences, A and B, then BERT will determine B is A&rsquo;s next sentence or not, i.e. outputting <code>IsNextSentence</code> or <code>NotNextSentence</code></p><p>With NSP training, BERT will have better performance.</p><table><thead><tr><th>Task</th><th>MNLI-m (acc)</th><th>QNLI (acc)</th><th>MRPC (acc)</th><th>SST-2 (acc)</th><th>SQuAD (f1)</th></tr></thead><tbody><tr><td>With NSP</td><td>84.4</td><td>88.4</td><td>86.7</td><td>92.7</td><td>88.5</td></tr><tr><td>Without NSP</td><td>83.9</td><td>84.9</td><td>86.5</td><td>92.6</td><td>87.9</td></tr></tbody></table><p><a href=https://arxiv.org/pdf/1810.04805.pdf rel=noopener>Table source</a>
<a href=/computer_sci/Deep_Learning_And_Machine_Learning/LLM/metircs/some_task/ rel=noopener class=internal-link data-src=/computer_sci/Deep_Learning_And_Machine_Learning/LLM/metircs/some_task/>Table metrics explain</a></p><a href=#check-lm-actually-trained><h1 id=check-lm-actually-trained><span class=hanchor arialabel=Anchor># </span>Check LM actually trained</h1></a><a href=#take-bert-as-example><h2 id=take-bert-as-example><span class=hanchor arialabel=Anchor># </span>Take BERT as example</h2></a><p>Aside from looking at the training and eval losses going down, we can check our model using <code>FillMaskPipeline</code>.</p><p>This is a method input <em>a masked token (here,¬†<code>&lt;mask></code>) and return a list of the most probable filled sequences, with their probabilities.</em></p><p>With this method, we can see our LM captures more semantic knowledge or even some sort of (statistical) common sense reasoning.</p><a href=#fine-tune-our-lm-on-a-downstream-task><h1 id=fine-tune-our-lm-on-a-downstream-task><span class=hanchor arialabel=Anchor># </span>Fine-tune our LM on a downstream task</h1></a><p>Finally, we can fine-tune our LM on a downstream task such as translation, chatbot, text generation and so on.</p><p>Different downstream task may need different methods to do fine-tune.</p><a href=#example><h1 id=example><span class=hanchor arialabel=Anchor># </span>Example</h1></a><p><a href="https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb#scrollTo=G-kkz81OY6xH" rel=noopener>https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb#scrollTo=G-kkz81OY6xH</a></p><a href=#reference><h1 id=reference><span class=hanchor arialabel=Anchor># </span>Reference</h1></a><ul><li><a href=https://huggingface.co/blog/how-to-train rel=noopener>HuggingFace blog, How to train a new language model from scratch using Transformers and Tokenizers</a></li><li><a href=https://medium.com/nerd-for-tech/nlp-tokenization-2fdec7536d17 rel=noopener>Medium blog, NLP Tokenization</a></li><li><a href=https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf rel=noopener>Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. (2018). Improving language understanding by generative pre-training.¬†, .</a></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/computer_sci/Deep_Learning_And_Machine_Learning/LLM/LLM_MOC/ data-ctx="‚≠ê‚≠ê‚≠êTrain LLM from scratch" data-src=/computer_sci/Deep_Learning_And_Machine_Learning/LLM/LLM_MOC class=internal-link>Large Language Model(LLM) - MOC</a></li><li><a href=/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/RLHF/ data-ctx="Pretraining language models" data-src=/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/RLHF class=internal-link>Reinforcement Learning from Human Feedback</a></li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://pinktalk.online/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by JudeW using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, ¬© 2023</p><ul><li><a href=https://pinktalk.online/>Home</a></li><li><a href=https://twitter.com/PinkR1ver>Twitter</a></li><li><a href=https://github.com/PinkR1ver>GitHub</a></li><li><a href=https://www.instagram.com/jude.wang.yc/>Instagram</a></li></ul></footer></div></div></body></html>