{"/":{"title":"Home","content":"\n🕵️‍♂️ This is Jude Wang's vault about his notebook, his knowledge, his second brain. \n\n☁️ Here to find his notes:\n\n* [📒Notes](atlas.md)\n\n🎦 Here to find what he watch and read recently:\n\n* [🎬Watching List](https://pinkr1ver.notion.site/5e136466f3664ff1aaaa75b85446e5b4?v=a41efbce52a84f7aa89d8f649f4620f6\u0026pvs=4)\n\n🏔 To know more about me:\n\n* [🍉Resume](resume.md)\n\n","lastmodified":"2023-09-18T02:45:48.121253148Z","tags":null},"/%E6%96%87%E5%AD%A6/%E5%8F%A5%E5%AD%90/%E5%8F%A5%E5%AD%90":{"title":"句子","content":"\n* [🌸Love about](文学/句子/Love_about.md)\n* [🌄Poem](文学/句子/Poem.md)\n* [🧗🏻‍♂️Motivation](文学/句子/Motivation.md)\n* [🧶Feeling](文学/句子/Feeling.md)\n* [📽Movie](文学/句子/Movie.md)\n* [🎹Novel](文学/句子/Novel.md)\n* [🥐Comments](文学/句子/Comments.md)\n* [🎺Music](文学/句子/Music.md)\n* [🧙‍♂️Wisdom](文学/句子/Wisdom.md)\n","lastmodified":"2023-09-18T02:45:48.465257122Z","tags":null},"/%E6%96%87%E5%AD%A6/%E5%8F%A5%E5%AD%90/Comments":{"title":"🥐Comments","content":"\n  \u003cstyle\u003e\n    p {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n\n    a {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n  \u003c/style\u003e\n\n\u003e [!quote] \n\u003e  From comments\n\u003e  \n\u003e   被一些影评人的高度评价给诈骗到了;看来不同人对浪漫的定义非常不同，对于有些人来说“宇宙”“存在”等词以及语焉不详的现代诗歌排列组合在一起即可触发内心浪漫情结，就跟大学生会用夏天、自由、苏打、快乐with黄油相机滤镜加字加字照片来营造自觉出众的氛围感朋友圈一样。女儿的线也太刻奇，套了一个寻找外星人的噱头、还有伪纪录片的形式，镜头的设计还有手持的感觉在大荧幕上显得非常粗糙，之前很喜欢导演那个《法制未来时》的短片,结果电影有种加长版视频的感觉，还是感觉撑不起来啊... ...\n\n\n--- \n\n\n \u003e [!quote] \n\u003e From CC98, credits to someone\n\u003e \n\u003e \n\u003e感觉你女朋友是那种 初中喜欢混混 高中喜欢体育生 军训喜欢教官 大学喜欢rapper 打工爱上领导 理发爱上托尼 看病爱上医生 离婚爱上律师的人\n\n\n--- \n\n\n\n\u003e [!quote] \n\u003e From a video talking about 坂本龙一 by [HOPICO](https://www.bilibili.com/video/BV1pa4y1T7v2/?spm_id_from=333.1007.top_right_bar_window_history.content.click\u0026vd_source=c47136abc78922800b17d6ce79d6e19f) \n\u003e \n\u003e 1. 脱下合成器的修饰之后，这首作品变成了一个最赤裸的样子。我们听过印象深刻的钢琴曲，我们热爱他们，我们在形容它们的时候，多数是“热情”、“悲伤”、“欢乐”、“雄伟”。但这首歌不一样，之于我第一次听到它的时候，我再想为什么会有一首歌，那么精确地，在开头把“安静”这两个字讲了出来，明明无声的真空才是最安静，可这几个音符勾勒出来的安静，就是胜过了无声的真空。我想，那是因为我们在这几个音符背后，仿佛能够看到一个作曲家，找到他的钢琴，好像全世界只剩下他们的样子。\n\u003e    \n\u003e    要说幸运的是，我亲眼见过教授带着管弦乐团的全编制演奏这首作品。虽然我内心一直觉得，这是一首寂寞的作品，我心中它最好的样子，就是保持在三人编制以下，但是我清楚记得，那天在现场听到最后一个段落时，我眼泪止不住地往下流，我还记得我当时心里想的是：“md，这么多年了，终于听到了”\n\u003e    \n\u003e    就像教授在后来采访里说的，它可能不是首好的电影配乐，因为它不需要画面就已经自成一体了。可是重新听到它地时候，仍然意气风发。\n\u003e    \n\u003e    \u003cp style=\"text-align: right\"\u003e——HOPICO 评价 「\u003ca href=\"https://music.apple.com/cn/album/merry-christmas-mr-lawrence-coda/1404842855?i=1404843053\"\u003eMerry Christmas, Mr. Lawrence\u003c/a\u003e」\u003c/p\u003e\n\u003e    \n\u003e 2. 坂本龙一小的时候很喜欢Beatles，甚至一度以为只有了解Beatles的人才能和自己做朋友。而「末代皇帝」当中，许多作品的录制，都是在abbey road 2号录影棚完成的，这里是The Beatles 1962年到1969年录音的地方。教授也通过这样的方式，和自己的偶像有了*重合*。\n\u003e    \n\u003e    而在这里录制的作品，就包括整部原声带当中我最喜欢的「Where is Armo?」，在这首歌的写作里，他构建出了一种徐徐前行，奔赴宿命的坦荡。\n\u003e    \n\u003e   \u003cp style=\"text-align: right\"\u003e——HOPICO 评价 「\u003ca href=\"https://music.apple.com/cn/album/where-is-armo/714659119?i=714659278\"\u003eWhere is Armo?\u003c/a\u003e」\u003c/p\u003e\n\u003e\n\u003e 3. 在这之后，教授有陆续发行自己钢琴演奏版本的「A Flower is Not A Flower」。如果说文金龙版本的是线状绵延的凄凉，那在教授钢琴的版本里，我们听到的是点状的，在夜里，一边开，一边败的花的失落之美。\n\u003e   \n\u003e\t  在教授这个阶段的很多作品里，我们都能找到类似的气质。或者说，教授本身就很擅长用琴键去勾勒这种气质。如果允许我用很自私的感受去总结的话，我想对于我而言就是，在这样的作品里，*哪怕是简单排列的单音，我们也能听到一边生长，一边流失*。\n\u003e\n\u003e    \u003cp style=\"text-align: right\"\u003e——HOPICO 评价 「\u003ca href=\"\"\u003eA Flower is Not A Flower\u003c/a\u003e」\u003c/p\u003e\n\u003e\n\u003e 4. 伟大的艺术家是在不停迭代自己的生命周期\n\u003e   \n\u003e \u003cp style=\"text-align: right\"\u003e——「\u003ca href=\"https://www.imdb.com/title/tt6578572/\"\u003eRyuichi Sakamoto: Coda\u003c/a\u003e」\u003c/p\u003e\n\u003e\n\u003e 5. 教授大概说过这么一句话，就是，钢琴的声音按下去之后，会逐渐地开始衰减，哪怕非常微弱地持续，最后也会消失，所以他也渴望可以永恒发展下去的声音。\n\u003e    \n\u003e    而在「andata」这首作品里，我们可以听到在下面这段与管风琴音色所演奏出来的主旋律的稳定所呼应的是一种在持续出现的，让人不安或者不稳定的合成器的声响。这看似矛盾的两者出现在一起，正是一种异步。他在去除人为对音乐冠以的快乐、悲伤这些明显的情绪化的修饰。声音的产生和这种不规律是去人性的，是高于人为定义的，生命也是如此。\n\u003e    \n\u003e    \u003cp style=\"text-align: right\"\u003e——HOPICO 评价 「\u003ca href=\"https://music.apple.com/cn/album/andata/1507014129?i=1507014130\"\u003eandata\u003c/a\u003e」\u003c/p\u003e\n\u003e    \n\u003e   6.  专辑的概念「异步」通过不同的形式贯穿每一首作品，在下面这首音乐里，我们可以听到一个好像扮演心跳信号的音色，贯穿始终。一般的做法，可能会使这个音色的节律和作品的速度保持一致，然后同步落到作品的正拍上。但明显，在这部作品里，教授让这个心跳节律的音色和作品的拍子，处在一个完全异步的过程当中。*我自私的理解是*，**生命中的声音与音乐不和谐的共存，才是真实的空间。毕竟和谐在多数时候都是人为制造的巧合，而生命中巧合的几率又只是少数**。\n\u003e \n\u003e \u003cp style=\"text-align: right\"\u003e——HOPICO 评价 「\u003ca href=\"https://music.apple.com/cn/album/ubi/1507014129?i=1507014136\"\u003eubi\u003c/a\u003e」\u003c/p\u003e\n\u003e \n\n![](文学/句子/attachments/Pasted%20image%2020230409171853.png)\n\n\n--- \n\n\u003e [!quote] \n\u003e  [大卫·福斯特·华莱士](https://www.salon.com/1996/03/09/wallace_5/)，美国小说家\n\u003e  \n\u003e  每天，我会接触到250个广告和无数的娱乐选择，它们大部分都是由想卖给我东西的公司资助的。\n\u003e  \n\u003e  这就是世界对我产生影响的方式。我是一个作家，我的小说大量使用这些流行元素，这与100年前的小说家写花园散步和步行到河边取水的生活，并没有什么不同，人类的日常生活已经变了。\n\n--- \n\n\u003e [!quote] \n\u003e From [年轻用户对B站不是财富，是诅咒【解构B站2】](https://www.bilibili.com/video/BV1Hz4y1Y7XS/?spm_id_from=333.337.search-card.all.click\u0026vd_source=c47136abc78922800b17d6ce79d6e19f) \n\u003e \n\u003e 我没觉得往左就有问题，我也没觉得生活和审美上的保守主义有问题，我也不觉得举出问题，探讨问题这件事有问题；\n\u003e \n\u003e 我觉得什么有问题\n\u003e \n\u003e 我觉得单一狭隘的价值观是有问题的，我觉得党同伐异、充满戾气的心态是有问题的，我觉得阴阳怪气，玩梗扣帽子把他当成习惯是有问题的，我觉得把自己认识世界的方式当成唯一正确的真理，并且攻击其他任何有不同看法的人，这是有问题的。\n\n--- \n\n\u003e [!quote] \n\u003e  From [年轻用户对B站不是财富，是诅咒【解构B站2】](https://www.bilibili.com/video/BV1Hz4y1Y7XS/?spm_id_from=333.337.search-card.all.click\u0026vd_source=c47136abc78922800b17d6ce79d6e19f) \n\u003e  \n\u003e  我再给自己反向叠个甲\n\u003e  \n\u003e  我觉得原神是文化输出，科比比詹姆斯厉害，飞盘是个好玩的运动，丁太升是不错的音乐评论人，但是同时华晨宇写的一些歌也挺好，同时我觉得塞尔达这个游戏我根本玩不下去，我不喜欢；而且我还是一个自由主义者，而且还有，我觉得无知不是错，但是愚蠢是一种道德缺陷。\n\n\n","lastmodified":"2023-09-18T02:45:48.457257029Z","tags":null},"/%E6%96%87%E5%AD%A6/%E5%8F%A5%E5%AD%90/Feeling":{"title":"🧶Feeling","content":"\n  \u003cstyle\u003e\n    p {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n\n    a {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n  \u003c/style\u003e\n\n\n\u003e [!quote] \n\u003e  From [去看花火大会，女友说不如橘子洲头的烟花！](https://www.bilibili.com/video/BV1tu411A7jj/?spm_id_from=333.999.0.0\u0026vd_source=c47136abc78922800b17d6ce79d6e19f)\n\u003e  \n\u003e  一个小小的光点迅速上升，升到了最高点，猛然绽放\n\u003e  \n\u003e  盛大 金黄 灿烂\n\u003e  \n\u003e  在场的所有人，都沉醉在这一刻的流光溢彩里\n\u003e  \n\u003e  而唯独我慌了\n\u003e  \n\u003e  我从镜头中感觉到不太对劲儿，烟花飞出了我的画面，大脑开始飞速地运转，要调动云台，试图追上它，但是它离我们太近太近了，35mm的镜头不够广，长焦拍出来带不到海平面儿。\n\u003e  \n\u003e  于是我开始折腾，翻包换镜头，重新构图，甚至开始考虑我们要不要往后撤。但转念又想，这个时候站起来，会影响到后面好多人看花火。\n\u003e  \n\u003e  我坐立不安，在摄影师的本能和异乡游客的旅行准则之间 **来回拉扯**\n\u003e  \n\u003e  但珍贵的花火大会，只有短短的十五分钟\n\u003e  \n\u003e  米老师有点不高兴，她说你能不能就肉眼看啊，不拍了，\n\u003e  \n\u003e  “你知道很好看的时候，你都在前面弄相机”\n\u003e  \n\u003e  我有点沮丧，心里想着，好不容易看到的花火怎么可能不记录下来呢，不然我怎么更vlog呀 对不对\n\u003e  \n\u003e  我回头看了一眼，她用前置摄像头拍的手机机位，竟然出人意料的很不错。而且画面里面，是带着我们两个人的。\n\u003e  \n\u003e  **我一直在想，如何把花火拍得更好看，而她则一直在想，如何把我们在这的一瞬间记录下来**\n\u003e  \n\u003e  突然间，有点惭愧\n\u003e  \n\u003e  摄影师，要拍摄的，究竟是一段素材，还是一段人生呢。如果这是我的一段素材，那我必须要保证，拍到一场完美无暇的花火。而如果这是一段人生，那我也必须要接受，**这其中有遗憾，不精准和慌乱，却也是不可重来的体验**\n\u003e  \n\u003e  于是我把唯一的三脚架转向了我们自己。前面是什么样的花火我想都不重要了，此刻的我们才是最重要的。\n\u003e  \n\u003e  此刻翻涌的情绪，才是为我们所拥有的纪念曾存在过的痕迹\n\n--- \n\n\u003e [!quote] \n\u003e A Sentence\n\u003e \n\u003e 一个太过于文艺的人注定不会快乐，因为心中有爱 有善良，骨子里住着孩子般的纯真，但也往往容易多愁善感，容易感知美好，也更容易体会悲伤。她喜欢文字，往往不善言辞，不是文字太少，而是感受太多。\n\u003e  \u003cp style=\"text-align:right\"\u003e——三毛\u003c/p\u003e\n\n\n--- \n\n\u003e [!quote] \n\u003e From bilibili vedio -  [陶喆给别人写的歌是什么水平?丨HOPICO](https://www.bilibili.com/video/BV1fo4y1z7jf/?spm_id_from=333.999.0.0\u0026vd_source=c47136abc78922800b17d6ce79d6e19f)\n\u003e \n\u003e 我们在面对回忆时，有一种态度就是，在疯狂之后都会回到安静里。举例证明，那大概就是在计程车后排，放空的眼神里，实际上在脑海中汹涌着，翻滚的回忆。\n\n\n--- \n\n\u003e [!quote] \n\u003e From JudeW, me\n\u003e \n\u003e 你的心软，世界的触感也更加真实\n\n\n--- \n\n\u003e [!quote] \n\u003e  A sentence\n\u003e  \n\u003e  “一个真正想死的人，不会再计较人们说什么，一个拿死说来说去的人，以我的经验来看并不是真的想死，而是......”   \n\u003e  \n\u003e  “而是什么？”  \n\u003e  \n\u003e  “而是还在......还在渴望爱。”\n\u003e  \n\u003e  \u003cp style=\"text-align: right\"\u003e——史铁生《务虚笔记》\u003c/p\u003e\n\n---\n\n\u003e [!quote] \n\u003e From somebody\n\u003e \u003cbr\u003e\n\u003e 爱情是需要结局的。\n\n--- \n\n\u003e [!quote] \n\u003e  From [很抱歉告诉你，审美真的分高低](https://www.bilibili.com/video/BV1hk4y1T7Gu/?spm_id_from=333.999.0.0\u0026vd_source=c47136abc78922800b17d6ce79d6e19f)\n\u003e  \u003cbr\u003e\n\u003e  爱斯基摩人描述雪的词汇多达上千个，它们对雪的鉴赏能力，就是比我们多元。因为词汇量，决定了思想的边界。\n\u003e\u003cbr\u003e\n\u003e当你知道了克莱因蓝，勃艮第红，莫奈灰，你的色彩体验就不再是单纯的赤橙黄绿青蓝紫。面对色彩时，你的感知维度就会被打开。\n\n\n--- \n\n\u003e [!quote] \n\u003e   From [小红书，我要WhatYouNeed，”一个好的恋人，会重新塑造你对爱情的理解。“](https://www.xiaohongshu.com/explore/64a5350e000000003500bd53)\n\u003e   \u003cbr\u003e\n\u003e   男朋友对待玩偶的态度，\u003cbr\u003e\n\u003e   能让我随着他一起放松。\u003cbr\u003e\n\u003e   \u003cbr\u003e\n\u003e   我家有一只青蛙玩偶，有的晚上我和男友睡不着，就会给它编故事；\n\u003e   \u003cbr\u003e\n\u003e   \"它叫洼袜，想成为选秀歌手的美男子\"\u003cbr\u003e\n\u003e   \"曾经也摆过鱼蛋摊，最后自己吃太多导致血本无归\"\u003cbr\u003e\n\u003e   ... ... \u003cbr\u003e\n\u003e   \u003cbr\u003e\n\u003e   编着编着，洼袜好像真成了我们家的一员。\u003cbr\u003e\n\u003e   \u003cbr\u003e\n\u003e   后来我们又买了其他动物，每一只都有自己的身世。我和男朋友有时候会模仿他们进行对话，画面可能有点傻，但很奇怪，**那些平时不知道怎么表达的情绪，通过他们就能轻易说出来了**。\u003cbr\u003e\n\n\n--- \n\n![](文学/句子/attachments/Pasted%20image%2020230905221135.png)\n\n\u003e [!quote] \n\u003e From 先知少女\u003cbr\u003e\n\u003e 我是被时间裹挟前进的 \n\n\n","lastmodified":"2023-09-18T02:45:48.457257029Z","tags":null},"/%E6%96%87%E5%AD%A6/%E5%8F%A5%E5%AD%90/Love_about":{"title":"🌸Love","content":"\n  \u003cstyle\u003e\n    p {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n\n    a {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n  \u003c/style\u003e\n\n  \u003e [!quote] \n\u003e [A poem](https://www.bilibili.com/video/BV1V24y1x7Nh/?buvid=YF4AFCFA7E0887094E329B9A6FADF98BF343\u0026is_story_h5=false\u0026mid=B1quk6Mlu6tnRY8zjwxWeg%3D%3D\u0026p=1\u0026plat_id=116\u0026share_from=ugc\u0026share_medium=iphone\u0026share_plat=ios\u0026share_session_id=4AD8E5F4-D617-499B-9C19-D5897A7EB825\u0026share_source=QQ\u0026share_tag=s_i\u0026timestamp=1679378304\u0026unique_k=tXa4xdJ\u0026up_id=315154029\u0026vd_source=c47136abc78922800b17d6ce79d6e19f) \u003cbr\u003e\n\u003e “至少有两次喜欢\u003cbr\u003e\n\u003e 一次发生在在一起之前，一次发生在之后，\u003cbr\u003e\n\u003e 第一次是喜欢上你。第二次是喜欢上我们，\u003cbr\u003e\n\u003e 我只敢把第二次翻译成爱，\u003cbr\u003e\n\u003e 第一次是因为你很好，第二次是因为我还没有坏到敢放满揉碎你的好。\u003cbr\u003e\n\u003e 我要小心的捧着第一次的喜欢，就像掏出一份手写的初稿，\u003cbr\u003e\n\u003e 你会接过我的目光，在岁月里重新誊写岁月，\u003cbr\u003e\n\u003e 要删改的地方还很多，包括但不限于，把差错翻译成幽默\u003cbr\u003e\n\u003e 改了还是存在啊，爱情本就是听起来很美的，阴差阳错”\n\n\n--- \n\n\u003e [!quote] \n\u003e A Sentence\n\u003e \n\u003e \"I am too full of life to be half-loved\"\n\u003e \n\n![400](文学/attachments/Pasted%20image%2020230321142115.png)\n\n---\n\n\u003e [!quote] \n\u003e [A poem](https://www.bilibili.com/video/BV1Vd4y187Tq/?buvid=YF4AFCFA7E0887094E329B9A6FADF98BF343\u0026is_story_h5=false\u0026mid=B1quk6Mlu6tnRY8zjwxWeg%3D%3D\u0026p=1\u0026plat_id=116\u0026share_from=ugc\u0026share_medium=iphone\u0026share_plat=ios\u0026share_session_id=F81E6185-E382-4E78-95FD-3155869F570B\u0026share_source=QQ\u0026share_tag=s_i\u0026timestamp=1679380048\u0026unique_k=Q9GSCLM\u0026up_id=2009238634\u0026vd_source=c47136abc78922800b17d6ce79d6e19f)\u003cbr\u003e\n\u003e  “他听的小众音乐，\u003cbr\u003e\n\u003e  第二天上了抖音热榜。\u003cbr\u003e\n\u003e  他爱吃的苍蝇小馆，\u003cbr\u003e\n\u003e  第二天全国连锁了一万家。\u003cbr\u003e\n\u003e  他爱的县城姑娘，\u003cbr\u003e\n\u003e  第二天出国留学再也没回来。\u003cbr\u003e\n\u003e  他觉得那些特别，\u003cbr\u003e\n\u003e  都已烟消云散，\u003cbr\u003e\n\u003e  可那些特别一直都在，\u003cbr\u003e\n\u003e  错就错在他认为特别\u003cbr\u003e\n\u003e  只属于他。”\u003cbr\u003e\n\u003e  \u003ca href=\"https://space.bilibili.com/2009238634\"\u003e\u003cp style=\"text-align:right\"\u003e——祺白石\u003c/p\u003e\u003c/a\u003e\n\n![400](文学/attachments/Pasted%20image%2020230321143300.png)\n\n--- \n\n\u003e [!quote] \n\u003e  From Network\n\u003e  \n\u003e  暗恋，像不停对焦的长镜头\n\n\n--- \n\n\u003e [!quote] \n\u003e  From [Xiaogongshu](https://www.xiaohongshu.com/explore/6466d391000000001300b055)\n\u003e  \n\u003e  标准是对不喜欢的人设定的，只有你才是例外和偏爱\n\n\n![](文学/句子/attachments/Pasted%20image%2020230519160552.png)\n\n\n--- \n\n\u003e [!quote] \n\u003e  From [氛围帅哥杰西卡](https://www.xiaohongshu.com/explore/649164b300000000140270e7)\n\u003e  \u003cbr\u003e\n\u003e  我说，\u003cbr\u003e\n\u003e  不可否认的是我把她神化了\u003cbr\u003e\n\u003e  但以我难以启齿的人生经验而言\u003cbr\u003e\n\u003e  这确实是我贫瘠土地升起的神庙\u003cbr\u003e\n\u003e  \n\n\n---\n\n\u003e [!quote] \n\u003e From somebody\n\u003e \u003cbr\u003e\n\u003e 爱情是需要结局的。\n\n---\n\n\u003e [!quote] \n\u003e From 网易云\n\u003e \u003cbr\u003e\n\u003e 遇到好看的云，满心欢喜\u003cbr\u003e\n\u003e 可你不在了，这欢喜便没了去处\n\n --- \n\n \u003e [!quote] \n\u003e From [小红书，我要WhatYouNeed，”一个好的恋人，会重新塑造你对爱情的理解。“](https://www.xiaohongshu.com/explore/64a5350e000000003500bd53)\n\u003e \u003cbr\u003e\n\u003e 她突然说要照镜子\u003cbr\u003e\n\u003e 然后摘下了我的眼镜\u003cbr\u003e\n\u003e \n\n--- \n\n\u003e [!quote] \n\u003e   From [小红书，我要WhatYouNeed，”一个好的恋人，会重新塑造你对爱情的理解。“](https://www.xiaohongshu.com/explore/64a5350e000000003500bd53)\n\u003e   \u003cbr\u003e\n\u003e   男朋友对待玩偶的态度，\u003cbr\u003e\n\u003e   能让我随着他一起放松。\u003cbr\u003e\n\u003e   \u003cbr\u003e\n\u003e   我家有一只青蛙玩偶，有的晚上我和男友睡不着，就会给它编故事；\n\u003e   \u003cbr\u003e\n\u003e   \"它叫洼袜，想成为选秀歌手的美男子\"\u003cbr\u003e\n\u003e   \"曾经也摆过鱼蛋摊，最后自己吃太多导致血本无归\"\u003cbr\u003e\n\u003e   ... ... \u003cbr\u003e\n\u003e   \u003cbr\u003e\n\u003e   编着编着，洼袜好像真成了我们家的一员。\u003cbr\u003e\n\u003e   \u003cbr\u003e\n\u003e   后来我们又买了其他动物，每一只都有自己的身世。我和男朋友有时候会模仿他们进行对话，画面可能有点傻，但很奇怪，**那些平时不知道怎么表达的情绪，通过他们就能轻易说出来了**。\u003cbr\u003e\n\u003e   \n\n--- \n\n\u003e [!quote] \n\u003e From [小猫说，在胃痛面前，心动一文不值](https://www.bilibili.com/video/BV1hh4y157Nk/?spm_id_from=333.999.0.0\u0026vd_source=c47136abc78922800b17d6ce79d6e19f) \n\u003e \n\u003e 既新鲜又长久的爱情是，反复爱上同一个人\n\n\n","lastmodified":"2023-09-18T02:45:48.457257029Z","tags":null},"/%E6%96%87%E5%AD%A6/%E5%8F%A5%E5%AD%90/Motivation":{"title":"🧗🏻‍♂️Motivation","content":"\n  \u003cstyle\u003e\n    p {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n\n    a {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n  \u003c/style\u003e\n\n\n\n\u003e [!quote] \n\u003e From [火柴钰's Comment ](https://www.bilibili.com/video/BV1634y1N78j/?spm_id_from=333.337.search-card.all.click) \n\u003e \n\u003e 三明治直播自己说了\"我把所有强队的习惯录像全部记下来了，我都知道A队如果是穹顶的圈，二圈刷的时候，Effect会自己去那个树木旁边山\"。当时我就震惊了，细到这种程度。\n\n--- \n\n\u003e [!quote] \n\u003e A Sentence\n\u003e \n\u003e \"No easy basket\"\n\n“如果你想了解American篮球的根基，你要去看看美高”\n\n--- \n\n\u003e [!quote] \n\u003e  [人生是一个长板问题](https://github.com/ruanyf/weekly/blob/master/docs/issue-254.md)\n\u003e  \n\u003e  大家可能听说过“[水桶原理](https://baike.baidu.com/item/%E6%B0%B4%E6%A1%B6%E6%95%88%E5%BA%94/10942611)”：水桶的容量由最短的那块木板决定。\n\u003e  \n\u003e  它的意思是，某些系统的关键，不在于发展最强点，而在于避免最弱点。99%的地方都没有问题，只要1%的地方出现问题，整个系统就会失败。\n\u003e  \n\u003e  人体健康就是这样，有一个器官出现严重问题，哪怕其他器官完全正常，生活甚至生命就会受到影响。\n\u003e  \n\u003e  这类由短板决定的问题，统称为“**短板问题**”。日常生活有很多这样的例子，除了人体健康，还有食品安全，只要有一样成份不干净，你可能就会食物中毒。\n\u003e  \n\u003e  汽车、电视机、手机等消费品也是这样，只要有一个部件不合格，这个产品就有质量问题。\n\u003e  \n\u003e  但是，这不是今天的主题。我最近读到[一篇文章](https://www.experimental-history.com/p/science-is-a-strong-link-problem)，才意识到除了短板问题，还有长板问题\n\u003e  \n\u003e  **“长板问题”指的是，问题的关键不在于最弱点，而在于最强点。** 只要有一个点特别出色，这件事情就成功了，其他点的好坏无所谓。\n\u003e  \n\u003e  文艺作品就属于这种情况。你购买了一张专辑，其他的歌曲都不爱听，但是有一首歌你特别喜欢，这张专辑就值得了。电影和小说只要有一个角色或情节特别打动人，作品就成功了。\n\u003e  \n\u003e  风险投资也是这样，只要投了一个特别成功的项目，就能把所有损失补回来。\n\u003e  \n\u003e  最重要的是，**人生就是一个“长板问题”。** 一生中，失败和挫折其实不重要，多少次都不重要，只要有一次大的成功，人生就成功了。\n\u003e  \n\u003e  最大的那一次成功，决定了你一生的成就和高度。很多诺贝尔奖得主，一生就做出了一个重要的科学发现，就足够成为伟大科学家了。\n\u003e  \n\u003e  程序员写过多少代码不重要，只要创造过一个重大影响力的软件，职业生涯就成功了。\n\u003e  \n\u003e  **我们必须学会区分“短板问题”和“长板问题”，它们的解决方法完全不同。** 短板问题的解决，需要盯着薄弱环节，补齐最短的那块板；长板问题的解决，只需要推进最强的环节，不要在乎别的。\n\u003e  \n\u003e  人生不必在乎那些不重要的事情，没必要为了挫折和拒绝而沮丧，都会过去的。你要做的是向前看，拼命争取一次大的成功，让它足够大、更大，只要一次就够了。\n\n---\n\n\u003e [!quote] \n\u003e From [废柴责任有限公司-高考失利也莫得事 ](https://www.bilibili.com/video/BV1Uz4y1J7cQ/?spm_id_from=333.999.0.0\u0026vd_source=c47136abc78922800b17d6ce79d6e19f)\n\u003e \n\u003e 都说人生是长跑，但没人说是长跑比赛啊\n\u003e \u003cbr\u003e\n\u003e 只要你经历过，你就已经赢了\n\n\n\n\n","lastmodified":"2023-09-18T02:45:48.457257029Z","tags":null},"/%E6%96%87%E5%AD%A6/%E5%8F%A5%E5%AD%90/Movie":{"title":"🎞Movie","content":"\n  \u003cstyle\u003e\n    p {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n\n    a {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n  \u003c/style\u003e\n\n\u003e [!quote] \n\u003e  铃芽之旅\n\u003e  \n\u003e  \"鎮住土地的是人心的重量。\"\n\n\n--- \n\n\n\u003e [!quote] \n\u003e  From comments\n\u003e  \n\u003e   被一些影评人的高度评价给诈骗到了;看来不同人对浪漫的定义非常不同，对于有些人来说“宇宙”“存在”等词以及语焉不详的现代诗歌排列组合在一起即可触发内心浪漫情结，就跟大学生会用夏天、自由、苏打、快乐with黄油相机滤镜加字加字照片来营造自觉出众的氛围感朋友圈一样。女儿的线也太刻奇，套了一个寻找外星人的噱头、还有伪纪录片的形式，镜头的设计还有手持的感觉在大荧幕上显得非常粗糙，之前很喜欢导演那个《法制未来时》的短片,结果电影有种加长版视频的感觉，还是感觉撑不起来啊... ...\n\n","lastmodified":"2023-09-18T02:45:48.457257029Z","tags":null},"/%E6%96%87%E5%AD%A6/%E5%8F%A5%E5%AD%90/Music":{"title":"🎺Music","content":"\n  \u003cstyle\u003e\n    p {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n\n    a {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n  \u003c/style\u003e\n\nMusic links usually link to apple music\n\n\u003e [!quote] \n\u003e From a video talking about 坂本龙一 by [HOPICO](https://www.bilibili.com/video/BV1pa4y1T7v2/?spm_id_from=333.1007.top_right_bar_window_history.content.click\u0026vd_source=c47136abc78922800b17d6ce79d6e19f) \n\u003e \n\u003e 1. 脱下合成器的修饰之后，这首作品变成了一个最赤裸的样子。我们听过印象深刻的钢琴曲，我们热爱他们，我们在形容它们的时候，多数是“热情”、“悲伤”、“欢乐”、“雄伟”。但这首歌不一样，之于我第一次听到它的时候，我再想为什么会有一首歌，那么精确地，在开头把“安静”这两个字讲了出来，明明无声的真空才是最安静，可这几个音符勾勒出来的安静，就是胜过了无声的真空。我想，那是因为我们在这几个音符背后，仿佛能够看到一个作曲家，找到他的钢琴，好像全世界只剩下他们的样子。\n\u003e    \n\u003e    要说幸运的是，我亲眼见过教授带着管弦乐团的全编制演奏这首作品。虽然我内心一直觉得，这是一首寂寞的作品，我心中它最好的样子，就是保持在三人编制以下，但是我清楚记得，那天在现场听到最后一个段落时，我眼泪止不住地往下流，我还记得我当时心里想的是：“md，这么多年了，终于听到了”\n\u003e    \n\u003e    就像教授在后来采访里说的，它可能不是首好的电影配乐，因为它不需要画面就已经自成一体了。可是重新听到它地时候，仍然意气风发。\n\u003e    \n\u003e    \u003cp style=\"text-align: right\"\u003e——HOPICO 评价 「\u003ca href=\"https://music.apple.com/cn/album/merry-christmas-mr-lawrence-coda/1404842855?i=1404843053\"\u003eMerry Christmas, Mr. Lawrence\u003c/a\u003e」\u003c/p\u003e\n\u003e    \n\u003e 2. 坂本龙一小的时候很喜欢Beatles，甚至一度以为只有了解Beatles的人才能和自己做朋友。而「末代皇帝」当中，许多作品的录制，都是在abbey road 2号录影棚完成的，这里是The Beatles 1962年到1969年录音的地方。教授也通过这样的方式，和自己的偶像有了*重合*。\n\u003e    \n\u003e    而在这里录制的作品，就包括整部原声带当中我最喜欢的「Where is Armo?」，在这首歌的写作里，他构建出了一种徐徐前行，奔赴宿命的坦荡。\n\u003e    \n\u003e   \u003cp style=\"text-align: right\"\u003e——HOPICO 评价 「\u003ca href=\"https://music.apple.com/cn/album/where-is-armo/714659119?i=714659278\"\u003eWhere is Armo?\u003c/a\u003e」\u003c/p\u003e\n\u003e\n\u003e 3. 在这之后，教授有陆续发行自己钢琴演奏版本的「A Flower is Not A Flower」。如果说文金龙版本的是线状绵延的凄凉，那在教授钢琴的版本里，我们听到的是点状的，在夜里，一边开，一边败的花的失落之美。\n\u003e   \n\u003e\t  在教授这个阶段的很多作品里，我们都能找到类似的气质。或者说，教授本身就很擅长用琴键去勾勒这种气质。如果允许我用很自私的感受去总结的话，我想对于我而言就是，在这样的作品里，*哪怕是简单排列的单音，我们也能听到一边生长，一边流失*。\n\u003e\n\u003e    \u003cp style=\"text-align: right\"\u003e——HOPICO 评价 「\u003ca href=\"\"\u003eA Flower is Not A Flower\u003c/a\u003e」\u003c/p\u003e\n\u003e\n\u003e 4. 伟大的艺术家是在不停迭代自己的生命周期\n\u003e   \n\u003e \u003cp style=\"text-align: right\"\u003e——「\u003ca href=\"https://www.imdb.com/title/tt6578572/\"\u003eRyuichi Sakamoto: Coda\u003c/a\u003e」\u003c/p\u003e\n\u003e\n\u003e 5. 教授大概说过这么一句话，就是，钢琴的声音按下去之后，会逐渐地开始衰减，哪怕非常微弱地持续，最后也会消失，所以他也渴望可以永恒发展下去的声音。\n\u003e    \n\u003e    而在「andata」这首作品里，我们可以听到在下面这段与管风琴音色所演奏出来的主旋律的稳定所呼应的是一种在持续出现的，让人不安或者不稳定的合成器的声响。这看似矛盾的两者出现在一起，正是一种异步。他在去除人为对音乐冠以的快乐、悲伤这些明显的情绪化的修饰。声音的产生和这种不规律是去人性的，是高于人为定义的，生命也是如此。\n\u003e    \n\u003e    \u003cp style=\"text-align: right\"\u003e——HOPICO 评价 「\u003ca href=\"https://music.apple.com/cn/album/andata/1507014129?i=1507014130\"\u003eandata\u003c/a\u003e」\u003c/p\u003e\n\u003e    \n\u003e   6.  专辑的概念「异步」通过不同的形式贯穿每一首作品，在下面这首音乐里，我们可以听到一个好像扮演心跳信号的音色，贯穿始终。一般的做法，可能会使这个音色的节律和作品的速度保持一致，然后同步落到作品的正拍上。但明显，在这部作品里，教授让这个心跳节律的音色和作品的拍子，处在一个完全异步的过程当中。*我自私的理解是*，**生命中的声音与音乐不和谐的共存，才是真实的空间。毕竟和谐在多数时候都是人为制造的巧合，而生命中巧合的几率又只是少数**。\n\u003e \n\u003e \u003cp style=\"text-align: right\"\u003e——HOPICO 评价 「\u003ca href=\"https://music.apple.com/cn/album/ubi/1507014129?i=1507014136\"\u003eubi\u003c/a\u003e」\u003c/p\u003e\n\u003e \n\n![](文学/句子/attachments/Pasted%20image%2020230409171853.png)\n\n\n--- \n\n\u003e [!quote] \n\u003e [身骑白马](https://music.apple.com/cn/album/%E8%BA%AB%E9%AA%91%E7%99%BD%E9%A9%AC/672648486?i=672648896), 徐佳莹 \n\u003e \n\u003e 而你却 靠近了\n\u003e \n\u003e 逼我们视线交错\n\n\n\n\n\n\n","lastmodified":"2023-09-18T02:45:48.457257029Z","tags":null},"/%E6%96%87%E5%AD%A6/%E5%8F%A5%E5%AD%90/Novel":{"title":"From Novel","content":"\n  \u003cstyle\u003e\n    p {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n\n    a {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n  \u003c/style\u003e\n\n\n\u003e [!quote] \n\u003e From CC98, credits to someone\n\u003e \n\u003e 很奇怪，与一个人告别之后男生记起来的永远是一些微不足道的细节，男生不知道女生的名字，也不记得女生常穿的衣服是什么颜色，男生忘掉了每天在桥边上等女生的时间，女生的眼睛也慢慢在男生的记忆里蒙尘。 \n\u003e \n\u003e 不过男生记得等女生时桥下的流水潺潺，也记得校园里某个角落他们经常去看的四叶草，记得第一次注意到女生时公交站顶上那片不同的落叶，也记得步道旁女生指给他看的树皮。 \n\u003e \n\u003e 后来男生看了阿尔瓦雷斯的《行走的距离》，里面有句话是“别人稍一注意你，你就敞开心扉，你觉得这是坦率，其实这是孤独。”\n\u003e \n\u003e  男生不知道自己是不是孤独，也不知道自己是不是坦率。男生不关心。 \n\u003e  \n\u003e  男生很感谢那个女生。 \n\u003e  \n\u003e  男生很想写下“不过男生并不想念那个女生”。 \n\u003e  \n\u003e  不过男生很想念那个女生。\n","lastmodified":"2023-09-18T02:45:48.457257029Z","tags":null},"/%E6%96%87%E5%AD%A6/%E5%8F%A5%E5%AD%90/Poem":{"title":"🖋Poem","content":"\n  \u003cstyle\u003e\n    p {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n\n    a {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n  \u003c/style\u003e\n\n\u003e [!quote] \n\u003e [A poem](https://www.bilibili.com/video/BV1V24y1x7Nh/?buvid=YF4AFCFA7E0887094E329B9A6FADF98BF343\u0026is_story_h5=false\u0026mid=B1quk6Mlu6tnRY8zjwxWeg%3D%3D\u0026p=1\u0026plat_id=116\u0026share_from=ugc\u0026share_medium=iphone\u0026share_plat=ios\u0026share_session_id=4AD8E5F4-D617-499B-9C19-D5897A7EB825\u0026share_source=QQ\u0026share_tag=s_i\u0026timestamp=1679378304\u0026unique_k=tXa4xdJ\u0026up_id=315154029\u0026vd_source=c47136abc78922800b17d6ce79d6e19f) \u003cbr\u003e\n\u003e “至少有两次喜欢\u003cbr\u003e\n\u003e 一次发生在在一起之前，一次发生在之后，\u003cbr\u003e\n\u003e 第一次是喜欢上你。第二次是喜欢上我们，\u003cbr\u003e\n\u003e 我只敢把第二次翻译成爱，\u003cbr\u003e\n\u003e 第一次是因为你很好，第二次是因为我还没有坏到敢放满揉碎你的好。\u003cbr\u003e\n\u003e 我要小心的捧着第一次的喜欢，就像掏出一份手写的初稿，\u003cbr\u003e\n\u003e 你会接过我的目光，在岁月里重新誊写岁月，\u003cbr\u003e\n\u003e 要删改的地方还很多，包括但不限于，把差错翻译成幽默\u003cbr\u003e\n\u003e 改了还是存在啊，爱情本就是听起来很美的，阴差阳错”\n\n---\n\n\u003e [!quote] \n\u003e [A poem](https://www.bilibili.com/video/BV1Vd4y187Tq/?buvid=YF4AFCFA7E0887094E329B9A6FADF98BF343\u0026is_story_h5=false\u0026mid=B1quk6Mlu6tnRY8zjwxWeg%3D%3D\u0026p=1\u0026plat_id=116\u0026share_from=ugc\u0026share_medium=iphone\u0026share_plat=ios\u0026share_session_id=F81E6185-E382-4E78-95FD-3155869F570B\u0026share_source=QQ\u0026share_tag=s_i\u0026timestamp=1679380048\u0026unique_k=Q9GSCLM\u0026up_id=2009238634\u0026vd_source=c47136abc78922800b17d6ce79d6e19f)\u003cbr\u003e\n\u003e  “他听的小众音乐，\u003cbr\u003e\n\u003e  第二天上了抖音热榜。\u003cbr\u003e\n\u003e  他爱吃的苍蝇小馆，\u003cbr\u003e\n\u003e  第二天全国连锁了一万家。\u003cbr\u003e\n\u003e  他爱的县城姑娘，\u003cbr\u003e\n\u003e  第二天出国留学再也没回来。\u003cbr\u003e\n\u003e  他觉得那些特别，\u003cbr\u003e\n\u003e  都已烟消云散，\u003cbr\u003e\n\u003e  可那些特别一直都在，\u003cbr\u003e\n\u003e  错就错在他认为特别\u003cbr\u003e\n\u003e  只属于他。”\u003cbr\u003e\n\u003e  \u003ca href=\"https://space.bilibili.com/2009238634\"\u003e\u003cp style=\"text-align:right\"\u003e——祺白石\u003c/p\u003e\u003c/a\u003e\n\n![400](文学/attachments/Pasted%20image%2020230321143300.png)\n\n--- \n\n\u003e [!quote] \n\u003e  From Network\n\u003e  \n\u003e  像不停对焦的长镜头\n\n\n--- \n\n\u003e [!quote] \n\u003e  From [氛围帅哥杰西卡](https://www.xiaohongshu.com/explore/649164b300000000140270e7)\n\u003e  \u003cbr\u003e\n\u003e  我说，\u003cbr\u003e\n\u003e  不可否认的是我把她神化了\u003cbr\u003e\n\u003e  但以我难以启齿的人生经验而言\u003cbr\u003e\n\u003e  这确实是我贫瘠土地升起的神庙\u003cbr\u003e\n\u003e  \n\n---\n\n\u003e [!quote] \n\u003e From 网易云\n\u003e \u003cbr\u003e\n\u003e 遇到好看的云，满心欢喜\u003cbr\u003e\n\u003e 可你不在了，这欢喜便没了去处\n\n","lastmodified":"2023-09-18T02:45:48.457257029Z","tags":null},"/%E6%96%87%E5%AD%A6/%E5%8F%A5%E5%AD%90/Wisdom":{"title":"🧙‍♂️Wisdom","content":"\n  \u003cstyle\u003e\n    p {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n\n    a {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n  \u003c/style\u003e\n\n\n\u003e [!quote] \n\u003e  [人生是一个长板问题](https://github.com/ruanyf/weekly/blob/master/docs/issue-254.md)\n\u003e  \n\u003e  大家可能听说过“[水桶原理](https://baike.baidu.com/item/%E6%B0%B4%E6%A1%B6%E6%95%88%E5%BA%94/10942611)”：水桶的容量由最短的那块木板决定。\n\u003e  \n\u003e  它的意思是，某些系统的关键，不在于发展最强点，而在于避免最弱点。99%的地方都没有问题，只要1%的地方出现问题，整个系统就会失败。\n\u003e  \n\u003e  人体健康就是这样，有一个器官出现严重问题，哪怕其他器官完全正常，生活甚至生命就会受到影响。\n\u003e  \n\u003e  这类由短板决定的问题，统称为“**短板问题**”。日常生活有很多这样的例子，除了人体健康，还有食品安全，只要有一样成份不干净，你可能就会食物中毒。\n\u003e  \n\u003e  汽车、电视机、手机等消费品也是这样，只要有一个部件不合格，这个产品就有质量问题。\n\u003e  \n\u003e  但是，这不是今天的主题。我最近读到[一篇文章](https://www.experimental-history.com/p/science-is-a-strong-link-problem)，才意识到除了短板问题，还有长板问题\n\u003e  \n\u003e  **“长板问题”指的是，问题的关键不在于最弱点，而在于最强点。** 只要有一个点特别出色，这件事情就成功了，其他点的好坏无所谓。\n\u003e  \n\u003e  文艺作品就属于这种情况。你购买了一张专辑，其他的歌曲都不爱听，但是有一首歌你特别喜欢，这张专辑就值得了。电影和小说只要有一个角色或情节特别打动人，作品就成功了。\n\u003e  \n\u003e  风险投资也是这样，只要投了一个特别成功的项目，就能把所有损失补回来。\n\u003e  \n\u003e  最重要的是，**人生就是一个“长板问题”。** 一生中，失败和挫折其实不重要，多少次都不重要，只要有一次大的成功，人生就成功了。\n\u003e  \n\u003e  最大的那一次成功，决定了你一生的成就和高度。很多诺贝尔奖得主，一生就做出了一个重要的科学发现，就足够成为伟大科学家了。\n\u003e  \n\u003e  程序员写过多少代码不重要，只要创造过一个重大影响力的软件，职业生涯就成功了。\n\u003e  \n\u003e  **我们必须学会区分“短板问题”和“长板问题”，它们的解决方法完全不同。** 短板问题的解决，需要盯着薄弱环节，补齐最短的那块板；长板问题的解决，只需要推进最强的环节，不要在乎别的。\n\u003e  \n\u003e  人生不必在乎那些不重要的事情，没必要为了挫折和拒绝而沮丧，都会过去的。你要做的是向前看，拼命争取一次大的成功，让它足够大、更大，只要一次就够了。\n\n\n--- \n\n\n\u003e [!quote] \n\u003e  [《世界运作的几种方式》](https://collabfund.com/blog/one-big-web-a-few-ways-the-world-works/)\n\u003e  \n\u003e  金钱就像疫苗，它可以避免很多痛苦，但不一定会让你快乐。\n\n\n---\n\n\u003e [!quote] \n\u003e  [推特网友](https://twitter.com/landgren/status/1650054767987548160)\n\u003e  \n\u003e  AI 发展到最后，无非就是两种结果。一种是人类灵魂被证明只是一种基于概率算法的预测机制，另一种是 AI 发生了质变，产生了自主意识，拥有了与人类相似的灵魂。\n\n\n--- \n\n\u003e [!quote] \n\u003e From [废柴责任有限公司-高考失利也莫得事 ](https://www.bilibili.com/video/BV1Uz4y1J7cQ/?spm_id_from=333.999.0.0\u0026vd_source=c47136abc78922800b17d6ce79d6e19f)\n\u003e \n\u003e 都说人生是长跑，但没人说是长跑比赛啊\n\u003e \u003cbr\u003e\n\u003e 只要你经历过，你就已经赢了\n\n---\n\n\u003e [!quote] \n\u003e From [年轻用户对B站不是财富，是诅咒【解构B站2】](https://www.bilibili.com/video/BV1Hz4y1Y7XS/?spm_id_from=333.337.search-card.all.click\u0026vd_source=c47136abc78922800b17d6ce79d6e19f) \n\u003e \n\u003e 我没觉得往左就有问题，我也没觉得生活和审美上的保守主义有问题，我也不觉得举出问题，探讨问题这件事有问题；\n\u003e \n\u003e 我觉得什么有问题\n\u003e \n\u003e 我觉得单一狭隘的价值观是有问题的，我觉得党同伐异、充满戾气的心态是有问题的，我觉得阴阳怪气，玩梗扣帽子把他当成习惯是有问题的，我觉得把自己认识世界的方式当成唯一正确的真理，并且攻击其他任何有不同看法的人，这是有问题的。\n\n\n---\n\n\u003e [!quote] \n\u003e  From [年轻用户对B站不是财富，是诅咒【解构B站2】](https://www.bilibili.com/video/BV1Hz4y1Y7XS/?spm_id_from=333.337.search-card.all.click\u0026vd_source=c47136abc78922800b17d6ce79d6e19f) \n\u003e  \n\u003e  我再给自己反向叠个甲\n\u003e  \n\u003e  我觉得原神是文化输出，科比比詹姆斯厉害，飞盘是个好玩的运动，丁太升是不错的音乐评论人，但是同时华晨宇写的一些歌也挺好，同时我觉得塞尔达这个游戏我根本玩不下去，我不喜欢；而且我还是一个自由主义者，而且还有，我觉得无知不是错，但是愚蠢是一种道德缺陷。\n\n\n--- \n\n\u003e [!quote] \n\u003e From  [限制我们的往往不是方法，而是思维和看问题的角度。](https://www.bilibili.com/video/BV1294y1C72W/?spm_id_from=333.999.0.0\u0026vd_source=c47136abc78922800b17d6ce79d6e19f)\n\u003e \n\u003e \"你不能用导致问题产生的思维去解决问题\"\n\u003e \n\u003e “问题”并不是一个客观存在的东西，它是一种叙事。它建立在提问者过去理解并回应这个世界的视角之上。既然问题是在这个角度下产生的，就无法通过相同的角度解决。\n\u003e \n\n\n--- \n\n\u003e [!quote] \n\u003e [如果财富不能共享，为什么罪恶要平摊？](https://www.bilibili.com/video/BV1Mu4y1d7Jy/?spm_id_from=333.999.0.0\u0026vd_source=c47136abc78922800b17d6ce79d6e19f)\n\n\n--- \n\n\u003e [!quote] \n\u003e From [一个关于买车和游戏机，预算不够的故事](https://www.bilibili.com/video/BV1du4y1m7t1/?spm_id_from=333.999.0.0\u0026vd_source=c47136abc78922800b17d6ce79d6e19f)\n\u003e\n\u003e人会因为逞强失去很多东西的，但是也会因为心软留下很多遗憾，所以既然这次你有的选，那就勇敢一点嘛\n\n\n--- \n\n\u003e [!quote] \n\u003e From [Infantilism as a Norm](https://iq.hse.ru/en/news/219491658.html)\n\u003e\n\u003e Views on human age need to be revisited. The value of adulthood as a period of certainty has declined for many, which means that this period is being delayed. The processes of personality development vary, and adults are preserving signs of infantilism.\n\n\n \n ","lastmodified":"2023-09-18T02:45:48.457257029Z","tags":null},"/%E6%96%87%E5%AD%A6/%E6%96%87%E5%AD%A6_MOC":{"title":"文学","content":"\n  \u003cstyle\u003e\n    p {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n\n    a {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n  \u003c/style\u003e\n\nIn this MOC, it shows you the path to what I record for some interesting sentences, including Chinese and English, even Japanese.\n\n[🌌句子](文学/句子/句子.md)\n\n[📜原创诗](文学/poem/Poem_by_me.md)","lastmodified":"2023-09-18T02:45:48.465257122Z","tags":null},"/%E6%96%87%E5%AD%A6/poem/2018":{"title":"Poem in 2018","content":"\n  \u003cstyle\u003e\n    p {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n\n    a {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n  \u003c/style\u003e\n\n\u003e [!quote] \n\u003e 短诗四\n\u003e \u003cbr\u003e\n\u003e 无人寻找惆怅\u003cbr\u003e\n\u003e 但惆怅裹挟了我\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e 孤独，\u003cbr\u003e\n\u003e 是没有反义词的\u003cbr\u003e\n\u003e 至多，也只算是近义词\u003cbr\u003e\n\u003e 譬如热闹\u003cbr\u003e\n\u003e 背后\u003cbr\u003e\n\u003e 是无数个孤独\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e 我咀嚼快乐\u003cbr\u003e\n\u003e 却尝到痛苦\n\n![](文学/poem/attachments/961eb44f141fa0d8e7598e910110d1c.jpg)\n\n\n--- \n\n\u003e [!quote] \n\u003e  短诗三\n\u003e  \u003cbr\u003e\n\u003e  你或许已经看穿了我的虚伪\u003cbr\u003e\n\u003e  但你不知道\u003cbr\u003e\n\u003e  我有两层面具\u003cbr\u003e\n\u003e  一层给你看\u003cbr\u003e\n\u003e  一层给别人看\n\n![](文学/poem/attachments/QQ图片20230612132828.jpg)\n\n\n--- \n\n\u003e [!quote] \n\u003e 剑客\n\u003e \u003cbr\u003e\n\u003e 剑客本来不喝酒\u003cbr\u003e\n\u003e 只是他的心和剑\u003cbr\u003e\n\u003e 一样锋利\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e 江湖的剑客\u003cbr\u003e\n\u003e 或许在决斗中，逢凶化吉\u003cbr\u003e\n\u003e 可他始终赢不了\u003cbr\u003e\n\u003e 推杯换盏的酒局\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e \"十年磨一剑\"\u003cbr\u003e\n\u003e 是贾岛的剑客\u003cbr\u003e\n\u003e \n\u003e 十年，\u003cbr\u003e\n\u003e 对剑客太长\u003cbr\u003e\n\u003e 他早就一个人\u003cbr\u003e\n\u003e 在无人懂他的客栈里\u003cbr\u003e\n\u003e 醉了\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e 他的心，和他的剑\u003cbr\u003e\n\u003e 在酒中，\u003cbr\u003e\n\u003e 绣了\n\n\n ![](文学/poem/attachments/050be4498ef68507f851d3c8faa3751.jpg)\n\n\n--- \n\n\u003e [!quote] \n\u003e 在深巷开一家店\n\u003e \u003cbr\u003e\n\u003e 在深巷开一家店\u003cbr\u003e\n\u003e 外观奇特的它\u003cbr\u003e\n\u003e 看上去那么不同\u003cbr\u003e\n\u003e 用舶来的文字做店名\u003cbr\u003e\n\u003e 充满异域的味道\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e 用昂贵的书籍装饰角落\u003cbr\u003e\n\u003e 请名叫津宁的画家替我\u003cbr\u003e\n\u003e 雕绘走廊\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e 雇流浪的诗人\u003cbr\u003e\n\u003e 调配他最擅长的\u003cbr\u003e\n\u003e 暮光红色的酒\u003cbr\u003e\n\u003e 收留失群的精灵\u003cbr\u003e\n\u003e 在深夜\u003cbr\u003e\n\u003e 唱出回家的忧伤\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e 曾经我有世界\u003cbr\u003e\n\u003e 现在我有这家店\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e 深巷的店\u003cbr\u003e\n\u003e 一定是吉普赛人的风格\u003cbr\u003e\n\u003e 希望云游的客人\u003cbr\u003e\n\u003e 将它的名字\u003cbr\u003e\n\u003e 传播四方\u003cbr\u003e\n\u003e 传到她的远方\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e 我心爱的姑娘\u003cbr\u003e\n\u003e 只有她\u003cbr\u003e\n\u003e 知道那些舶来文字背后\u003cbr\u003e\n\u003e 悄悄地过往\n\n![](文学/poem/attachments/843fe68324e3795eab897988998a553.jpg)\n\n\n\n","lastmodified":"2023-09-18T02:45:48.449256937Z","tags":null},"/%E6%96%87%E5%AD%A6/poem/2022":{"title":"Poem in 2022","content":"\n  \u003cstyle\u003e\n    p {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n\n    a {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n  \u003c/style\u003e\n\n\u003e [!quote] \n\u003e 猫猫喜欢老鼠\n\u003e \u003cbr\u003e\n\u003e  猫猫喜欢老鼠\u003cbr\u003e\n\u003e  老鼠也喜欢猫猫\u003cbr\u003e\n\u003e  但老鼠对未来充满担忧\u003cbr\u003e \n\u003e  就算猫猫的保证也没用\u003cbr\u003e \n\u003e  后来老鼠不辞而别\u003cbr\u003e \n\u003e  猫猫哭的很大声\u003cbr\u003e \n\u003e  猫猫脱下面具\u003cbr\u003e \n\u003e  它只是个胆小的老鼠\u003cbr\u003e\n\u003e  \n\n--- \n\n\u003e [!quote] \n\u003e 小赵不会断舍离\n\u003e \u003cbr\u003e\n\u003e 小赵不会断舍离\u003cbr\u003e\n\u003e 他不懂\u003cbr\u003e\n\u003e 为什么对面可以走的这么干净\u003cbr\u003e\n\u003e 他还傻傻顶着情头\u003cbr\u003e\n\u003e 发霉\u003cbr\u003e\n\u003e \n\n---\n\n\u003e [!quote] \n\u003e 悲伤\n\u003e \u003cbr\u003e\n\u003e 悲伤或许是个好东西\u003cbr\u003e\n\u003e 它给予诗人灵感\u003cbr\u003e\n\u003e 赋予深度\u003cbr\u003e\n\u003e 但我不想要\u003cbr\u003e\n\u003e 我只想要\u003cbr\u003e\n\u003e 她的回头\n\n--- \n\n \u003e [!quote] \n\u003e 她要下船\n\u003e \u003cbr\u003e\n\u003e  到了她的岛\u003cbr\u003e\n\u003e  请不要流泪挽留\u003cbr\u003e\n\u003e  笑着告别 .\u003cbr\u003e\n\u003e  .. ...\u003cbr\u003e\n\u003e \u003cbr\u003e \n\u003e  船开了\u003cbr\u003e \n\u003e  风中有泪滴 ... ...\u003cbr\u003e \n\u003e  \u003cbr\u003e\n\u003e  你没走，因为没有下座岛的指针\n\n\n","lastmodified":"2023-09-18T02:45:48.449256937Z","tags":null},"/%E6%96%87%E5%AD%A6/poem/2023":{"title":"Poem in 2023","content":"\n  \u003cstyle\u003e\n    p {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n\n    a {\n        font-family: Source Sans Pro, SimSun;\n        font-variant-east-asian: traditional;\n    }\n  \u003c/style\u003e\n\n\u003e [!quote] \n\u003e 清单\n\u003e \u003cbr\u003e\n\u003e 他有一个清单\u003cbr\u003e\n\u003e 没那么清楚，也没那么模糊\u003cbr\u003e\n\u003e 清单都打上了勾\u003cbr\u003e\n\u003e 那就是表白地时候\u003cbr\u003e\n\u003e 他想\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e 动物园，\u003cbr\u003e\n\u003e 海洋馆，\u003cbr\u003e\n\u003e 电影院，\u003cbr\u003e\n\u003e ... ...\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e 清单的初稿一个一个被勾上\u003cbr\u003e\n\u003e 只剩下表带的选项\u003cbr\u003e\n\u003e 日子还长\u003cbr\u003e\n\u003e 他想\u003cbr\u003e\n\u003e 悄悄在心里把清单又延长\n\n\n\n--- \n\n\n \u003e [!quote] \n\u003e 空气\n\u003e \u003cbr\u003e\n\u003e 空气有味道，也有形状\u003cbr\u003e\n\u003e 我看过\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e 尴尬的空气\u003cbr\u003e\n\u003e 冷漠的空气\u003cbr\u003e\n\u003e 暧昧的空气\u003cbr\u003e\n\u003e ... ...\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e 其实我会，你也会\u003cbr\u003e\n\u003e 阅读空气\u003cbr\u003e\n\u003e \u003cbr\u003e\n\u003e 只是有时，\u003cbr\u003e\n\u003e 装作不会😉\n\n\n ","lastmodified":"2023-09-18T02:45:48.449256937Z","tags":null},"/%E6%96%87%E5%AD%A6/poem/Poem_by_me":{"title":"My Poem","content":"\n* [2018](文学/poem/2018.md)\n* [2022](文学/poem/2022.md)\n* [2023](文学/poem/2023.md)\n\n\n\n","lastmodified":"2023-09-18T02:45:48.449256937Z","tags":null},"/Math/MOC":{"title":"Math MOC","content":"\n# Statistics\n\n## Basic concept\n\n* [Quantile](Math/Statistics/Basic/Quantile.md)\n\n# Discrete mathematics\n\n## Set theory\n\n* [Cantor Expansion](Math/discrete_mathematics/set_theory/cantor_expansion/cantor_expansion.md)","lastmodified":"2023-09-18T02:45:46.969239843Z","tags":null},"/Math/Statistics/Basic/Quantile":{"title":"Quantile","content":"\n**分位数**（英语：Quantile），亦称**分位点**，是指用分割点（cut point）将一个随机变量的概率分布范围分为几个具有相同概率的连续区间。分割点的数量比划分出的区间少1，例如3个分割点能分出4个区间。\n\n常见的分位数包括中位数（二分位数）、四分位数（四分位数）和百分位数。\n\n1.  中位数：中位数是将一组数据按照大小排序后，处于中间位置的值。将数据分成两部分，有一半的观察值小于中位数，另一半的观察值大于中位数。\n    \n2.  四分位数：四分位数将数据分成四个等分，分别是下四分位数（25%分位数）、中位数（50%分位数）和上四分位数（75%分位数）。下四分位数是将数据排序后，处于25%位置的值；中位数是处于50%位置的值；上四分位数是处于75%位置的值。\n    \n3.  百分位数：百分位数将数据分成100个等分，可以表示某个特定百分比处的数据值。例如，75%的百分位数表示将数据排序后，处于75%位置的值","lastmodified":"2023-09-18T02:45:46.969239843Z","tags":null},"/Math/discrete_mathematics/set_theory/cantor_expansion/cantor_expansion":{"title":"Cantor expansion","content":"\n\n康托展开（Cantor expansion），也称为康托编码（Cantor encoding），是由德国数学家乔治·康托（Georg Cantor）于19世纪末提出的一种数学技术。它用于将**一个无限序列的数字（或有限序列的数字）映射到一个唯一的实数，从而实现序列的编码和排序。**\n\n# Objective\n\n康托展开与逆展开是*将全排列和它的字典序互相转化*的两种算法\n\n# Application\n\n* 作为枚举问题的hash function\n\n# Step by Step\n\n## Deriving Cantor Expansion\n\n### Lemma1 and Lemma2\n\n以DFS生成的4阶全排列为例~~*(no this algorithm detail here)*~~，带编号：\n\n```text\n0   1 2 3 4\n1   1 2 4 3\n2   1 3 2 4\n3   1 3 4 2\n4   1 4 2 3\n5   1 4 3 2\n6   2 1 3 4\n7   2 1 4 3\n8   2 3 1 4\n9   2 3 4 1\n10  2 4 3 1\n11  2 4 3 1\n12  3 1 2 4\n13  ...\n```\n\n\n可以发现，首位为 1 的全排列表示的数全部在区间 [0,5] ；首位为 2 的全排列全部在区间 [6,11]；首位为 3 的则在 [12,17] ；4 的在 [18,23] 。因为首位有 4 种取值的可能，所以把所有的 4 阶全排列划分成了 4 个长度为 $\\frac{4!}{4}=3!=6$ 的区间，首位为 1 的处在第 1 个这样长为 6 的区间，首位为 2 的处在第 2 个，首位为 3 的处在第 3 个……\n\n\u003e [!Lemma1] \n\u003e 衍生到一般情况，对于首位为$k$的$n$阶全排列，它所在的区间为：$[(k-1) \\times (n-1)!,\\quad k \\times (n-1)!]$ \n\n在确定大致范围后，如何定位到具体的编号呢？\n\n观察遮住第一位的情况：\n\n```text\n0   X 2 3 4  \u003c==\u003e  1 2 3\n1   X 2 4 3  \u003c==\u003e  1 3 2\n2   X 3 2 4  \u003c==\u003e  2 1 3\n3   X 3 4 2  \u003c==\u003e  2 3 1\n4   X 4 2 3  \u003c==\u003e  3 1 2\n5   X 4 3 2  \u003c==\u003e  3 2 1\n6   X 1 3 4  \u003c==\u003e  1 2 3\n7   X 1 4 3  \u003c==\u003e  1 3 2\n8   X 3 1 4  \u003c==\u003e  2 1 3\n9   X 3 4 1  \u003c==\u003e  2 3 1\n10  X 4 3 1  \u003c==\u003e  3 1 2\n11  X 4 3 1  \u003c==\u003e  3 2 1\n12  X 1 2 4  \u003c==\u003e  1 2 3\n13  ...\n```\n\n观察上表我们发现，**只考虑元素间的相对大小关系**（或者说各个数字——表示相对大小的符号，之间的相对大小关系），*遮掉首位的 4 阶全排列可以认为就是 3 阶全排列*，只不过它们使用的数字（表示大小的符号）不同。\n\n\u003e [!Lemma2] \n\u003e 所以我们推导$n$阶全排列对应的$(n-1)$阶全排列，如上面所示，去掉首位后，需要对每个能与**首位构成顺序的数字**(*即，比首位数字大的数*)自减少1 \n\n### Calculate Series index by Lemmas\n\n对于任意序列，迭代使用引理1和引理2就可以得到它的index。\n\n#### Step\n1. 利用**引理 1**确定与它同阶同首位的全排列表示的数字的范围，取左边界累加到结果上\n2. 利用**引理 2**将$n$阶全排列转化为$(n-1)$阶全排列\n3. 得到1阶全排列前，重复1，2；得到1阶全排列后输出结果；\n\n#### Example\n\n$$\n35142 \\rightarrow 3\\dot{5}1\\dot{4}2 \\rightarrow 34132 \\rightarrow 341\\dot{3}\\dot{2} \\rightarrow 34121\n$$\n\n$$\nindex = (3-1) \\times 4! + (4-1) \\times 3! + (1-1) \\times 2! + (2-1) \\times 1! = 67\n$$\n\n## Definition\n\n\u003e [!hint] \n\u003e  顺序对是由在两个在序列中的元素组成的有序对，它前项在序列中的位置比后项靠前，且前项小于后项。\n\n$a_{1\\cdots n}$表示一个n阶的全排列，$a_i$表示这个全排列的$i$的数字，定义$a_{1\\cdots n}$的退位序列为$b_{1\\cdots n}$, $b_j$等于$a_j$在全排列中作顺序对后项的顺序对个数，形式为:\n$$\n\\forall \\ 1 \\leq j \\leq n, b_j = |\\{(a_i, a_j) \\ | \\ 1 \\leq i \\leq j \\  \\text{and} \\ a_i \\leq a_j\\}|\n$$\n\n其康托展开公式为：\n$$\nF(a_{1\\cdots n}) = \\sum_{i=1}^n (a_i-b_i-1)\\times(n-i)!\n$$\n\n\n# Code \n\n## Method 1\n\n直接用定义写出，但是不生成$b$序列，只在用到时求当时的$b_i$\n\n```python\nclass CantorExpansion():\n    def cantor_encode(self, s:list) -\u003e int:\n\n        '''\n        Encode a list of integers to a single integer using Cantor expansion.\n        '''\n\n        count  = 0\n\n        for i in range(len(s)):\n            count += self.factorial(len(s) - i - 1) * (s[i] - self.count_smaller(s, i) - 1)\n\n        return count\n\n    def factorial(self, x:int) -\u003e int:\n        if x == 1 or x == 0:\n            return 1\n        else:\n            return self.factorial(x - 1) * x\n        \n    def count_smaller(self, s:list, i:int) -\u003e int:\n        count = 0\n        for j in range(i):\n            if s[j] \u003c s[i]:\n                count += 1\n        return count\n```\n\npython file goto: [cantor_expansion.py](https://github.com/PinkR1ver/Jude.W-s-Knowledge-Brain/blob/master/Math/discrete_mathematics/set_theory/cantor_expansion/code/cantor_expansion.py)\n\n复杂度会是$\\varTheta(n^2)$\n\n## Method 2\n\n再次明确顺序对的概念：\n\n\u003e [!tip]\n\u003e  顺序对是指数组中的一对元素arr[i]和arr[j]，其中i \u003c j且arr[i] \u003e arr[j]\n\n遍历每一个数的时候，需要计算`count_smaller(s,i)`，因此复杂度被提升到$\\varTheta(n^2)$。\n\n其实`count_smaller(s,i)`这个方法的目的就是计算数组的顺序对，提高计算数组顺序对的高速算法可以提高算法的复杂度。\n\n树状数组（Fenwick Tree）是一种用于高效计算[前缀和（Prefix Sum）](tmp_script/prefix_sum.md)的数据结构，它可以在$O(\\log{n})$的时间复杂度内完成前缀和的计算和更新操作。\n\n### Trick - 使用树状数组求顺序对的详细步骤\n\nStep 1: 离散化 为了方便处理，我们首先对值域数组进行离散化处理，将其转化为一个以0为起始索引的连续整数数组。离散化的目的是将原始的值域映射到一个连续的范围内，以便于在树状数组中使用。\n\nStep 2: 初始化树状数组 创建一个长度为n+1的树状数组bit，并将所有元素初始化为0。这个额外的元素bit[0]不会被使用，我们只是为了方便计算。\n\nStep 3: 统计顺序对 从右往左遍历离散化后的值域数组arr，对于每个元素arr[i]，我们需要统计在其左侧且比它大的元素的个数。\n\n在树状数组中，我们可以通过查询前缀和的方式来计算某个位置的值。因此，对于当前的元素arr[i]，我们查询树状数组中索引为arr[i]的前缀和，得到的结果就是arr[i]左侧比它大的元素的个数。\n\nStep 4: 更新树状数组 在统计完当前元素的顺序对后，我们需要更新树状数组，以便下一次查询能够正确计算前缀和。具体操作如下：\n\n- 在树状数组中，将索引为arr[i]的位置的值加1，表示arr[i]的出现次数加1。\n- 重复上述操作，直到遍历完所有元素。\n\nStep 5: 计算总顺序对数 完成整个遍历后，树状数组中的每个位置的值表示该值在原始数组中的出现次数。通过查询树状数组的前缀和，我们可以计算出总的顺序对数。\n\n--- \n\n利用上述使用树状数组求顺序对的算法就可以将Cantor Expansion复杂度降低到$\\varTheta(n\\log{n})$\n\n\n# Inverse Cantor Expansion\n\n逆康托展开的思想是用引理1去定位每一个位置\n\nExample:\n\n`inverse_cantor_expansion(n=5, x=96)`:\n\n* Step 1. 如果是字典序从1开始，则(x - 1) = 95，说明在这个数已经有95个数\n* Step 2. floor(95 / (n-1)!) = floor(95 / 4!) = 3，说明有3个数比第一位小，所以第一位被定位为4，余数为23\n* Step 3. 剩下数字被23定位，floor(23 / 3!) = 3，余数为5，说明有3个数比第二位小，被定位为4，但是4已经出现过，因此是5 \n* Step 4. 剩下的数字用5定位，floor(5 / 2!) = 2，余数为1，说明有2个数比第三位小，被定位为3。\n* Step 5. 同理，剩下第四位被定位为2，最后一位被定位为1\n\n# Generalized Cantor Expansion\n\nTODO ... ...\n\nGeneralized Cantor Expansion可能并不能满足双射条件\n\n# Reference\n\n* ChatGPT\n* [“【给初心者的】康托展开.” 知乎专栏, https://zhuanlan.zhihu.com/p/39377593. Accessed 6 July 2023.](https://zhuanlan.zhihu.com/p/39377593)\n\n","lastmodified":"2023-09-18T02:45:46.969239843Z","tags":null},"/Photography/Aesthetic/Landscape/Landscape_MOC":{"title":"Landscape Photography MOC","content":"\n* [🌊Sea MOC](Photography/Aesthetic/Landscape/Sea/Sea_MOC.md)","lastmodified":"2023-09-18T02:45:46.969239843Z","tags":null},"/Photography/Aesthetic/Landscape/Sea/%E8%B1%8A%E5%B3%B6_Instagram_shiifoncake":{"title":"豊島🏝","content":"![](Photography/Aesthetic/Landscape/Sea/attachments/shiifoncake_338949220_771246770941652_287141902256013940_n.jpg)\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/shiifoncake_339164445_155642070453847_6842139942547564019_n%20(1).jpg)\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/shiifoncake_339164445_155642070453847_6842139942547564019_n.jpg)\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/shiifoncake_338803198_1141886276488589_5464974698780309052_n%20(1).jpg)\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/shiifoncake_338803198_1141886276488589_5464974698780309052_n.jpg)\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/shiifoncake_338758486_601356648715316_3737336679741136784_n.jpg)\n\n\n# Reference\n\n* (https://www.instagram.com/p/Cqh4Ci8vV5u/)[https://www.instagram.com/p/Cqh4Ci8vV5u/]","lastmodified":"2023-09-18T02:45:47.125241645Z","tags":null},"/Photography/Aesthetic/Landscape/Sea/Fujifilm_Blue_by_%E5%B0%8F%E7%BA%A2%E4%B9%A6_Philips%E8%B0%A2%E9%AA%8F":{"title":"Sea in Fujiflm Blue","content":"\n![](Photography/Aesthetic/Landscape/Sea/attachments/Pasted%20image%2020230420014349.png)\n\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/Pasted%20image%2020230420014354.png)\n\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/Pasted%20image%2020230420014401.png)\n\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/Pasted%20image%2020230420014613.png)\n\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/Pasted%20image%2020230420014622.png)\n\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/Pasted%20image%2020230420014634.png)\n\n# Reference\n\n* [太绝了！我拍出了富士蓝！- 小红书，Philips谢骏](https://www.xiaohongshu.com/user/profile/6272c025000000002102353b/641299a200000000130129bb)\n\n","lastmodified":"2023-09-18T02:45:46.969239843Z","tags":null},"/Photography/Aesthetic/Landscape/Sea/Sea_MOC":{"title":"🌊Sea MOC","content":"\n* [Fujifilm Blue🌊, 小红书-Philips谢骏](Photography/Aesthetic/Landscape/Sea/Fujifilm_Blue_by_小红书_Philips谢骏.md)\n* [豊島🏝, Instagram-shiifoncake](Photography/Aesthetic/Landscape/Sea/豊島_Instagram_shiifoncake.md)","lastmodified":"2023-09-18T02:45:46.969239843Z","tags":null},"/Photography/Aesthetic/Polaroid/Polaroid_aesthetic_MOC":{"title":"Polaroid Aestheic MOC","content":"\n* [🖼How to show Polaroid photo in a great way](Photography/Aesthetic/Polaroid/Polaroid_showcase.md)","lastmodified":"2023-09-18T02:45:47.125241645Z","tags":null},"/Photography/Aesthetic/Polaroid/Polaroid_showcase":{"title":"How to show Polaroid photo in a great way","content":"\n\n\n![](Photography/Aesthetic/Polaroid/attachments/IMG_5330.jpg)\n\n\n\n![](Photography/Aesthetic/Polaroid/attachments/IMG_5329.jpg)\n\n\n\n![](Photography/Aesthetic/Polaroid/attachments/IMG_5327.jpg)\n\n\n\n![](Photography/Aesthetic/Polaroid/attachments/IMG_5334.jpg)\n\nCredits to  [比扫描仪更easy的宝丽来翻拍解决方案 -BonBon的Pan](https://www.xiaohongshu.com/user/profile/6272c025000000002102353b/6331af53000000001701acfd)","lastmodified":"2023-09-18T02:45:47.125241645Z","tags":null},"/Photography/Aesthetic/Portrait/Flower_and_Girl":{"title":"🌸Flower \u0026 Girl","content":"\nCredits to [Marta Bevacqua](https://www.martabevacquaphotography.com/), \nThanks🌸\n\n![](Photography/Aesthetic/Portrait/attachments/14.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/15.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/16.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/17.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/18.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/19.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/20.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/21.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/22.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/content%20(1).jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/content%20(2).jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/content%20(3).jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/content%20(4).jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/content%20(5).jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/content%20(6).jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/content%20(7).jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/content%20(8).jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/content%20(9).jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/content%20(11).jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/content%20(12).jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/content.jpg)\n\n","lastmodified":"2023-09-18T02:45:47.129241691Z","tags":null},"/Photography/Aesthetic/Portrait/From-Korean-MV-Todays_Mod":{"title":"Cute Portrait from Korean MV \u003cToday's Mood\u003e","content":"\nCredits to [MV - CHEEZE(치즈) _ Today's Mood(오늘의 기분)](https://www.youtube.com/watch?v=zRq_DlEzygk),\nThanks\n\nAlso, I see this in [摄影灵感｜那有一点可爱 - by   \n小八怪](https://www.xiaohongshu.com/explore/63f0a27e0000000013002b05)\n\n![](Photography/Aesthetic/Portrait/attachments/photo_4_2023-03-27_23-53-20.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/photo_5_2023-03-27_23-53-20.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/photo_6_2023-03-27_23-53-20.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/photo_7_2023-03-27_23-53-20.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/photo_8_2023-03-27_23-53-20.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/photo_9_2023-03-27_23-53-20.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/photo_1_2023-03-27_23-53-20%201.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/photo_2_2023-03-27_23-53-20%201.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/photo_3_2023-03-27_23-53-20%201.jpg)\n\n![](Photography/Aesthetic/Portrait/attachments/photo_2023-03-27_23-55-45.jpg)","lastmodified":"2023-09-18T02:45:47.129241691Z","tags":null},"/Photography/Aesthetic/Portrait/Portrait_MOC":{"title":"👧Portrait","content":"\n* [🌸Flower \u0026 Girl](Photography/Aesthetic/Portrait/Flower_and_Girl.md)\n* [👧🇰🇷Cute Portrait from Korean MV \u003cToday's Mood\u003e](Photography/Aesthetic/Portrait/From%20Korean%20MV%20Todays_Mod.md)\n","lastmodified":"2023-09-18T02:45:47.129241691Z","tags":null},"/Photography/Aesthetic/Style/Grainy_Green":{"title":"Grainy Green","content":"\n![](Photography/Aesthetic/Style/attachments/cinematicshine_326914596_601425291912114_4038822895364546166_n.jpg)\n\n\n![](Photography/Aesthetic/Style/attachments/cinematicshine_341207739_637183131584785_7839745357939483631_n.jpg)\n\n\n# Reference\n\n* [https://www.instagram.com/p/CrGoBoeo8NF/](https://www.instagram.com/p/CrGoBoeo8NF/)","lastmodified":"2023-09-18T02:45:47.145241876Z","tags":null},"/Photography/Aesthetic/Style/Style_MOC":{"title":"☝Style","content":"\n* [🌅Warmth - Nguan](Photography/Aesthetic/Style/Warmth_by_Nguan.md)\n* [📗 Grainy Green](Photography/Aesthetic/Style/Grainy_Green.md)\n","lastmodified":"2023-09-18T02:45:47.145241876Z","tags":null},"/Photography/Aesthetic/Style/Warmth_by_Nguan":{"title":"🎈Warmth - Nguan","content":"\nCredits to [Nguan](https://www.instagram.com/_nguan_/)\n\n\n![](Photography/Aesthetic/Style/attachments/167396766_118928406833773_7462235788758622009_n.jpg)\n\n![](Photography/Aesthetic/Style/attachments/275801921_507726407459443_2779968335661218284_n.jpg)\n\n![](Photography/Aesthetic/Style/attachments/275101252_116346090976633_4116581661408205933_n.jpg)\n\n\n![](Photography/Aesthetic/Style/attachments/152391470_356387755409221_8144178651765781801_n.jpg)\n\n\n![](Photography/Aesthetic/Style/attachments/153386473_426909131936316_8535520818773302544_n.jpg)\n\n\n![](Photography/Aesthetic/Style/attachments/156216827_337435770999537_8250898900544979316_n.jpg)\n\n\n","lastmodified":"2023-09-18T02:45:47.145241876Z","tags":null},"/Photography/Basic/MTF_Curve":{"title":"Modulation transfer function(MTF) Curve","content":"\n有很多因素影响lens performance：\n\n* diffraction\n* optical aberrations\n* design criteria and philosophy\n* manufacturing tolerances and errors\n\n一般，可以用MTF Curve来作为一个标准来衡量lens performance\n\n\u003e [!abstract] \n\u003e 本篇笔记会从摄影角度浅浅了解MTF曲线，而不从物理光学角度分析 \n\n# What is MTF Curve\n\n\n调制传递函数 (MTF) 曲线是一种信息密集型指标(information-dense metric)，反映了镜头如何*将对比度再现为空间频率（分辨率）的函数*。MTF Curve在一组设定好的基础参数下，提供一个composite view，关于光学像差([**optical aberrations**](Physics/Optical/optical_abberation.md))如何影响镜头性能。\n\n通过MTF图，我们可以知道\n\n1. 分辨率, (*代表着镜头对细节的表现能力*)\n2. 对比度, (*代表着镜头表现光线亮和暗的能力*)\n3. 色散和横向色差\n4. 像场弯曲\n\n不可以知道：\n\n1. 镜头畸变\n2. 径向色差\n3. 晕影\n4. 眩光\n\n# How to measure MTF Curve\n\n大家应该知道，一个镜头的中心比边缘成像能力要好很多，因此只测试镜头的中心或边缘，是不能代表镜头的好坏的，所以厂家会从中心到边缘，选取多个点进行测试。如下图，尼康的全画幅机器，选取了距离中心5毫米，10mm，15mm，20mm的点测试。如果是APS-C画幅，因为感光元件小，会选取3mm，6mm，9mm，12mm等，不同厂家可能不一样。\n\n![](Photography/Basic/attachments/Pasted%20image%2020230424143258.png)\n\n测试方法一般使用白色背景、黑色直线\n\n![](Photography/Basic/attachments/Pasted%20image%2020230424143425.png)\n\n* **粗线**用来测试**对比度**，粗度为 10 lines/mm\n* **细线**用来测试**分辨率**，粗度为 30 lines/mm\n* 粗细各有两组，一组与半径平行，叫做Sagittal，另一组与半径垂直，叫做Meridonial，这样做主要是为了测试**色散**和**色差**的。\n\n下图的成像质量是越来越差：\n\n![](Photography/Basic/attachments/Pasted%20image%2020230424143543.png)\n\n# How to read MTF curve\n\n![](Photography/Basic/attachments/Pasted%20image%2020230424143711.png)\n\n横坐标代表了到镜头中心的距离，纵坐标代表了对比度和分辨率的值。\n\n最完美的镜头的曲线应该是下面这样的，一条红线一条蓝线，\n\n红线是通过**粗线**测试得到的，代表**对比度**；\n\n蓝线是通过**细线**测试得到的，代表**分辨率**。\n\n![](Photography/Basic/attachments/Pasted%20image%2020230424143940.png)\n\n普通的镜头的曲线应该是下面这样的(红线代表对比度，蓝线代表分辨率)，在中心点，镜头的对比度和分辨率最好，越往边缘越差。\n\n一般来讲，值大于0.9就代表镜头非常优秀，0.7-0.9是优秀，0.5-0.7就是普通，低于0.5就算差了。\n\n注意到线的中级部位有呈波浪状，这表明了镜头的另一个参数素质：像场弯曲(curvature of field)\\\n\n有波浪就代表有像场弯曲，越大就越严重，实际情况一般问题不大。\n\n![](Photography/Basic/attachments/Pasted%20image%2020230424144046.png)\n\n最常见的MTF曲线如图：\n\n![](Photography/Basic/attachments/Pasted%20image%2020230424144112.png)\n\n1. 红线，10lines/mm，也就是上面测试时说的粗线，用来测对比度的，从镜头中心到边缘，数值逐渐降低，表明镜头的对比度从镜头到边缘，逐渐降低。\n2. 分辨率，从中心到边缘逐渐降低\n3. 色散和色差\n\t* 测试时粗细都有两组线吗，一组与半径平行，另一组垂直，用来测试色散和色差，这样就分别得到两条线，与半径平行的一组得到实线，与半径垂直的一组得到虚线。**虚线实线越接近，代表镜头的色散和色差控制的很好，越背离，表示越严重**。\n4. 像场弯曲\n\n# Reference\n\n* [The Modulation Transfer Function (MTF), https://www.edmundoptics.com](https://www.edmundoptics.com/knowledge-center/application-notes/imaging/modulation-transfer-function-mtf-and-mtf-curves/)\n* [MTF 曲线图应该怎么看？, 知乎](https://www.zhihu.com/question/19713211)","lastmodified":"2023-09-18T02:45:47.153241968Z","tags":null},"/Photography/Basic/Saturation":{"title":"Saturation - 饱和度","content":"\nto be written...","lastmodified":"2023-09-18T02:45:47.153241968Z","tags":null},"/Photography/Cameras_Research/Lens_Structure/Lens_Structure_MOC":{"title":"Lens Structure MOC","content":"\n* ","lastmodified":"2023-09-18T02:45:47.153241968Z","tags":null},"/Photography/Cameras_Research/Pocket_film/Pocket_film_camera_MOC":{"title":"Pocket Film camera MOC","content":"\n# Rollei\n\n* [Rollei35](Photography/Cameras_Research/Pocket_film/Rollei_35.md)","lastmodified":"2023-09-18T02:45:47.153241968Z","tags":null},"/Photography/Cameras_Research/Pocket_film/Rollei_35":{"title":"Rollei 35 review","content":"\n\n","lastmodified":"2023-09-18T02:45:47.153241968Z","tags":null},"/Photography/Cameras_Research/Polaroid/Polaroid":{"title":"Polaroid","content":"\n# Polaroid Background\n\n![](Photography/Cameras_Research/Polaroid/attachments/Pasted%20image%2020230330195031.png)\n\nPolaroid是一家成立于1937年的美国相机及照片制造公司，该公司曾经是即时相机市场的领导者。Polaroid公司在20世纪50年代推出了第一台即时相机，并在随后的几十年里不断推出各种型号的即时相机和胶片，成为了全球广泛使用的品牌。\n\nPolaroid最著名的特点之一是它的“即时影像”技术，这种技术可以使用户在拍摄后几秒钟内看到他们所拍摄的照片。Polaroid的即时相机成为了许多人记录重要时刻和创造独特艺术作品的选择。\n\n除了即时相机，Polaroid还生产和销售其他相机、相机附件、数码相框和照片打印机等产品。此外，Polaroid还与其他品牌合作，推出了许多联名款式的相机和其他产品。\n\n在Polaroid成立近90年的历史中，它的相机和胶片已经成为了文化和艺术的象征，并继续影响着人们对摄影和影像创作的认知。\n\n# Polaroid Camera Review\n\n* [Polaroid one600](Photography/Cameras_Research/Polaroid/Polaroid_one600.md)\n* [Polaroid Integral 600 Series](Photography/Cameras_Research/Polaroid/Polaroid_600.md)\n","lastmodified":"2023-09-18T02:45:47.157242014Z","tags":null},"/Photography/Cameras_Research/Polaroid/Polaroid_600":{"title":"Polaroid 600","content":"\n# Reference\n\n* [How do I use my Vintage Polaroid 600 camera? – Retrospekt](https://retrospekt.com/blogs/ask-the-expert/how-do-i-use-my-vintage-polaroid-600-instant-camera)\n* [Polaroid Integral 600 Series - Camera-wiki.org - The free camera encyclopedia](http://camera-wiki.org/wiki/Polaroid_Integral_600_Series)\n","lastmodified":"2023-09-18T02:45:47.157242014Z","tags":null},"/Photography/Cameras_Research/Polaroid/Polaroid_one600":{"title":"Polaroid_one600","content":"\n\n![](Photography/Cameras_Research/Polaroid/attachments/Pasted%20image%2020230330195707.png)\n\n# Specifications\n\n- **(Wide) 100mm lens with minimum focus distance of 3 feet.**\n- **Maximum Aperture F12.9 (Don't know if it can change)**\n- **1/200 s to 1/3 s**\n- **Fixed focus.**\n- Exposure modes - **Program automatic**\n- \"Aerodynamic\" styling (particularly when folded) with downward curve at back.\n- Flash moved to right hand side of user and can be manually switched on and off.\n- Hand grip on right.\n- LCD frame counter.\n- Self-timer.\n\n## Functionally similar models\n\n-   Polaroid One (silver/grey)\n-   Polaroid One600 Job Pro (black/silver/yellow) (Close-focus to 18 inches!)\n-   Polaroid One600 Nero (all black)\n-   Polaroid One600 \"Flowers\" (white with purple and yellow flower design)\n-   Polaroid One600 Panna (white/black)\n-   Polaroid One600 \"Poison Frog\" (silver/grey with yellow/black pattern)\n-   Polaroid One600 Polala 2006 (red/silver with gold Chinese dragon)\n-   Polaroid One600 Pro (all silver) (Like Job Pro, close-focus to 18 inches!)\n-   Polaroid One600 Royksopp (grey/silver with 'Royksopp - Only This Moment' branding)\n-   Polaroid One600 Superheadz Special Edition Red Hat (silver/black, with 'red hat' cartoon character)\n-   Polaroid One600 Rossa (bright red/black)\n-   Polaroid One Rossa (as above)\n-   Polaroid One Ultra (silver/black) (Close focus to 2 feet)\n-   Polaroid Pop Kit (silver/black with stickers for user's customization)\n\n# Reference\n\n* [Polaroid One 600 Camera Review - by Dan Finnen](https://danfinnen.com/review/polaroid-one-600-camera-review/)\n* [Polaroid One600 (Classic) - Camera-wiki.org - The free camera encyclopedia](http://camera-wiki.org/wiki/Polaroid_One600_(Classic))\n","lastmodified":"2023-09-18T02:45:47.157242014Z","tags":null},"/Photography/MoodBoard/Sea_20230428/Sea_20230428":{"title":"🌊Sea - 2023.04.28","content":"\n\n* [idea - reference image](Photography/MoodBoard/Sea_20230428/idea.md)\n","lastmodified":"2023-09-18T02:45:47.157242014Z","tags":null},"/Photography/MoodBoard/Sea_20230428/idea":{"title":"idea - reference image","content":"\n# [Fujifilm_Blue_by_小红书_Philips谢骏](Photography/Aesthetic/Landscape/Sea/Fujifilm_Blue_by_小红书_Philips谢骏.md)\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/Pasted%20image%2020230420014349.png)\n\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/Pasted%20image%2020230420014354.png)\n\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/Pasted%20image%2020230420014401.png)\n\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/Pasted%20image%2020230420014613.png)\n\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/Pasted%20image%2020230420014622.png)\n\n\n![](Photography/Aesthetic/Landscape/Sea/attachments/Pasted%20image%2020230420014634.png)\n\n# [豊島_Instagram_shiifoncake](Photography/Aesthetic/Landscape/Sea/豊島_Instagram_shiifoncake.md)\n\n![](Photography/MoodBoard/Sea_20230428/attachments/shiifoncake_338949220_771246770941652_287141902256013940_n.jpg)\n\n![](Photography/MoodBoard/Sea_20230428/attachments/shiifoncake_339164445_155642070453847_6842139942547564019_n%20(1).jpg)\n\n![](Photography/MoodBoard/Sea_20230428/attachments/shiifoncake_339164445_155642070453847_6842139942547564019_n.jpg)\n\n![](Photography/MoodBoard/Sea_20230428/attachments/shiifoncake_338803198_1141886276488589_5464974698780309052_n%20(1).jpg)\n\n![](Photography/MoodBoard/Sea_20230428/attachments/shiifoncake_338803198_1141886276488589_5464974698780309052_n.jpg)\n\n![](Photography/MoodBoard/Sea_20230428/attachments/shiifoncake_338758486_601356648715316_3737336679741136784_n.jpg)\n\n\n# [寄り道の理由。- Instagram, photono_gen](https://www.instagram.com/p/CrVPFjZvvlr/)\n\n![](Photography/MoodBoard/Sea_20230428/attachments/photono_gen_336060179_2380745882102401_2427706248624984378_n.jpg)","lastmodified":"2023-09-18T02:45:47.16124206Z","tags":null},"/Photography/Photography_MOC":{"title":"Photography - MOC","content":"\n# 🌊Photo Portfolio\nYou can see my photography works in:\n\n* [🎨Slide show](https://pinkr1ver.com/PhotoGallery/)\n* [🌄Photo Collection in Notion](https://www.notion.so/pinkr1ver/3cfdd332b9a94b20bca041f2aa2bdcd2?v=24e696e6ab754386a710bc8e83976357)\n* [🍻Instagram](https://www.instagram.com/jude.wang.yc/?next=%2F)\n* [🧶小红书](https://www.xiaohongshu.com/user/profile/6272c025000000002102353b)\n\n# Notes\nAlso, here's my notes about learning photography\n\n## About Basic Concepts:\n\n* [Saturation](Photography/Basic/Saturation.md)\n\n## Appreciation of other works - about ***aesthetic***\n\n* [👧Portrait](Photography/Aesthetic/Portrait/Portrait_MOC.md)\n* [🏔Landscape](Photography/Aesthetic/Landscape/Landscape_MOC.md)\n* [☝Style](Photography/Aesthetic/Style/Style_MOC.md)\n* [✨Polaroid](Photography/Aesthetic/Polaroid/Polaroid_aesthetic_MOC.md)\n\n## Camera Research\n\n* [✨Polaroid](Photography/Cameras_Research/Polaroid/Polaroid.md)\n* [📷Lens Structure](Photography/Cameras_Research/Lens_Structure/Lens_Structure_MOC.md)\n* [📸Pocket film camera](Photography/Cameras_Research/Pocket_film/Pocket_film_camera_MOC.md)\n\n## Skills I learned\n\n* [How to measure light using Polaroid?](Photography/Skills/Polaroid_light.md)\n* [How to use Moodboard](Photography/Skills/Moodboard.md)\n* [How to show your Polaroid Picture](Photography/Aesthetic/Polaroid/Polaroid_showcase.md)\n\n## Photography story\n\n* [夜爬蛤蟆峰拍Polaroid慢门 - 2023.04.14](Photography/Story/Rainy_evening_hiking_Polaroid.md)\n\n##  Mood Board\n\n* [🌊Sea - 2023.04.28](Photography/MoodBoard/Sea_20230428/Sea_20230428.md)\n\n## Meme\n\n* [Photography meme](Photography/Photography_meme/Photography_meme.md)\n\n\n# Reference\n\n## Platform\n\n* [Magnum Photos](https://www.magnumphotos.com/)\n* [CNU - Catch Next Ultimate](http://www.cnu.cc/)\n\n## Greatest Artist\n\n* [linksphotograph](https://www.linksphotograph.com/)\n* [HAMADA Hideaki / 濱田英明](https://www.hideakihamada.com)\n* [Jason Kummerfeldt](https://graincheck.darkroom.com/) and [his youtube](https://www.youtube.com/@grainydaysss)\n* [Nguan](https://nguan.tv/)\n* [Marta Bevacqua](https://www.martabevacquaphotography.com/)\n* [Sam Zhang](https://www.instagram.com/itscapturedbysam/)\n\n## Content Collector \u0026 Photographer\n\n* [🦺搬运UP主 - 豆腐素包](https://space.bilibili.com/196700312/video)\n* [小八怪 - 小红书](https://www.xiaohongshu.com/user/profile/5558b47f5894463d532a632c)\n\n","lastmodified":"2023-09-18T02:45:47.16124206Z","tags":null},"/Photography/Photography_meme/Photography_meme":{"title":"Photography Meme","content":"\n![](Photography/Photography_meme/attachments/QQ图片20230424193512.png)","lastmodified":"2023-09-18T02:45:47.16124206Z","tags":null},"/Photography/Skills/Moodboard":{"title":"How to use Moodboard","content":"\n# Overview\n\n1. 选题\n2. 风格\n3. 色彩\n4. 服装道具\n5. 模特\n6. 场地\n7. 构图\n8. 布光\n\n# 选题\n\n将参考图放进灵感文件夹\n\n# 风格\n\n在参考图中风格提取，一般可以收集200张参考图\n\n# 色彩\n\n使用[Adobe Color](https://color.adobe.com/)确定色彩方案\n\n\n# 服装道具\n\n略\n\n# 模特\n\n略\n\n# 场地\n\n略\n\n# 构图\n\n使用参考图和手绘\n\n# 布光\n\n略\n\n# Reference\n\n* [要做出完美的拍摄策划，必须知道的8个重点 - 小红书, Tripitaka Wu](https://www.xiaohongshu.com/user/profile/6272c025000000002102353b/62024914000000002103cedf)","lastmodified":"2023-09-18T02:45:47.165242107Z","tags":null},"/Photography/Skills/Polaroid_light":{"title":"How to measure light using Polaroid?","content":"\nThe most thing you need to know is that, **the right exposure is in your head**.\n\n# Basic\n\n\n\n# Practice\n\n\n# Reference\n\n* [How to EXPOSE your POLAROID PICTURE - Youtuber Analog Things](https://www.youtube.com/watch?v=iqU5YRG8WiE)\n\n","lastmodified":"2023-09-18T02:45:47.165242107Z","tags":null},"/Photography/Skills/howToShowPolaroid":{"title":"How to Show Polaroid?","content":"\n* [宝丽来翻拍9宫格](Photography/Aesthetic/Polaroid/Polaroid_showcase.md)","lastmodified":"2023-09-18T02:45:47.165242107Z","tags":null},"/Photography/Story/Rainy_evening_hiking_Polaroid":{"title":"夜爬蛤蟆峰拍Polaroid慢门 - 2023.04.14","content":"\n# Hiking\n\n周五，周潭来杭，计划去蛤蟆峰顶拍拍立得慢门，记录西湖夜景。\n\n晚饭后，雨渐起，兴致不减，亦去。\n\n山底已经在小雨中颇有丁达尔现象的感觉。\n\n![](Photography/Story/attachments/9970714720C0835E6547C263418D551B.jpg)\n\n雨让石头逐渐变得打滑，蛤蟆峰山顶的石头快的攀登会变得非常危险，这一点难以描述，或许你可以问你杭州本地的朋友。周潭在攀登最后一段路程之前摔倒，还好背包缓冲了几乎所有的冲撞，也让他意识到雨天来到这里的危险性，是具有极限运动的底色在的。\n\n最后，在小心翼翼中，登顶了。\n\n# Photographer\n\n在蛤蟆峰顶拍慢门需要一定的三脚架架设技巧和测光技巧，在雨中就显得更加困难。\n\n![](Photography/Story/attachments/QQ视频20230416012046.mp4)\n\n![](Photography/Story/attachments/FCB8B96468D3B459532E010E865D0B99.jpg)\n\n\n经过测光和宝丽来app曝光调整，这次拍摄夜景的计划以$f/22$, 30s shutter speed, i-type film 640 ISO进行拍摄，先看成片效果：\n\n![](Photography/Story/attachments/IMG_5553.jpg)\n\n照片由iPhone 12 mini Polaroid app scanner扫描完成的film -\u003e digital，效果比较一般，但我们能看出，曝光的效果不尽人意。这里的原因认为由以下原因导致：\n* 天气恶劣，空气湿度大，造成光线色散加重\n* 没有考虑i-type相纸**倒易率**，曝光时间不足（重要原因🚧🚧🚧）\n* 没有查询Polaroid now+镜头最好的光学素质的参数，自认为是$f/22$，导致曝光时间过长导致的点光源色散严重。（目前还没有查询到Polaroid now+镜头的光学参数曲线🚧🚧🚧）\n\n同时，那晚还不懂now+ +键的使用导致相纸浪费一张，下面是now+中+键的用法：\n\n![](Photography/Story/attachments/Pasted%20image%2020230416014050.png)\n\n同时，那晚曝光时，有一次光圈不小心打到$f/33$，导致欠曝地更为厉害，其效果大概如下：\n\n![](Photography/Story/attachments/IMG_5550.jpg)\n\n同时要注意的是，Polaroid的曝光时间最多是30s，如果要更长时间的曝光，可以不弹相纸进行二次曝光，但是长曝光30s以上可能效果很差。\n\n## 人像\n\n搞了两张人像，同样的曝光参数$f/22$, 30s shutter speed, i-type film 640 ISO，开了宝丽来闪关灯最大等级：\n\n![](Photography/Story/attachments/IMG_5492.jpg)\n\n\n![](Photography/Story/attachments/IMG_5493.jpg)\n\n第一张人像清晰些，以我个人观点来看，是因为伞造成的反射\n\n# 返程\n\n返程的故事有趣了些，因为三脚架落在了山脚，于是返回去取，但是去取的时候手机落在了打的网约车上。\n\n因为手机丢了，所以无法确认订单的详细信息，也就无法联系司机和客服。\n\n所以用周潭的手机去登录高德打车去确认订单，但是登录高德又需要手机验证码，非常傻逼的设计就是了，还好带了apple watch，连接了周潭的热点后可以同步手机消息收到验证码，这才登上了高德地图。\n\n高德地图的打车是联动多家打车公司的，所以情况错综复杂，我的订单来自天猫出行，高德端不允许我直接联系师傅，同时天猫出行的客服也无法拨通，最后还好拨通了高德客服的电话，联系上了师傅。\n\n师傅当时前往了滨江，所以我们只好在山脚下，也就是保俶路的忠儿面馆那里等待，刚好周潭没有吃饱，起源巧合下，在这也算吃了一顿还算杭州特色的拌川。\n\n![](Photography/Story/attachments/A9A6699D1859851AB1D66131BD1382DC.jpg)\n\n\n# Route\n\n![](Photography/Story/attachments/QQ图片20230417203443.jpg)","lastmodified":"2023-09-18T02:45:47.165242107Z","tags":null},"/Physics/Electromagnetism/Basic/Electric_units":{"title":"Electric Units","content":"# Electrical impedance\n\n$$\nZ = \\sqrt{R^2 + {(X_L-X_C)}^2}\n$$\n\n\n* $Z$ = impedance\n* $R$ = resistance\n* $X_L$  = inductive reactance\n* $X_C$  = capacitive reactance\n\n![](Physics/Electromagnetism/Basic/attachments/Pasted%20image%2020230330163734.png)\n\n**阻抗**是电路中电阻、电感、电容对交流电的阻碍作用的统称。阻抗是一个复数，实部称为**电阻**，虚部称为**电抗**；其中电容在电路中对交流电所起的阻碍作用称为**容抗**，电感在电路中对交流电所起的阻碍作用称为**感抗**，容抗和感抗合称为**电抗**。\n\n阻抗将电阻的概念加以延伸至交流电路领域，不仅描述*电压与电流的相对振幅*，也描述其*相对相位*。当通过电路的电流是直流电时，电阻与阻抗相等，电阻可以视为相位为零的阻抗。\n\n## 形式\n\n1. $R+jX$\n2. $Z_m\\angle\\theta$\n3. $Z_m e^{j\\theta}$\n\n阻抗定义为电压与电流的频域比率。阻抗的大小$Z_{m}$ 是电压振幅与电流振幅的绝对值比率，阻抗的相位 $\\theta$是电压与电流的相位差。\n\n## 欧姆定律\n\n$$\nv = iZ = iZ_m e^{j\\theta}\n$$\n\n阻抗大小$Z_m$的作用恰巧就像电阻，设定电流$i$，就可以计算出阻抗$Z$两端的电压降$v$。相位因子$e^{j\\theta}$则是电流滞后于电压的相位差$\\theta$ \n\n\u003e [!tip] \n\u003e 在时域中，电流信号会比电压信号慢$\\theta T/2\\pi$秒\n\n## 理想的阻抗\n$$\nZ_R = R\n$$\n\n$$\nZ_C = \\frac{1}{j\\omega C}\n$$\n\n$$\nZ_L = j \\omega L\n$$\n\n* 对于电容，交流电压滞后90°于交流电流；\n* 对于电感，交流电压超前90°于交流电流\n\n### 容抗\n\n$$\nX_C = -j/\\omega C\n$$\n随着$\\omega$趋向于0，电源趋向于直流电源，容抗的绝对值趋向于无穷；*因此，在低频率运作时，电容器貌似断路。假设电源的频率越高，则容抗越低，对于电流通过的阻碍也越低。在高频率运作时，电容器貌似短路。*\n\n### 阻抗\n\n$$\nX_L = j\\omega L\n$$\n从这方程可以观察到，当交流电源的角频率趋向于零时，电源会趋向于直流电源，感抗会趋向于零，对于电流的通过阻碍越低。*所以，在低频率运作时，电感器貌似短路。假设电源角频率越高，则感抗越高，假设给定电压源振幅，则电流会趋向于零。所以，在高频率运作时，电感器貌似断路。*\n\n\n# Reference\n\n[电气单位（V，A，Ω，W，...） (rapidtables.org)](https://www.rapidtables.org/zh-CN/electric/Electric_units.html)\n","lastmodified":"2023-09-18T02:45:47.2252428Z","tags":null},"/Physics/Electromagnetism/Electromagnetism_MOC":{"title":"Electromagnetism MOC","content":"\n# Basic\n\n* [Electric units](Physics/Electromagnetism/Basic/Electric_units.md)\n\n## Advanced\n\n* [Maxwell's equation](Physics/Electromagnetism/Maxwells_equation.md)\n\n# Circuit\n\n* [Resonant circuit](Physics/Electromagnetism/Resonant_circuit.md)","lastmodified":"2023-09-18T02:45:47.2252428Z","tags":null},"/Physics/Electromagnetism/Maxwells_equation":{"title":"Maxwell's Equation","content":"\n# Equation\n\n\n$$\n\\nabla \\cdot E = \\frac{\\rho}{\\epsilon_0}\n$$\n\n$$\n\\nabla \\cdot B = 0\n$$\n\n$$\n\\nabla \\times E = -\\frac{\\partial B}{\\partial t}\n$$\n\n$$\n\\nabla \\times B = \\mu_0 (J + \\epsilon_0 \\frac{\\partial E}{\\partial t})\n$$\n\n# Vector field\n\nEssentially a vector field is what you get if you associate each point in space with a vector, some magnitude and direction. Maybe those vectors represent the velocities of particles of fluid at each point in space or maybe they represent the force of gravity at many different points in space or maybe a magnetic field strength.\n\n\u003e [!note] \n\u003e  If you were to draw the vectors to scale, the longer ones end up just cluttering the whole thing, so it's common to basically lie a little and artificially shorten ones that are too long. Maybe using **color to give some vague sense of length**.\n\n![](Physics/Electromagnetism/attachments/Pasted%20image%2020230411151612.png)\n\n## Divergence\n\n![](Physics/Electromagnetism/attachments/my-life.gif)\n\nDivergence $\\cdot$ Vector filed是来衡量在(x, y)点你产生fluid的能力\n\n所以上述图中，产生fluid的source点，他们的Divergence $\\cdot$ Vector filed是positive的\n\n那些fluid流入的sink端，他们的Divergence $\\cdot$ Vector filed就是negative的\n\n![](Physics/Electromagnetism/attachments/Pasted%20image%2020230411155711.png)\n\n同时，如果点可以slow flow in变fast slow out，这个点位的divergence $\\cdot$ vector filed也是positive的\n\n![](Physics/Electromagnetism/attachments/my-life%201.gif)\n\nVector field input point得到的是一个多维的输出，指向一个方向并带有scale；divergence $\\cdot$ vector field，它的输出depends on the behavior of the field in small neighborhood around that point。输出为一个数值，衡量这个point acts as a source or a sink\n\n![](Physics/Electromagnetism/attachments/Pasted%20image%2020230411161346.png)\n\n\u003e [!note] \n\u003e  For actual fluid flow: $\\text{div} F = 0$ everywhere\n\n## Curl\n\n![](Physics/Electromagnetism/attachments/output%202.gif)\n\nCurl是衡量fluid在point被rotate的程度，clockwise方向是positive curl，counterclockwise是negative curl。\n\n![](Physics/Electromagnetism/attachments/curl.gif)\n\n上图中这个点的curl也是非零的，因为fluid上快下慢，result in clockwise influence\n\n## Calculate divergence and curl\n\n$$\n\\text{div} F = \\nabla \\cdot F = \n\\begin{bmatrix}\n\\frac{\\partial}{\\partial x} \\\\\n\\frac{\\partial}{\\partial y}\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\nF_x \\\\\nF_y\n\\end{bmatrix} = \\frac{\\partial F_x}{\\partial x} + \\frac{\\partial F_y}{\\partial y}\n$$\n\n$$\n\\text{curl} F = \\nabla \\times F = \n\\begin{bmatrix}\n\\frac{\\partial}{\\partial x} \\\\\n\\frac{\\partial}{\\partial y}\n\\end{bmatrix} \\times\n\\begin{bmatrix}\nF_x \\\\\nF_y\n\\end{bmatrix}\n= \\frac{\\partial F_y}{\\partial x} - \\frac{\\partial F_x}{\\partial y}\n$$\n\n![](Physics/Electromagnetism/attachments/calculation_result.gif)\n\n### Detail Explanation\n\n![](Physics/Electromagnetism/attachments/Pasted%20image%2020230412144351.png)\n\n![](Physics/Electromagnetism/attachments/Pasted%20image%2020230412144501.png)\n\n在$(x_0, y_0)$微分一个很小的tiny step，会有一个新的vector，它与原有的vector会有一个difference。\n\n![](Physics/Electromagnetism/attachments/div.gif)\n\n$\\text{div} F(x_0, y_0)$其实就是corresponds to $360^\\circ$方向的average的Step $\\cdot$ Difference\n\n可以想象一个source端，它朝四面发射vector，它的Step $\\cdot$ Difference自然就是positive的\n\n![](Physics/Electromagnetism/attachments/Pasted%20image%2020230412145732.png)\n\n同理，不难想象的是，$\\text{curl} F(x_0, y_0)$是corresponds to Step $\\times$ Difference\n\n# Understand Maxwell's Equation\n\n学会vector filed中的divergence和curl，是理解Maxwell’s Equation的关键\n\n## Gauss's Law\n\n$$\n\\text{div} E = \\frac{\\rho}{\\epsilon_0}\n$$\n\n\n![](Physics/Electromagnetism/attachments/Pasted%20image%2020230411163735.png)\n\n* $\\rho$是charge density\n* $\\epsilon_0$是Epsilon Naught，free space的介电常数，它决定free space空间中电场的强度\n\n\u003e [!note] \n\u003e 形象的\n\u003e \n\u003e Gauss's law stating that **divergence of an electric field at a given point is a proportional to the charge density at that point**. \n\u003e \n\u003e **Positively charged regions as acting like sources** of some imagined fluid and n**egatively charged regions as being the sinks** of that fluid.\n\u003e \n\u003e Parts of space where there is on charge the fluid **would be flowing incompressively** just like water.\n\n\n## Gauss's law for magnetism\n\n$$\n\\text{div} B = 0\n$$\n\n![](Physics/Electromagnetism/attachments/Pasted%20image%2020230411165048.png)\n\n磁场的divergence在任意地方为0，说明磁场的fluid是incompressible的，没有source也没有sinks，就像water一样。也有这样的interpretation，说明magnetic monopoles是不存在的\n\n## Maxwell–Faraday equation (Faraday's law of induction)\n\n$$\n\\nabla \\times E = - \\frac{1}{c} \\frac{\\partial B}{\\partial t}\n$$\n\n![](Physics/Electromagnetism/attachments/Pasted%20image%2020230419141438.png)\n\n![](Physics/Electromagnetism/attachments/Pasted%20image%2020230419141637.png)\n## Ampère's circuital law (with Maxwell's addition)\n\n$$\n\\nabla \\times B = \\frac{1}{c} (4\\pi J + \\frac{\\partial E}{\\partial t})\n$$\n\n![](Physics/Electromagnetism/attachments/Pasted%20image%2020230419141737.png)\n\n\n# Maxwells equation explain EM wave\n\nMaxwells的完备对称理论表明，电场力和磁力并不是分开的，而是同一事物——电磁力的不同表现形式。 这种力的经典统一是当前试图统一自然界中四种基本力——引力、电力、强核力和弱核力——的动机之一。\n\nMaxwells从Maxwells equation中预测了EM wave的存在。\n\nMaxwells意识到振荡电荷，就像交流电路中的电荷一样，会产生变化的电场。 他预测这些变化的场会像跳跃的鱼在湖上产生的波浪一样从源头传播。\n\n麦克斯韦预测的波将由振荡电场和磁场组成——定义为电磁波（EM 波）。 电磁波能够对距其源很远的电荷施加力，因此它们可能是可检测的。 Maxwells通过求解Maxwells方程组，可以求出EM的速度$c$，\n\n$$\nc = \\frac{1}{\\sqrt{\\mu_0 \\epsilon_0}} = 3 \\times 10^8 m/s\n$$\n\n电磁波的波段处于无法被肉眼观测的波段，直到Maxwells去世后，才被Hertz用实验证实了电磁波的存在。\n\n![](Physics/Electromagnetism/attachments/Pasted%20image%2020230419155744.png)\n\n# Reference\n\n* [Fun fluid-flow illustrations - by 3B1B](https://anvaka.github.io/fieldplay/?cx=0\u0026cy=0\u0026w=8.5398\u0026h=8.5398\u0026dt=0.01\u0026fo=0.998\u0026dp=0.009\u0026cm=1\u0026vf=%2F%2F%20p.x%20and%20p.y%20are%20current%20coordinates%0A%2F%2F%20v.x%20and%20v.y%20is%20a%20velocity%20at%20point%20p%0Avec2%20get_velocity%28vec2%20p%29%20%7B%0A%20%20vec2%20v%20%3D%20vec2%280.%2C%200.%29%3B%0A%0A%20%20%2F%2F%20change%20this%20to%20get%20a%20new%20vector%20field%0A%20%20v.x%20%3D%20p.y%3B%0A%20%20v.y%20%3D%20%28max%28cos%28sin%28p.y%29%29%2Csin%28p.y%29%2Fp.y%29%2Bp.y%29%3B%0A%0A%20%20return%20v%3B%0A%7D\u0026code=%2F%2F%20p.x%20and%20p.y%20are%20current%20coordinates%0A%2F%2F%20v.x%20and%20v.y%20is%20a%20velocity%20at%20point%20p%0Avec2%20get_velocity%28vec2%20p%29%20%7B%0A%20%20vec2%20v%20%3D%20vec2%280.%2C%200.%29%3B%0A%0A%20%20%2F%2F%20change%20this%20to%20get%20a%20new%20vector%20field%0A%20%20v.x%20%3D%20%28max%28p.x%2Cp.y%29%2Bmax%28p.y%2Cp.x%29%29%3B%0A%20%20v.y%20%3D%20p.y%3B%0A%0A%20%20return%20v%3B%0A%7D)\n* [Divergence and curl: The language of Maxwell's equations, fluid flow, and more - YouTube vedio by 3b1b](https://www.youtube.com/watch?v=rB83DpBJQsE)\n* [Let There Be Light: Maxwell's Equation EXPLAINED for BEGINNERS - YouTube vedio by Parth G](https://www.youtube.com/watch?v=0jW74lrpeM0)\n* [Faraday’s Law - online experiment](https://em.geosci.xyz/content/maxwell1_fundamentals/formative_laws/faraday.html)\n* [# Maxwell’s Equations- Electromagnetic Waves Predicted and Observed](https://phys.libretexts.org/Bookshelves/College_Physics/Book%3A_College_Physics_1e_(OpenStax)/24%3A_Electromagnetic_Waves/24.01%3A_Maxwells_Equations-_Electromagnetic_Waves_Predicted_and_Observed)","lastmodified":"2023-09-18T02:45:47.2252428Z","tags":null},"/Physics/Electromagnetism/Q_factor":{"title":"Q factor","content":"\n# Explanation\n\nIn physics and engineering, the quality factor or Q factor is a **dimensionless** parameter that describes how **underdamped** an oscillator or *resonator* is. It is defined as the ratio of the initial energy stored in the resonator to the *energy lost* in one radian of the cycle of oscillation. Q factor is alternatively defined as the ratio of a *resonator's center frequency to its bandwidth* when subject to an oscillating driving force. These two definitions give *numerically similar*, but not identical, results. \n\n\u003e [!tip] \n\u003e  高Q因子表示振子能量损失的速率较慢，振动可持续较长的时间; 单摆在空气中Q因子较高而在油中较低\n\n\n\n![](Physics/Electromagnetism/attachments/Pasted%20image%2020230404144801.png)\u003cfont size=1\u003eFig. A damped oscillation. A low Q factor – about 5 here – means the oscillation dies out rapidly.\u003c/font\u003e\n\n\nQ因子较高的振子在共振时，在共振频率附近的**振幅较大**，但会产生的共振的**频率范围比较小**，此频率范围可以称为频宽。\n\n例如一台无线电接收器内的调谐电路Q因子较高，要调整接收器对准一特定频率会比较困难，但其选择性较好，在过滤频谱上邻近电台的讯号上也有较佳的效果。\n\n系统的Q因子可能会随著应用场合及需求的不同而有大幅的差异。*强调阻尼特性的系统*（例如[防止门突然关闭的阻尼器](warehouse/dampers_keeping_a_door_from_slamming%20shut.md)）*其Q因子为1⁄2*，而时钟、雷射或是其他需要强烈共振或是要求频率稳定性的系统其Q因子也较高。音叉的Q因子大约为1000，原子钟、加速器中的超导射频或是光学共振腔的Q因子可以到$10^{11}$\n\n\u003e [!help] \n\u003e  There are many *alternative quantities* used by physicists and engineers to describe how damped an oscillator is. Important examples include: the [damping ratio](https://en.wikipedia.org/wiki/Damping_ratio \"Damping ratio\"), [relative bandwidth](https://en.wikipedia.org/wiki/Bandwidth_(signal_processing) \"Bandwidth (signal processing)\"), [linewidth](https://en.wikipedia.org/wiki/Oscillator_linewidth \"Oscillator linewidth\") and bandwidth measured in [octaves](https://en.wikipedia.org/wiki/Octave_(electronics) \"Octave (electronics)\").\n\n\n# Definition\n\n![](Physics/Electromagnetism/attachments/Pasted%20image%2020230404151254.png)\n\n\u003cfont size=1\u003eFig. 一阻尼谐振子的频宽, $\\Delta f$可以用频率和能量的图来表示。阻尼谐振子（或滤波器）的Q因子为$f_{0}/\\Delta f$。Q因子越大，其波峰高度会越高，而其宽度会越窄\u003c/font\u003e\n\nIn the context of resonators, there are two common definitions for Q, which aren't exactly equivalent. They become approximately equivalent *as Q becomes larger*, meaning the resonator becomes less damped.\n\n## Bandwidth definition\n\n$$Q\\stackrel{def}{=}\\frac{f_r}{\\Delta f}=\\frac{\\omega_r}{\\Delta \\omega}$$\n\n$f_r$为共振频率，$\\Delta f$为频宽，一般是 [full width at half maximum](https://en.wikipedia.org/wiki/Full_width_at_half_maximum \"Full width at half maximum\") (FWHM)\n\n## Stored energy definition\n\nQ因子可定义为在一系统的共振频率下，当信号振幅不随时间变化时，**系统储存能量和每个周期外界所提供能量的比例**（此时系统储存能量也不随时间变化）\n\n$$Q = 2\\pi \\times \\frac{\\text{Energy Stored}}{\\text{Energy dissipated per cycle}}=2\\pi f_r \\times \\frac{\\text{Energy Stored}}{\\text{Power Loss}}$$\n\n同时在像电感等储能元件的规格中，会用到和频率有关的Q因子，其定义如下\n\n$$Q(\\omega) = \\omega \\times \\frac{\\text{Maximum Energy Stored}}{\\text{Power Loss}}$$\n\n其中$\\omega$是计算储存能量和功率损失时的角频率\n\n\n# Reference\n\n* [Q factor in  wiki](https://en.wikipedia.org/wiki/Q_factor)\n* [品质因子](https://zh.wikipedia.org/zh-hans/%E5%93%81%E8%B3%AA%E5%9B%A0%E5%AD%90#:~:text=%E5%93%81%E8%B4%A8%E5%9B%A0%E5%AD%90%E6%88%96Q%E5%9B%A0%E5%AD%90,%E6%91%86Q%E5%9B%A0%E5%AD%90%E8%BE%83%E4%BD%8E%E3%80%82)","lastmodified":"2023-09-18T02:45:47.2252428Z","tags":null},"/Physics/Electromagnetism/Resonant_circuit":{"title":"Resonant circuit","content":"\n以RLC串联电路为例\n\n# 什么是谐振\n\n电路中电容器$L$、电感器$C$两组件之能量相等，当能量由电路中某一电抗组件释出时，且另一电抗组件必吸收相同之能量，即此两电抗组件间会产生一能量脉动。\n\n# 两种简单的谐振电路\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230330160535.png)\n\n\n以串联谐振为例\n\n## *Resonant Frequency*\n\n电容，电阻的[电抗](Physics/Electromagnetism/Basic/Electric_units.md#Electrical%20impedance)相同时发生谐振\n\n$$\n|X_C| = |\\frac{1}{j2\\pi fC}| = |X_L| = |j2\\pi fL|\n$$\nRearranging,\n\n$$\nf^2 =  \\frac{1}{(2\\pi)^2 C L}\n$$\n\n$$\nf = \\frac{1}{2\\pi \\sqrt{LC}}\n$$\n\n## 串联谐振特性\n\n* 阻抗最小，且为纯电阻，$Z = R+jXL-jXC = R$ \n\n## **品质因子** ([*Q factor*](Physics/Electromagnetism/Q_factor.md))\n\n* 电感器或电容器在谐振时产生的电抗功率与电阻器消耗的平均功率之比，称为谐振时之品质因子。\n\n$$Q=\\frac{Q_L}{P}=\\frac{I^2X_L}{I^2R}=\\frac{Q_C}{P}=\\frac{I^2X_C}{I^2R}=\\frac{1}{R}\\sqrt{\\frac{L}{C}}=\\frac{\\sqrt{X_LX_C}}{R}$$\n\n#### 阻抗与频率的关系\n\n$Z = R + j(X_L-X_C)$\n* 当$f＝f_r$时，$Z＝R$为最小值，电路为电阻性；\n* 当$f＞f_r$时，$X_L＞X_C$为最小值，电路为电感性；\n* 当$f＜f_r$时，$X_L＜X_C$为最小值，电路为电容性。\n","lastmodified":"2023-09-18T02:45:47.2252428Z","tags":null},"/Physics/Optical/optical_abberation":{"title":"Optical Abberation","content":"\n# What is optical aberration\n\n光学像差是指镜头设计中的缺陷，它会导致光线散开而不是聚焦以形成清晰的图像。 范围从图像中的所有光线到只有某些点或边缘失焦。 成像时可能会出现几种类型的光学像差。 构建一个校正了所有可能像差的理想视觉系统会显着增加镜头的成本。 实际上，镜头中总会存在某种形式的像差，但将像差的影响降至最低至关重要。 因此，制造任何镜头通常都会做出一些妥协。\n\n# Circle of confusion\n\n要解释像差如何使图像模糊，首先要解释一下：什么是混淆圈？ 当来自目标的光点到达镜头，然后会聚在传感器上时，它会很清晰。 否则，如果它在传感器之前或之后会聚，则传感器上的光分布会更广。 这可以在图 1 中看到，其中可以看到点光源会聚在传感器上，但随着传感器位置的变化，沿传感器散布的光量也会发生变化。\n\n![](Physics/Optical/attachments/Fig_1_Circles_of_confusion.gif)\n\n光线越分散，图像的焦点就越少。 除非光圈很小，否则图像中彼此距离较大的目标通常会使背景或前景失焦。 这是因为会聚在前景中的光与来自背景中较远目标的光会聚在不同的点。\n\n# Types of Optical Aberration\n\n## Coma（慧差）\n\n\n彗形像差，又称彗星像差，此种像差的分布形状以类似于彗星的拖尾而得名。\n\n![](Physics/Optical/attachments/Pasted%20image%2020230424110844.png)\n\n这是一些透镜固有的或是光学设计造成的缺点，导致离开光轴的点光源，例如恒星，产生变形。特别是，彗形像差被定义为偏离入射光孔的放大变异。在折射或衍射的光学系统，特别是在宽光谱范围的影像中，彗形像差是波长的函数。\n\n## Astigmatism （像散）\n\n在两个垂直平面中传播的光线在聚焦于不同点时可能会产生像散。\n\n这可以在图 3 中看到，其中两个焦点由红色水平面和蓝色垂直面表示。 图像中的最佳清晰度点将在这两个点之间，其中任一平面的混淆圈都不太宽。\n\n![](Physics/Optical/attachments/Pasted%20image%2020230424111226.png)\n\n当光学器件未对准时，散光会导致图像的侧面和边缘失真。 它通常被描述为在查看图像中的线条时缺乏清晰度。\n\n这种形式的像差可以使用大多数优质光学器件中的适当透镜设计来校正。 固定散光的光学元件的最初设计是由卡尔蔡司完成的，并且已经发展了一百多年。 在这一点上，它通常只出现在质量非常低的镜头中，或者内部光学元件已损坏或通过镜头滴移动的情况下。\n\n## (Petzval) Field Curvature （场曲）\n\n许多镜头都有圆形的焦点。 这会导致图像出现柔和的角，主要是使图像的中心保持在焦点上。 然而，大多数镜头都有一些圆形的焦点，如果不进行一些裁剪，就无法聚焦整个图像。\n\n场曲是图像平面由于多个焦点而变得不平坦的结果。\n\n![](Physics/Optical/attachments/Pasted%20image%2020230424112159.png)\n\n相机镜头已在很大程度上纠正了这一点，但在许多镜头上可能会发现一些场曲。 一些传感器制造商实际上正在研究可以校正弯曲焦点区域的弯曲传感器。 这种设计将允许传感器校正像差，而不需要以这种精度生产昂贵的镜头设计。 通过实施这种类型的传感器，可以使用更便宜的镜头来产生高质量的结果。 这方面的真实例子可以在开普勒太空天文台看到，那里使用弯曲的传感器阵列来校正望远镜的大型球面光学元件。\n\n## Distortion (畸变)\n\n畸变是指当一物体通过Lens系统成像时，会产生一种对物体不同部分有不同的放大率的像差，此种像差会导致物像的相似性变坏。但不影响像的清晰度。 根据对物体周边及中心有放大率的差异此种像差可分为两类： \n\n### Barrel distortion （桶形畸变）\n\n具有桶形失真的图像的边缘和侧面远离中心弯曲。 这在视觉上看起来像是图像中有一个凸起，因为它捕获了弯曲视场 (FoV, field of view) 的外观。 例如，当在高层建筑的高处使用较低焦距的镜头（也称为广角镜头）时，可以捕捉到更宽的 FoV。 如图 5 所示，使用产生非常扭曲和宽 FoV 的鱼眼镜头时，这种情况最为夸张。在此图像中，网格线用于帮助说明失真效果如何在靠近侧面的地方向外产生更拉伸的图像， 边缘。\n\n![](Physics/Optical/attachments/Pasted%20image%2020230424113453.png)\n\n\n### Pincushion distortion （枕型畸变）\n\n当光线通过枕形畸变向光轴弯曲时，图像看起来会向内拉伸。 因此，图像的边缘和侧面看起来会向图像的中心弯曲。\n\n这种形式的像差最常见于焦距较长的远摄镜头。\n\n![](Physics/Optical/attachments/Pasted%20image%2020230424113838.png)\n\n### Mustache distortion\n\n**小胡子畸变**😂是枕形失真和桶形失真的组合。 这会导致图像的内部向外弯曲，而图像的外部向内弯曲。 小胡子失真是一种相当罕见的像差，其中不止一种失真模式会影响图像。 小胡子畸变通常是镜头设计非常糟糕的标志，因为这是导致像差融合的光学错误的高潮。\n\n\n## Chromatic （位置色差）\n\n### Longitudinal / axial aberration\n\n光的颜色代表特定波长的光。 由于折射，彩色图像将有多个波长进入镜头并聚焦在不同的点。 纵向或轴向色差是由不同波长聚焦在沿光轴的不同点引起的。 波长越短，其焦点将离镜头越近，而波长越远，则反之，离镜头越远，如图 8 所示。通过引入较小的孔径，进入的光仍可能聚焦在不同的位置 点，但“混淆圈”的宽度（直径）会小得多，导致不那么剧烈的模糊。\n\n![](Physics/Optical/attachments/Fig_8_Chromatic_abberation_animation.gif)\n\n### Transverse / lateral aberration\n\n导致不同波长沿图像平面分布的离轴光是横向或横向色差。 这会导致图像中主体边缘出现彩色边纹。 这比纵向色差更难校正。\n\n![](Physics/Optical/attachments/Fig_9_Chromatic_aberration_lateral.gif)\n\n它可以使用引入不同折射率的消色差双合透镜来固定。 通过将可见光谱的两端置于一个焦点上，可以消除色边。 对于横向和纵向色差，减小光圈的大小也有帮助。 此外，在高对比度环境（即具有非常亮的背景的图像）中不成像目标可能是有益的。 在显微镜中，镜头可能使用复消色差透镜 (APO) 而不是消色差透镜，消色差透镜使用三个透镜元件来校正入射光的所有波长。 当颜色最重要时，确保减轻色差将产生最佳效果。\n\n# Reference\n\n* [SIX OPTICAL ABERRATIONS THAT COULD BE IMPACTING YOUR VISION SYSTEM, https://www.lumenera.com](https://www.lumenera.com/blog/six-optical-aberrations-that-could-be-impacting-your-vision-system)\n* [光学像差重要知识点详解|光学经典理论, 知乎 - 监控李誉](https://zhuanlan.zhihu.com/p/40149006)","lastmodified":"2023-09-18T02:45:48.117253102Z","tags":null},"/Physics/Physics_MOC":{"title":"Physics MOC","content":"\n# Electromagnetism\n\n* [Electromagnetism MOC](Physics/Electromagnetism/Electromagnetism_MOC.md)","lastmodified":"2023-09-18T02:45:48.117253102Z","tags":null},"/Physics/Wave/Doppler_Effect":{"title":"Doppler Effect","content":"\n多普勒效应（**Doppler effect**）是波源和观察者有相对运动时，观察者接受到波的频率与波源发出的频率并不相同的现象。\n\n远方急驶过来的火车鸣笛声变得尖细（即频率变高，波长变短），而离我们而去的火车鸣笛声变得低沉（即频率变低，波长变长），就是多普勒效应的现象，同样现象也发生在汽车鸣响与火车的敲钟声。\n\n# General\n\n在classical physics中，source的speed和receiver的speed远小于wave在medium中的移动速度，observed frequency $f$和emitted frequency$f_0$关系：\n\n$$\nf = (\\frac{c \\pm v_r}{c \\pm v_s})f_0\n$$\n* $c$是wave在介质中的速度\n* $v_r$是receiver相对于介质的速度，如果receiver向source移动，则分子为加号，反之为减号\n* $v_s$是source相对于介质的速度，如果source远离receiver，则分母为加号，反之为减号\n\n\u003e [!note] \n\u003e  请注意，此关系预测如果源或接收器中的任何一个远离另一个，频率将会降低。\n\n$$\n\\frac{f}{v_{wr}} = \\frac{f_0}{v_{ws}} = \\frac{1}{\\lambda}\n$$\n* $v_{\\omega r}$是wave speed相对于receiver\n* $v_{\\omega s}$是wave speed相对于source\n* $\\lambda$是波长\n\n## Example\n\n![](Physics/Wave/attachments/Dopplereffectsourcemovingrightatmach0.7.gif)\n\n其中$v_s = 0.7c$，波前开始在源的右侧（前面）聚集，并在源的左侧（后面）进一步分开。\n\n在前面的receiver会听到higher frequency，也就是$f = \\frac{c}{c-0.7c}f_0 = 3.33f_0$；后面的receiver会听到lower frequency，也就是$f = \\frac{c}{c + 0.7c}f_0 = 0.59f_0$\n\n\n\n\n# Reference\n\n*  [多普勒效应 - Wiki](https://zh.wikipedia.org/wiki/%E5%A4%9A%E6%99%AE%E5%8B%92%E6%95%88%E5%BA%94)","lastmodified":"2023-09-18T02:45:48.117253102Z","tags":null},"/Report/2023.04.16-%E5%A4%A9%E7%BA%BF%E6%B5%8B%E8%AF%95":{"title":"2023.04.16 天线测试","content":"\n 对天线进行测距能力的测试\n\n# 背景\n\n![](Report/attachments/96251ac46494ab01294e570e352c426.png)\n\n# 测试结果\n\n## 无穷远距离测量\n\n前方30cm内无反射，超出本雷达测距能力极限，近似为无穷远距离内无反射，得到收集端电压\n\n![](Report/attachments/7983094eb03d1dcc285edf9c1768018.png)\n\n以前的天线收集的数据：\n\n![](Report/attachments/f5d557933b15f8ea7f6861f70663d13.png)\n\n问题在于两点：\n\n* 目前天线稳定性不足\n* 核心信号峰值下降为1.7v左右，而之前核心信号为2.2v\n\n## 实时测距实验\n\n*实时测距实验为在天线段实时测量信号并在前面按照时间放置金属挡板检测天线的测距能力。*\n\n实验大致的放置时间为：\n1. 0-25s，不放置金属挡板\n2. 25-50s，金属挡板贴紧天线\n3. 50-75s，不放置金属挡板\n4. 75-100s，在10cm处放置金属挡板\n5. 100-125s，不放置金属挡板\n6. 125-150s，在20cm处放置金属挡板\n7. 175-200s，不放置金属挡板\n8. 150-175s，在30cm处放置金属挡板\n\n新天线收集数据：\n\n![](Report/attachments/abaec3368e16f2c9be67b5edbba39be.png)\n\n旧天线收集信号：\n\n![](Report/attachments/ac4c5aa53392835d3db04a78e73476b.png)\n\n问题在于：\n\n* 新天线信号不稳定，与无穷远测试中的结果吻合。\n* 导致了不同距离的信号区分度丧失\n","lastmodified":"2023-09-18T02:45:48.121253148Z","tags":null},"/assets/pdf/NUS_Transcript.pdf":{"title":"NUS_Transcript.pdf","content":"","lastmodified":"2023-09-18T02:45:48.121253148Z","tags":null},"/atlas":{"title":"Atlas - Map of Maps","content":"\n🚧 There are notebooks about his research career:\n\n* [Deep Learning \u0026 Machine Learning](computer_sci/Deep_Learning_And_Machine_Learning/Deep%20_Learning_MOC.md)\n\n* [[synthetic_aperture_radar_imaging/SAR_MOC| Synthetic Aperture Radar(SAR) Imaging]]\n\n* [Equipment Research MOC](equipment_research/equip_res_MOC.md)\n\n💻 Also, his research needs some basic science to support\n\n* [Data Structure and Algorithm MOC](computer_sci/data_structure_and_algorithm/MOC.md)\n\n* [Hardware](computer_sci/Hardware/Hardware_MOC.md)\n\n* [Physics](Physics/Physics_MOC.md)\n\n* [Signal Processing](signal_processing/signal_processing_MOC.md)\n\n* [Data Science](data_sci/data_sci_MOC.md)\n\n* [About coding language design detail](computer_sci/coding_knowledge/coding_lang_MOC.md)\n\n* [Math](Math/MOC.md)\n\n* [Computational Geometry](computer_sci/computational_geometry/MOC.md)\n\n🦺 I also need some tool to help me:\n\n* [Git](toolkit/git/git_MOC.md)\n\n💻 Code Practice:\n\n* [💽Programing Problem Solution Record](https://github.com/PinkR1ver/JudeW-Problemset)\n\n🛶 Also, he learn some knowledge about his hobbies:\n\n* [📷 Photography](Photography/Photography_MOC.md)\n\n* [📮文学](文学/文学_MOC.md)\n\n⭐ Here to find my recent study:\n\n* [Recent notes](recent.md)\n\n* [📸Log](log/log_MOC.md)","lastmodified":"2023-09-18T02:45:48.121253148Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/Deep-_Learning_MOC":{"title":"Deep Learning - MOC","content":"\n# Tech Explanation\n\n* [⭐Deep Learning  MOC](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/deep_learning_MOC.md)\n\n* [✨Machine Learning MOC](computer_sci/Deep_Learning_And_Machine_Learning/machine_learning/MOC.md)\n\n* [LLM - MOC](computer_sci/Deep_Learning_And_Machine_Learning/LLM/LLM_MOC.md)\n\n# Deep-learning Research\n\n* [Model Interpretability](computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/Model_Interpretability_MOC.md)\n\n* [Famous Model - MOC](computer_sci/Deep_Learning_And_Machine_Learning/Famous_Model/Famous_Model_MOC.md)\n\n* [Model Evaluation - MOC](computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/model_evaluation_MOC.md)","lastmodified":"2023-09-18T02:45:48.121253148Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/model_evaluation_MOC":{"title":"Model Evaluation - MOC","content":"\n* [Model Evaluation in Time Series Forecasting](computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/time_series_forecasting.md)","lastmodified":"2023-09-18T02:45:48.125253195Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/time_series_forecasting":{"title":"Model Evaluation in Time Series Forecasting","content":"\n![](computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/attachments/Pasted%20image%2020230526162839.png)\n\n# Some famous time series scoring technics\n\n1.  **MAE, RMSE and AIC**\n2.  **Mean Forecast Accuracy**\n3.  **Warning: The time series model EVALUATION TRAP!**\n4.  **RdR Score Benchmark**\n\n## MAE, RMSE, AIC\n\nMAE means **Mean Absolute Error (MAE)** and RMSE means **Root Mean Squared Error (RMSE)**.\n\n这是两个衡量 continuous variables的accuracy的著名指标，MAE在以前的文章中被时常使用，16年的观察已经发现RMSE或者其他version的R-squared逐渐被使用起来\n\n*我们需要了解何时使用哪种指标会更好*\n\n### MAE\n\n$$\n\\text{MAE} = \\frac{1}{n}\\sum_{j=1}^n |y_j - \\hat{y}_j|\n$$\nMAE的特点在于所有individual difference有着equal weight\n\n如果将绝对值去掉，MAE会变成**Mean Bias Error (MBE)**，使用MBE时，要注意正反bias相互抵消\n\n### RMSE\n\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{j=1}^n (y_j - \\hat{y}_j)^2}\n$$\n\n均方根误差（RMSE）是一种二次评分规则，它还测量误差的平均幅度。它是预测值和实际观测值之间差异的平方的平均值的平方根。\n\n### AIC\n\n$$\n\\text{AIC} = 2k - 2\\ln{(\\hat{L})}\n$$\n$k$是模型参数的估计，$\\hat{L}$是模型似然函数(likelihood function)的最大化值\n\n**Akaike information criterion**，赤池信息准则（AIC）是一个有助于比较模型的指标，因为它同时考虑了模型对数据的拟合程度和模型的复杂性。\n  \nAIC衡量信息的损失并**对模型的复杂性进行惩罚**。它是*参数数量惩罚后的负对数似然函数*。AIC的主要思想是模型参数越少越好。**AIC允许您测试模型在不过拟合数据集的情况下拟合数据的程度**\n\n### Comparison\n\n#### Similarities between MAE and RMSE\n\n均方误差（MAE）和均方根误差（RMSE）都以感兴趣变量的单位来表示平均模型预测误差。这两个指标都可以在0到∞的范围内变化，并且对误差的方向不敏感。它们是负向评分指标，也就是说数值越低越好。\n\n#### Differences between MAE and RMSE\n\n*由于误差在求平均之前被平方，RMSE对大误差给予相对较高的权重*。这意味着在特别不希望出现大误差的情况下，RMSE应该更有用；而在MAE的平均值中，这些大误差将被稀释，\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/attachments/Pasted%20image%2020230526161422.png)\n\nAIC the lower is better，但没有perfect score，只能用来相同dataset下不同model的性能\n\n## Mean Forecast Accuracy\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/attachments/Pasted%20image%2020230526162035.png)\n\n计算每个点的Forecast Accuracy，然后求平均，得到 Mean Forecast Accuracy\n\nMean Forecast Accuracy的重大缺陷在大的偏离值造成巨大的负面影响，比如$1 - \\frac{|\\hat{y}_j - y_j|}{y_j} = 1 - \\frac{250-25}{25} = -800\\%$\n\n解决方案是将Forecast Accuracy的最小值限制为0%，同时可以使用Median代替Mean。\n\n一般来说，**当你的误差分布偏斜时，你应该使用 Median 而不是 Mean**。 在某些情况下，Mean Forecast Accuray也可能毫无意义。 如果你还记得你的统计数据； 变异系数 (**coefficient of variation**, CV) 表示标准偏差与平均值的比率（$\\text{CV} = (\\text{Standard Deviation}/\\text{Mean} * 100)$）。 大 CV 值意味着大变异性，这也意味着围绕均值的离差程度更大。 **例如，我们可以将 CV 高于 0.7 的任何事物视为高度可变且不可真正预测的。 另外，还可以说明你的预测模型预测能力很不稳定！** \n\n## RdR Score Benchmark (这是一个具有实验性的指标，blogger指出这个指标并没有在research paper出现过)\n\nRdR metric stands for:\n* *R*: **Naïve Random Walk**\n* *d*: **Dynamic Time Warping**\n* *R*: **Root Mean Squared Error**\n\n### DTW to deal with shape similarity\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/attachments/Pasted%20image%2020230526163614.png)\n\nRMSE、MAE这些指标都没有考虑到一个重要的标准：**THE SHAPE SIMILARITY**\n\nRdR Score Benchmark使用 [**Dynamic Time Warping(DTW，动态时间调整)** ](computer_sci/Deep_Learning_And_Machine_Learning/Trick/DTW.md)作为shape similarity的指标\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/attachments/Pasted%20image%2020230526164106.png)\n欧氏距离在时间序列之间可能是一个不好的选择，因为时间轴上存在扭曲的情况。\n\n* DTW：通过“同步”/“对齐”时间轴上的不同信号，找到两个时间序列之间的最佳（最小距离）扭曲路径\n\n### RdR score means\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/attachments/Pasted%20image%2020230529130501.png)\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/attachments/Pasted%20image%2020230529130509.png)\n\n*RdR score*通过RMSE和DTW distance来计算，用于比较你的model和Radnom Walk(*Random Walk的RdR score = 0*)相比的优越性\n\n### RdR calculation details\n\n可以通过绘制 RMSE vs. DTW来计算RdR score，绘制的图如下所示：\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/attachments/Pasted%20image%2020230529130856.png)\n\n\n计算矩阵面积来计算RdR score，（文章里并没有完整介绍计算，在[github code](https://github.com/CoteDave/blog/tree/master/RdR%20score)里有，并不确定）\n\n# Reference\n\n* M.Sc, Dave Cote. “RdR Score Metric for Evaluating Time Series Forecasting Models.” _Medium_, 8 Feb. 2022, https://medium.com/@dave.cote.msc/rdr-score-metric-for-evaluating-time-series-forecasting-models-1c23f92f80e7.\n* JJ. “MAE and RMSE — Which Metric Is Better?” _Human in a Machine World_, 23 Mar. 2016, https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d.\n* _Accelerating Dynamic Time Warping Subsequence Search with GPU_. https://www.slideshare.net/DavideNardone/accelerating-dynamic-time-warping-subsequence-search-with-gpu. Accessed 29 May 2023.","lastmodified":"2023-09-18T02:45:48.125253195Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/Famous_Model/DeepAR":{"title":"DeepAR - Time Series Forcasting","content":"\nDeepAR, an autoregressive recurrent network developed by Amazon, is the first model that could natively work on multiple time-series. It's a milestone in time-series community.\n\n# What is DeepAR\n\n\u003e [!quote] \n\u003e  DeepAR is the first successful model to combine Deep Learning with traditional Probabilistic Forecasting.\n\n* **Multiple time-series support**\n* **Extra covariates**: *DeepAR* allows extra features, covariates. It is very important for me when I learn *DeepAR*, because in my task, I have corresponding feature for each time series.\n* **Probabilistic output**:  Instead of making a single prediction, the model leverages [**quantile loss**](computer_sci/Deep_Learning_And_Machine_Learning/Trick/quantile_loss.md) to output prediction intervals.\n* **“Cold” forecasting:** By learning from thousands of time-series that potentially share a few similarities, _DeepAR_ can provide forecasts for time-series that have little or no history at all.\n\n# Block used in DeepAR\n\n* [LSTM](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/LSTM.md)\n\n# *DeepAR* Architecture\n\nDeepAR模型并不直接使用LSTMs去计算prediction，而是去估计Gaussian likelihood function的参数，即$\\theta=(\\mu,\\sigma)$，估计Gaussian likelihood function的mean和standard deviation。\n\n## Training Step-by-Step\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Famous_Model/attachments/Pasted%20image%2020230523134255.png)\n\n假设目前我们在time-series $i$ 的 t 时刻，\n\n1. LSTM cell会输入covariates $x_{i,t}$，即$x_i$在t时刻的值，还有上一时刻的target variable，$z_{i,t-1}$，LSTM还需要输入上一时刻的隐藏状态$h_{i,t-1}$\n2. LSTM紧接着就会输出当前的hidden state $h_{i,t}$，会输入到下一步中\n3. Gaussian likelihood function里的parameter，$\\mu$和$\\sigma$会从$h_{i,t}$中不直接计算出，计算细节在后面\n\n\u003e [!quote] \n\u003e 换言之，这个模型是为了得到最好的$\\mu$和$\\sigma$去构建gaussian distribution，让预测更接近$z_{i,t}$；同时，因为*DeepAR*每次都是train and predicts a single data point，所以这个模型也被称为autoregressive模型\n\n\n## Inference Step-by-Step\n\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Famous_Model/attachments/Pasted%20image%2020230523141219.png)\n\n\n在使用model进行预测的时候，某一改变的就是使用预测值$\\hat{z}$ 代替真实值$z$，同时$\\hat{z}_{i,t}$是在我们模型学习到的Gaussian distribution里sample得到的，而这个Gaussian distribution里的参数$\\mu$和$\\sigma$并不是model直接学习到的，*DeepAR*如何做到这一点的呢？\n\n# Gaussian Likelihood\n\n$$\n\\ell_G(z|\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{(-\\frac{(z-\\mu)^2)}{2\\sigma^2}}\n$$\n\nEstimate gaussian distribution的任务一般会被转化成maximize gaussian log-likelihood function的任务，即**MLEformulas**(maximum log-likelihood estimators)\n**Gaussian log-likelihood function**:\n\n$$\n\\mathcal{L} = \\sum_{i=1}^{N}\\sum_{t=t_o}^{T} \\log{\\ell(z_{i,t}|\\theta(h_{i,t}))}\n$$\n\n\n# Parameter estimation in *DeepAR*\n\n\n在统计学中，预估Gaussian Distribution一般使用MLEformulas，但是在*DeepAR*中，并不这么去做，而是使用两个dense layer去做预估，如下图：\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Famous_Model/attachments/Pasted%20image%2020230523151201.png)\n\n使用dense layer的方式去预估Gaussian distribution的原因在于，可以使用backpropagation\n\n\n# Reference\n\n* [https://towardsdatascience.com/deepar-mastering-time-series-forecasting-with-deep-learning-bc717771ce85](https://towardsdatascience.com/deepar-mastering-time-series-forecasting-with-deep-learning-bc717771ce85)","lastmodified":"2023-09-18T02:45:48.125253195Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/Famous_Model/Famous_Model_MOC":{"title":"Famous Model MOC","content":"\n# Time-series\n\n* [DeepAR](computer_sci/Deep_Learning_And_Machine_Learning/Famous_Model/DeepAR.md)\n\n","lastmodified":"2023-09-18T02:45:48.125253195Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/Famous_Model/Temporal_Fusion_Transformer":{"title":"Temporal Fusion Transformer","content":"\n","lastmodified":"2023-09-18T02:45:48.125253195Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/LLM/LLM_MOC":{"title":"Large Language Model(LLM) - MOC","content":"\n# Training\n\n* [Training Tech Outline](computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/steps.md)\n* [⭐⭐⭐Train LLM from scratch](computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/train_LLM.md)\n* [⭐⭐⭐Detailed explanation of RLHF technology](computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/RLHF.md)\n* [How to do use fine tune tech to create your chatbot](computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/how_to_fine_tune.md)\n* [Learn finetune by Stanford Alpaca](computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/learn_finetune_byStanfordAlpaca.md)\n\n# Metrics\n\nHow to evaluate a LLM performance?\n\n* [Tasks to evaluate BERT - Maybe can be deployed in other LM](computer_sci/Deep_Learning_And_Machine_Learning/LLM/metircs/some_task.md)\n\n# Basic\n\n* [LLM Hyperparameter](computer_sci/Deep_Learning_And_Machine_Learning/LLM/basic/llm_hyperparameter.md)\n","lastmodified":"2023-09-18T02:45:48.129253241Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/LLM/basic/llm_hyperparameter":{"title":"LLM hyperparameter","content":"\n# LLM Temperature\n\nTemperature definition come from the physical meaning of temperature. The more higher temperature, the atoms moving more faster, meaning more randomness.\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/LLM/basic/attachments/physic_temp.gif)\n\nLLM temperature is a hyperparameter that regulates **the randomness, or creativity.** \n\n* Higher the LLM temperature, more diverse and creative, increasing likelihood of straying from context.\n* Lower the LLM temperature, more focused and deterministic, sticking closely to the most likely prediction\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/LLM/basic/attachments/Pasted%20image%2020230627160125.png)\n\n## More detail\n\nThe LLM model is to give a probability of next word, like this:\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/LLM/basic/attachments/Pasted%20image%2020230627162848.png)\n\n\"A cat is chasing a …\", there are lots of words can be filled in that blank. Different words have different probabilities, in the model, we output the next word ratings.\n\nSure, we can always pick the highest rating word, but that would result in very standard predictable boring sentences, and the model wouldn't be equivalent to human language, because we don't always use the most common word either. \n\nSo, we want to design a mechanism that **allows all words with a decent rating to occur with a reasonable probability**, that's why we need temperature in LLM model. \n\nLike real physic world,  we can do samples to describe the distribution, *we use SoftMax to describe the distribution  of the probability of the next word*. The temperature is the element $T$ in the formula:\n\n$$\np_i = \\frac{\\exp{(\\frac{R_i}{T})}}{\\sum_i \\exp{(\\frac{R_i}{T})}}\n$$\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/LLM/basic/attachments/Pasted%20image%2020230627163514.png)\n\nMore lower the $T$, the higher rating word's probability will goes to 100%, and more higher the $T$, the probability will be more smoother for very words.\n\n*The gif below is important and intuitive.*\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/LLM/basic/attachments/rating_probabililty.gif)\n\nSo, set different $T$, the next word's probability will be changed, we will output next word depending on the probability.\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/LLM/basic/attachments/Pasted%20image%2020230627165311.png)\n\n# Reference\n\n* [LLM Temperature, dedpchecks](https://deepchecks.com/glossary/llm-parameters/#:~:text=One%20intriguing%20parameter%20within%20LLMs,of%20straying%20from%20the%20context.)\n* [⭐⭐⭐https://www.youtube.com/watch?v=YjVuJjmgclU](https://www.youtube.com/watch?v=YjVuJjmgclU)","lastmodified":"2023-09-18T02:45:48.257254719Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/LLM/langchain/langchain_basic":{"title":"LangChain Explained","content":"\n# What is LangChain\n\nLangChain is an open source framework that allows AI developers to combine LLMs like GPT-4 *with external sources of computation and data*.\n\n# Why LangChain\n\nLangChain can make LLM answer question depending on your own documents. It can help you doing lots of amazing apps.\n\nYou can use LangChain to make GPT to do analysis on your own company data, booking flight depending on schedule. summarizing abstract on bunches of PDFs, .….\n\n# LangChain value propositions\n\n## Components\n\n* LLM Wrappers\n* Prompt Templates\n* Indexes for relevant information retrieval\n\n## Chains\n\nAssemble components to solve a specific task - finding info in a book...\n\n## Agents\n\nAgents allow LLMs to interact with it's environment. -  For instance, make API request with a specific action\n\n# LangChain Framework\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/LLM/langchain/attachments/Pasted%20image%2020230627154149.png)\n\n\n\n# Reference\n\n* [https://www.youtube.com/watch?v=aywZrzNaKjs](https://www.youtube.com/watch?v=aywZrzNaKjs)\n* ","lastmodified":"2023-09-18T02:45:48.261254765Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/LLM/metircs/some_task":{"title":"Tasks to evaluate BERT - Maybe can be deployed in other LM","content":"\n# Overview\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/LLM/metircs/attachments/Pasted%20image%2020230629140929.png)\n\n# MNLI-m (Multi-Genre Natural Language Inference - Matched): \n\nMNLI-m is a benchmark dataset and task for natural language inference (NLI). The goal of NLI is to determine the logical relationship between two given sentences: whether the relationship is \"entailment,\" \"contradiction,\" or \"neutral.\" MNLI-m focuses on matched data, which means the sentences are drawn from the same genres as the sentences in the training set. It is part of the GLUE (General Language Understanding Evaluation) benchmark, which evaluates the performance of models on various natural language understanding tasks.\n\n# QNLI (Question Natural Language Inference): \n\nQNLI is another NLI task included in the GLUE benchmark. In this task, the model is given a sentence that is a premise and a sentence that is a question related to the premise. The goal is to determine whether the answer to the question can be inferred from the given premise. The dataset for QNLI is derived from the Stanford Question Answering Dataset (SQuAD).\n\n# MRPC (Microsoft Research Paraphrase Corpus): \n\nMRPC is a dataset used for paraphrase identification or semantic equivalence detection. It consists of sentence pairs from various sources that are labeled as either paraphrases or not. The task is to classify whether a given sentence pair expresses the same meaning (paraphrase) or not. MRPC is also part of the GLUE benchmark and helps evaluate models' ability to understand sentence similarity and equivalence.\n\n# SST-2 (Stanford Sentiment Treebank - Binary Sentiment Classification): \n\nSST-2 is a binary sentiment classification task based on the Stanford Sentiment Treebank dataset. The dataset contains sentences from movie reviews labeled as either positive or negative sentiment. The task is to classify a given sentence as expressing a positive or negative sentiment. SST-2 is often used to evaluate the ability of models to understand and classify sentiment in natural language.\n\n# SQuAD (Stanford Question Answering Dataset): \n\nSQuAD is a widely known dataset and task for machine reading comprehension. It consists of questions posed by humans on a set of Wikipedia articles, where the answers to the questions are spans of text from the corresponding articles. The goal is to build models that can accurately answer the questions based on the provided context. SQuAD has been instrumental in advancing the field of question answering and evaluating models' reading comprehension capabilities.\n\nOverall, these tasks and datasets serve as benchmarks for evaluating natural language understanding and processing models. They cover a range of language understanding tasks, including natural language inference, paraphrase identification, sentiment analysis, and machine reading comprehension.\n\n\n","lastmodified":"2023-09-18T02:45:48.261254765Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/RLHF":{"title":"Reinforcement Learning from Human Feedback","content":"\n\n# Review: Reinforcement Learning Basics\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/attachments/Pasted%20image%2020230628145009.png)\n\n\nReinforcement learning is a mathematical framework. \n\nDemystify the reinforcement learning model, it's a open-ended model using reward function to optimize agent to solve complex task in target environment. \n\n\u003c!---\n# Origins of RLHF\n\n## Pre Deep RL\n\n![](Deep_Learning_And_Machine_Learning/LLM/train/attachments/Pasted%20image%2020230628160836.png)\n\n\nBefore, Deep RL don't use neural network to represent policy. What this system did was a machine learning system that created a policy by having humans label the actions that an agent took as being kind of correct or incorrect. This was just a simple decision rule where humans labeled every actions as good or bad.  This was essentially a reward model and a policy put together.\n\n## For Deep RL\n\n![](Deep_Learning_And_Machine_Learning/LLM/train/attachments/Pasted%20image%2020230628161627.png)\n\n---\u003e\n\n# Step by Step\n\nFor RLHF training method, here are three core steps:\n\n1. Pretraining a language model\n2. Gathering data(问答数据) and training a reward model\n3. Fine-tuning the LM with reinforcement learning\n\n## Step 1. Pretraining Language Models\n\nRead this to learn how to train a LM:\n\n[Pretraining language models](computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/train_LLM.md)\n\nOpenAI used a smaller version of GPT-3 for its first popular RLHF model - InstructGPT.\n\nNowadays, RLHF is new area, there's no answer to which model is the best for starting point of RLHF and using expensive augmented data to fine-tune is not necessarily.\n\n## Step 2. Reward model training\n\nIn reward model, we integrate human preferences into the system. \n\n![](computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/attachments/Pasted%20image%2020230629145231.png)\n\n\n\n# Reference\n\n* [Reinforcement Learning from Human Feedback: From Zero to chatGPT, YouTube, HuggingFace](https://www.youtube.com/watch?v=2MBJOuVq380)\n* [Hugging Face blog, ChatGPT 背后的“功臣”——RLHF 技术详解](https://huggingface.co/blog/zh/rlhf)","lastmodified":"2023-09-18T02:45:48.261254765Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/dataset/make_custom_dataset":{"title":"How to make custom dataset?","content":"\n","lastmodified":"2023-09-18T02:45:48.261254765Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/how_to_fine_tune":{"title":"How to do use fine tune tech to create your chatbot","content":"\n","lastmodified":"2023-09-18T02:45:48.265254812Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/learn_finetune_byStanfordAlpaca":{"title":"Learn finetune by Stanford Alpaca","content":"\n![](computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/attachments/Pasted%20image%2020230627145954.png)\n\n\n\n\n\n# Reference\n\n* [https://www.youtube.com/watch?v=pcszoCYw3vc](https://www.youtube.com/watch?v=pcszoCYw3vc)\n* [https://crfm.stanford.edu/2023/03/13/alpaca.html](https://crfm.stanford.edu/2023/03/13/alpaca.html)","lastmodified":"2023-09-18T02:45:48.265254812Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/steps":{"title":"LLM training steps","content":"\n训练大型语言模型（LLM）的方法通常涉及以下步骤：\n\n1. **数据收集**：收集大规模的文本数据作为训练数据。这些数据可以是互联网上的文本、书籍、文章、新闻、对话记录等。数据的质量和多样性对于训练出高质量的LLM非常重要。\n\n2. **预处理**：对数据进行预处理以使其适合模型训练。这包括分词（将文本划分为词或子词单元）、建立词汇表（将词映射到数字表示）、清理和规范化文本等操作。\n\n3. **构建模型架构**：选择适当的模型架构来构建LLM。目前最常用的模型架构是Transformer，其中包含多层的自注意力机制和前馈神经网络层。\n\n4. **预训练**：使用大规模的文本数据集对模型进行预训练。预训练是指在无监督的情况下，通过让模型学习预测缺失的词语或下一个词语等任务来提取语言知识。这使得模型能够学习到丰富的语言表示。\n\n5. **微调（Fine-tuning）**：在预训练之后，使用特定的任务数据对模型进行微调。微调是指在特定任务的标注数据上进行有监督的训练，例如文本生成、问题回答等。通过微调，模型可以更好地适应特定任务的要求。\n\n6. **超参数调优**：调整模型的超参数，例如学习率、批量大小、模型层数等，以获得更好的性能和效果。\n\n7. **评估和迭代**：对训练后的模型进行评估，并根据评估结果进行迭代改进。这可能包括调整模型架构、增加训练数据、调整训练策略等。\n\n这些步骤通常是迭代进行的，通过不断的训练和改进，使LLM能够在各种自然语言处理任务中展现出更好的性能和生成能力。值得注意的是，LLM的训练需要大量的计算资源和时间，并且通常由专业团队在大规模的计算环境中进行。","lastmodified":"2023-09-18T02:45:48.265254812Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/train_LLM":{"title":"Train LLM from scratch","content":"\n# Find a dataset\n\nFind a corpus of text in language you prefer.\n*  Such as [OSCAR](https://oscar-project.org/)\n\nIntuitively, the more data you can get to pretrain on, the better results you will get.\n\n# Train a tokenizer\n\nThere are something you need take into consideration when train a tokenizer\n\n## Tokenization\n\nYou can read more detailed post - [Tokenization](computer_sci/Deep_Learning_And_Machine_Learning/NLP/basic/tokenization.md)\n\nTokenization is the process of **breaking text into words of sentences**. These tokens helps machine to learn context of the text. This helps in *interpreting the meaning behind the text*. Hence, tokenization is *the first and foremost process while working on the text*. Once the tokenization is performed on the corpus, the resulted tokens can be used to prepare vocabulary which can be used for further steps to train the model.\n\nExample:\n\n“The city is on the river bank” -\u003e “The”, ”city”, ”is”, ”on”, ”the”, ”river”, ”bank”\n\nHere are some typical tokenization:\n* Word ( White Space ) Tokenization\n* Character Tokenization\n* **Subword Tokenization (SOTA)**\n\n\nSubword Tokenization can handle OOV(Out Of Vocabulary) problem effectively.\n\n### Subword Tokenization Algorithm\n\n* **Byte pair encoding** *(BPE)*\n* **Byte-level byte pair encoding**\n* **WordPiece**\n* **unigram**\n* **SentencePiece**\n\n## Word embedding\n\nAfter tokenization, we make our text into token. We also wants to present token in math type. Here we use word embedding technique, converting word to math.\n\nHere are some typical word embedding algorithms:\n\n* **Word2Vec**\n\t* skip-gram\n\t* continuous bag-of-words (CBOW)\n* **GloVe** (Global Vectors for Word Representations)\n* **FastText**\n* **ELMo** (Embeddings from Language Models)\n* **BERT** (Bidirectional Encoder Representations from Transformers)\n\t* a language model rather than a traditional word embedding algorithm. **While BERT does generate word embeddings as a byproduct of its training process**, its primary purpose is to learn contextualized representations of words and text segments.\n\n# Train a language model from scratch\n\nWe need clear the definition of language model.\n\n## Language model definition\n\nSimply to say, the language model is  a computational model or algorithm that is designed to understand and generate human language. It is a type of artificial intelligence(AI) model that uses *statistical and probabilistic techniques to predict and generate sequences of words and sentences*. \n\nIt captures the statistical relationships between words or characters and *builds a probability distribution of the likelihood of a particular word or sequence of words appearing in a given context.*\n\nLanguage model can be used for various NLP tasks, including machine translation, speech recognition, text generation and so on.... \n\nAs usual, a language model takes a seed input or prompt and uses its *learned knowledge of language(model weights)* to predict most likely words or characters to follow.\n\nThe SOTA of language model today is GPT-4.\n\n## Language model algorithm\n\n\n### Classical LM\n\n* **n-gram**\n\t* N-gram can be used as *both a tokenization algorithm and a component of a language model*. In my searching experience, n-grams are easier to understand as a language model to predict a likelihood distribution.\n* **HMMs** (Hidden Markov Models)\n* **RNNs** (Recurrent Neural Networks)\n\n### Cutting-edge\n\n* **GPT** (Generative Pre-trained Transformer)\n* **BERT** (Bidirectional Encoder Representations from Transformers)\n* **T5** (Text-To-Text Transfer Transformer)\n* **Megatron-LM**\n\n## Train Method\n\nDifferent designed models usually have different training methods. Here we take BERT-like model as example.\n\n### BERT-Like model\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/attachments/Pasted%20image%2020230629104307.png)\n\nTo train BERT-Like model, we'll train it on a task of **Masked Language Modeling**(MLM), i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset.\n\nAlso, we'll train BERT-Like model using **Next Sentence Prediction** (NSP). *MLM teaches BERT to understand relationships between words and NSP teaches BERT to understand long-term dependencies across sentences.* In NSP training, give BERT two sentences, A and B, then BERT will determine B is A's next sentence or not, i.e. outputting `IsNextSentence` or `NotNextSentence`\n\nWith NSP training, BERT will have better performance.\n\n| Task | MNLI-m (acc) | QNLI (acc) | MRPC (acc) | SST-2 (acc) | SQuAD (f1) |\n| --- | --- | --- | --- | --- | --- |\n| With NSP | 84.4 | 88.4 | 86.7 | 92.7 | 88.5 |\n| Without NSP | 83.9 | 84.9 | 86.5 | 92.6 | 87.9 |\n\n[Table source](https://arxiv.org/pdf/1810.04805.pdf)\n[Table metrics explain](computer_sci/Deep_Learning_And_Machine_Learning/LLM/metircs/some_task.md)\n\n\n# Check LM actually trained\n\n## Take BERT as example\n\nAside from looking at the training and eval losses going down, we can check our model using `FillMaskPipeline`.\n\nThis is a method input *a masked token (here, `\u003cmask\u003e`) and return a list of the most probable filled sequences, with their probabilities.*\n\nWith this method, we can see our LM captures more semantic knowledge or even some sort of (statistical) common sense reasoning.\n\n# Fine-tune our LM on a downstream task\n\nFinally, we can fine-tune our LM on a downstream task such as translation, chatbot, text generation and so on. \n\nDifferent downstream task may need different methods to do fine-tune.\n\n# Example\n\n[https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb#scrollTo=G-kkz81OY6xH](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb#scrollTo=G-kkz81OY6xH)\n\n\n# Reference\n\n* [HuggingFace blog, How to train a new language model from scratch using Transformers and Tokenizers](https://huggingface.co/blog/how-to-train)\n* [Medium blog, NLP Tokenization](https://medium.com/nerd-for-tech/nlp-tokenization-2fdec7536d17)\n* [Radford, A., Narasimhan, K., Salimans, T. \u0026 Sutskever, I. (2018). Improving language understanding by generative pre-training. , .](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\n\n","lastmodified":"2023-09-18T02:45:48.265254812Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/Model_Interpretability_MOC":{"title":"Model Interpretability - MOC","content":"\n* [SHAP](computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/SHAP.md)\n","lastmodified":"2023-09-18T02:45:48.265254812Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/SHAP":{"title":"SHAP - a reliable way to analyze model interpretability","content":"\nSHAP is the most popular model-agnostic technique that is used to explain predictions. SHAP stands for **SH**apley **A**dditive ex**P**lanations\n\nShapely values are obtained by incorporating concepts from *Cooperative Game Theory*  and *local explanations*\n\n# Mathematical and Algorithm Foundation\n\n## Shapely Values\n\nShapely values were from game theory and invented by Lloyd Shapley. Shapely values were invented to be a way of providing a fair solution to the following question:\n\n\u003e [!question] \n\u003e  If we have a coalition **C** that collaborates to produce a value **V**: How much did each individual member contribute to the final value\n\nThe method here we assess each individual member’s contribution is to removing each member to get a new coalition and then compare their production, like this graphs:\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329165429.png)\n\nAnd then, we get every member 1 included or not included coalitions like this:\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329165523.png)\n\nUsing left value - right value, we can get difference like image left above; And then we calculate the mean of them:\n\n$$\n\\varphi_i=\\frac{1}{\\text{Members}}\\sum_{\\forall \\text{C s.t. i}\\notin \\text{C}} \\frac{\\text{Marginal Contribution of i to C}}{\\text{Coalitions of size |C|}}\n$$\n\n## Shapely Additive Explanations\n\nWe need to know what’s **additive** mean here. Lundberg and Lee define an additive feature attribution as follows:\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329165623.png)\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329165818.png)\n\n$x'$, the simplified local inputs usually means that we turn a feature vector into a discrete binary vector, where features are either included or excluded. Also, the $g(x')$ should take this form:\n\n$$\ng(x')=\\varphi_0+\\sum_{i=1}^N \\varphi_i {x'}_i\n$$\n\n* $\\varphi_0$ is the **null output** of this model, that is, the **average output** of this model\n-  $\\varphi_i$ is **feature affect**, is how much that feature changes the output of the model, introduced above. It’s called **attribution**\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329165840.png)\n\nNow Lundberg and Lee go on to describe a set of three desirable properties of such an additive feature method, **local accuracy**, **missingness**, and **consistency**.\n\n### Local accuracy\n\n$$\ng(x')\\approx f(x) \\quad \\text{if} \\quad x'\\approx x\n$$\n\n### Missingness\n\n$$\n{x_i}' = 0 \\rightarrow \\varphi_i = 0\n$$\n\nif a feature excluded from the model. it’s attribution must be zero; that is, the only thing that can affect the output of the explanation model is the inclusion of features, not the exclusion.\n\n### Consistency\n\nIf feature contribution changes, the feature effect cannot change in the opposite direction\n\n# Why SHAP\n\nLee and Lundberg in their paper argue that only SHAP satisfies all three properties if **the feature attributions in only additive explanatory model are specifically chosen to be the shapley values of those features**\n\n# SHAP, step-by-step Process, same as shap.explainer\n\nFor example, we consider a ice cream shop in the airport, it has four features we can know to predict his business.\n\n$$\n\\begin{bmatrix}\n\\text{temperature} \u0026 \\text{day of weeks} \u0026 \\text{num of flights} \u0026 \\text{num of hours}\n\\end{bmatrix}\n\\\\\n\\rightarrow \\\\\n\\begin{bmatrix}\nT \u0026 D \u0026 F \u0026 H\n\\end{bmatrix}\n$$\n\nFor, example, we want to know the temperature 80 in sample [80 1 100 4] shapley value, here’s the step\n\n- Step 1. Get random permutation of features, and give a bracket to the feature we care and everything in its right. (manually)\n\n$$\n\\begin{bmatrix}\nF \u0026 D \u0026 \\underbrace{T \\quad H}\n\\end{bmatrix}\n$$\n\n- Step 2. Pick random sample from dataset\n \nFor example, [200 5 70 8], form: [F D T H]\n\n- Step 3. Form vectors $x_1 \\quad x_2$\n\n$$\nx_1=[100 \\quad 1 \\quad 80 \\quad \\color{#BF40BF} 8 \\color{#FFFFFF}] \n$$\n\n$x_1$ is partially from original sample and partially from the random chosen one, the feature in bracket will from random chosen one, exclude what we care\n\n$$\nx_2 = [100 \\quad 1 \\quad \\color{#BF40BF} 70 \\quad  8 \\color{#FFFFFF}]\n$$\n\n$x_2$  just change the feature we care into the same as random chosen one’s feature value\n\nThen, calculate the diff and record\n\n$$\nDIFF = c_1 - c_2\n$$\n\n- Step 4. Record the diff \u0026 return to step 1. and repeat many times\n\n$$\n\\text{SHAP}(T=80 | [80 \\quad 1 \\quad 100 \\quad 4]) = \\text{average(DIFF)}\n$$\n\n# Shapley kernel\n\n## Too many coalitions need to be sampled\n\nLike we introduce shapley values above, for each $\\varphi_i$ we need to sample a lot of coalitions to compute the difference. \n\nFor 4 features, we need 64 total coalitions to sample; For 32 features, we need 17.1 billion coalitions to sample.\n\nIt’s entirely untenable.\n\nSo, to get over this difficulty, we need devise a **shapley kernel**, and that’s how the Lee and Lundberg do\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329181956.png)\n\n## Detail\n![](computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329182011.png)\n\nThough most of ML models won’t just let you omit a feature, what we do is define a **background dataset** B, one that contains a set of representative data points that model was trained over. We then filled in out omitted feature of features with values from background dataset, while holding the features are included in the permutation fixed to their original values. We then take the average of the model output over all of these new synthetic data point as our model output for that feature permutation which we call $\\bar{y}$.\n\n$$\nE[y_{\\text{12i4}}\\ \\  \\forall \\ \\text{i}\\in B] = \\bar{y}_{\\text{124}}\n$$ \n![](computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329205039.png)\n\nThem we have a number of samples computed in this way,like image in left.\n\nWe can formulate this as a weighted linear regression, with each feature assigned a coefficient.\n\nAnd we can prove that, in the special choice, the coefficient can be the shaplely values. **This weighting scheme is the basis of the Shapley Kernal.** In this situation, the weighted linear regression process as a whole is Kernal SHAP.\n\n### Different types of SHAP\n\n- **Kernal SHAP**\n- Low-order SHAP\n- Linear SHAP\n- Max SHAP\n- Deep SHAP\n- Tree SHAP\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329205130.png)\n\n### You need to notice\nWe can see that, we calculate shapley values using linear regression lastly. So there must be the error here, but some python packages can not give us the error bound, so it’s confusion to konw if this error come from linear regression or the data, or the model.\n\n\n# Reference\n\n[Shapley Additive Explanations (SHAP)](https://www.youtube.com/watch?v=VB9uV-x0gtg)\n\n[SHAP: A reliable way to analyze your model interpretability](https://towardsdatascience.com/shap-a-reliable-way-to-analyze-your-model-interpretability-874294d30af6)\n\n[【Python可解释机器学习库SHAP】：Python的可解释机器学习库SHAP](https://zhuanlan.zhihu.com/p/483622352)\n\n[Shapley Values : Data Science Concepts](https://www.youtube.com/watch?v=NBg7YirBTN8)\n\n# Appendix\n\nOther methods to interprete model:\n\n[Papers with Code - SHAP Explained](https://paperswithcode.com/method/shap)","lastmodified":"2023-09-18T02:45:48.265254812Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/NLP/basic/tokenization":{"title":"Tokenization","content":"\n","lastmodified":"2023-09-18T02:45:48.273254904Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/Trick/DTW":{"title":"Dynamic Time Warping (DTW)","content":"\n![](computer_sci/Deep_Learning_And_Machine_Learning/Trick/attachments/Pasted%20image%2020230526164724.png)\n\n欧氏距离在时间序列之间可能是一个不好的选择，因为时间轴上存在扭曲的情况。DTW 是一个考虑到这种扭曲的，测量距离来比较两个时间序列的一个指标，本section讲解如何计算 DTW distance\n\n# Detail\n\n\n## Step 1.  准备输入序列\n\n假设两个time series, A \u0026 B\n\n## Step 2. 计算距离矩阵\n\n创建一个距离矩阵，其中的元素表示序列 A 和序列 B 中每个时间点之间的距离。常见的距离度量方法包括欧氏距离、曼哈顿距离、余弦相似度等。根据你的数据类型和需求选择适当的距离度量方法。\n\n## Step 3. 初始化累积距离矩阵\n\n创建一个与距离矩阵大小相同的累积距离矩阵，用于存储从起点到每个位置的累积距离。将起点 (0, 0) 的累积距离设为距离矩阵的起始点距离。\n\n## Step 4. 计算累积距离\n\n从起点开始，按照动态规划的方式计算累积距离矩阵中每个位置的累积距离。对于每个位置 (i, j)，**累积距离等于该位置的距离加上三个相邻位置中选择最小累积距离的值。**\n\n$$\nDTW(i, j) = d_{i,j} + \\min{\\{DTW(i-1,j), DTW(i, j-1), DTW(i-1, j-1)\\}}\n$$\n\n\n## Step 5. 回溯最优路径\n\n从累积距离矩阵的最右下角开始，根据最小累积距离的路径回溯到起点 (0, 0)。记录下经过的路径，即为最优路径。\n\n## Step 6. 计算最终距离\n\n根据最优路径上的累积距离，计算出最终的 DTW 距离。\n\n# Example\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Trick/attachments/Pasted%20image%2020230526170120.png)\n\n左边是距离矩阵，右边是DTW矩阵，也就是累积距离矩阵\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Trick/attachments/Pasted%20image%2020230526170921.png)\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Trick/attachments/Pasted%20image%2020230526171119.png)\n\n通过回溯，找到optimal warping path，DTW distance就是 the optimal warping path的square root，本例中就是$\\sqrt{15}$\n\n\n\n","lastmodified":"2023-09-18T02:45:48.273254904Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/Trick/quantile_loss":{"title":"Quantile loss","content":"\n在大多数现实世界的预测问题中，我们的预测所带来的不确定性具有重要价值。相较于仅仅提供点估计，了解预测范围能够显著改善许多商业应用的决策过程。**Quantile loss**就是为例帮助我们了解预测范围的loss function。\n\nQuantile loss用于衡量预测分布和目标分布之间的差异，特别适用于处理不确定性较高的预测问题。\n\n# What is quantile\n\n[Quantile](Math/Statistics/Basic/Quantile.md)\n\n# What is a prediction interval\n\n  \n预测区间是对预测的不确定性进行量化的一种方法。它为结果变量的估计提供了**概率上限和下限的范围**。\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/Trick/attachments/Pasted%20image%2020230522151015.png)\n\n输出本身是随机变量，因此具有分布特性。预测区间的目的在于了解结果的正确性可能性。\n\n# What is Quantile Loss\n\n在Quantile loss中，我们将预测结果和目标值都表示为分位数形式，例如，我们可以用预测的α分位数来表示预测结果，用真实值的α分位数来表示目标值。然后，Quantile loss衡量了这两个分布之间的差异，通常使用分位数损失函数来计算。\n\n分位数回归损失函数(Quantile Regression Loss)用于预测分位数(Quantile)。例如，对于分位数为0.9的预测，应该在90%的情况下做出过高的预测。\n\n对于一条数据，prediction是$y_i^p$，真实值是$y_i$，mean regression loss for a quantile q:\n\n$$\nL(y_i^p, y_i) = \\max[q(y_i^p - y_i), (q-1)(y_i^p - y_i)]\n$$\n\n一系列prediction数据来通过minimize这个loss function后，得到quantile - $q$\n\n\n## Intuitive Understanding\n\n在上述的回归损失方程中，由于 q 的取值范围在 0 到 1 之间，当进行过高预测（$y_i^p$ \u003e $y_i$）时，第一项将为正并占主导地位；而当进行过低预测（$y_i^p$ \u003c $y_i$）时，第二项将占主导地位。当 q 等于 0.5 时，过低预测和过高预测将受到相同的惩罚因子，从而得到中位数。q 的值越大，相比于过低预测，过高预测将受到更严厉的惩罚。例如，当 q 等于 0.75 时，过高预测将受到 0.75 的惩罚因子，而过低预测将受到 0.25 的惩罚因子。模型做出过高预测的可能性的*难度*将会是过低预测可能性的3倍，从而得到 0.75 分位数。\n\n## Why Quantile loss\n\n\u003e [!quote] \n\u003e **“同方差性”，“恒定方差假设”**\n\u003e \n\u003e 在最小二乘回归中，预测区间基于一个假设，即残差在自变量的各个取值上具有恒定的方差。这假设被称为“同方差性”或“恒定方差假设”。\n\u003e \n\u003e 这个假设是基于对回归模型中误差项的性质的一种合理假设。在最小二乘回归中，我们假设因变量的观测值是由真实值和一个误差项组成的，而这个误差项是独立同分布的，即在每个自变量取值上都具有相同的分布。\n\u003e \n\u003e 如果残差在自变量的各个取值上具有恒定的方差，意味着误差的大小不会随着自变量的变化而发生显著的变化。这样的话，我们可以使用统计方法来计算出预测区间，这个区间能够给出对未来观测值的置信度。\n\u003e \n\u003e 然而，如果恒定方差假设不成立，也就是残差在自变量的取值上具有不同的方差，那么最小二乘回归的结果可能会出现问题。在这种情况下，预测区间可能会低估或高估预测的不确定性，导致对未来观测值的置信度估计不准确。\n\nQuantile Loss Regression可以提供合理的预测区间，即使对于具有非恒定方差或非正态分布的残差也是如此\n\n\n# Reference\n\n* [Kandi, Shabeel. “Prediction Intervals in Forecasting: Quantile Loss Function.” _Analytics Vidhya_, 24 Apr. 2023, https://medium.com/analytics-vidhya/prediction-intervals-in-forecasting-quantile-loss-function-18f72501586f.](https://medium.com/analytics-vidhya/prediction-intervals-in-forecasting-quantile-loss-function-18f72501586f)","lastmodified":"2023-09-18T02:45:48.285255043Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/clustering/k-means/k_means":{"title":"K-means Clustering Algorithm","content":"\n# Step by Step\n\nOur algorithm works as follows, assuming we have inputs $x_1, x_2, \\cdots, x_n$ and value of $K$\n\n- **Step 1** - Pick $K$ random points as cluster centers called centroids.\n- **Step 2** - Assign each $x_i$ to nearest cluster by calculating its distance to each centroid.\n- **Step 3** - Find new cluster center by taking the average of the assigned points.\n- **Step 4** - Repeat Step 2 and 3 until none of the cluster assignments change.\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/clustering/k-means/attachments/k4XcapI.gif)\n\n# Implementation\n\n## Core code\n\n### Distance calculation:\n\n```python\n# Euclidean Distance Caculator\ndef dist(a, b, ax=1):\n    return np.linalg.norm(a - b, axis=ax)\n```\n\n\n### Generate Random Clustering center at first\n\n```python\n# Number of clusters\nk = 3\n# X coordinates of random centroids\nC_x = np.random.randint(0, np.max(X)-20, size=k)\n# Y coordinates of random centroids\nC_y = np.random.randint(0, np.max(X)-20, size=k)\nC = np.array(list(zip(C_x, C_y)), dtype=np.float32)\nprint(C)\n```\n\n### Calculate dis and tag point, then update every tag's new center\n\n```python\n# To store the value of centroids when it updates\nC_old = np.zeros(C.shape)\n# Cluster Lables(0, 1, 2)\nclusters = np.zeros(len(X))\n# Error func. - Distance between new centroids and old centroids\nerror = dist(C, C_old, None)\n# Loop will run till the error becomes zero\nwhile error != 0:\n    # Assigning each value to its closest cluster\n    for i in range(len(X)):\n        distances = dist(X[i], C)\n        cluster = np.argmin(distances)\n        clusters[i] = cluster\n    # Storing the old centroid values\n    C_old = deepcopy(C)\n    # Finding the new centroids by taking the average value\n    for i in range(k):\n        points = [X[j] for j in range(len(X)) if clusters[j] == i]\n        C[i] = np.mean(points, axis=0)\n    error = dist(C, C_old, None)\n```\n\n## Simple approach by scikit-learn\n\n```python\nfrom sklearn.cluster import KMeans\n\n# Number of clusters\nkmeans = KMeans(n_clusters=3)\n# Fitting the input data\nkmeans = kmeans.fit(X)\n# Getting the cluster labels\nlabels = kmeans.predict(X)\n# Centroid values\ncentroids = kmeans.cluster_centers_\n\n# Comparing with scikit-learn centroids\nprint(C) # From Scratch\nprint(centroids) # From sci-kit learn\n```\n\n# Application \n\n## 8bit style\n\nRead image and use k-means to do clustering for pixel value. Make pic to 8bit color style.\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/clustering/k-means/attachments/3ed5fee41bd566be093bebd62a33d12.jpg)\n\n[color8bit_style.py](https://github.com/PinkR1ver/Jude.W-s-Knowledge-Brain/blob/master/Deep_Learning_And_Machine_Learning/clustering/k-means/application/color8bit_style.py)\n\n# Reference\n\n* [K-Means Clustering in Python, https://mubaris.com/posts/kmeans-clustering/. Accessed 3 July 2023.](https://mubaris.com/posts/kmeans-clustering/)","lastmodified":"2023-09-18T02:45:48.297255181Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/AdaBoost":{"title":"AdaBoost","content":"\n# Video you need to watch first\n\n* [AdaBoost, Clearly Explained](https://www.youtube.com/watch?v=LsK-xG1cLYA)\n\n# Key words and equation\n\n- **Stump(树桩) means classification just by one feature**\n- Amount of say\n\n$$\n\\text{Amout of say} = \\frac{1}{2}\\log{(\\frac{1-\\text{Total Error}}{\\text{Total Error}})}\n$$\n\n- Wrong Classified Sample New Weight\n\n$$\n\\text{New Sample Weight} = \\text{Sample Weight}\\times e^{\\text{amount of say}}\n$$\n\n- Correct Clasified Sample New Weight\n\n$$\n\\text{New Sample Weight} = \\text{Sample Weight}\\times e^{-\\text{amount of say}}\n$$\n\n- After reassing sample weight, do bootstrap sample based on their new weight, it will select big weight sample lots of times to adjust next model\n- In last prediction, the **amount of say** decide which results we will pick.\n\n# Question\n\n- **[why decision stumps instead of trees?](https://stats.stackexchange.com/questions/520667/adaboost-why-decision-stumps-instead-of-trees)**","lastmodified":"2023-09-18T02:45:48.297255181Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Attention":{"title":"⭐Attenion","content":"# Self-Attention\n\n讲述self-attention我们以*sequence labeling*任务作为任务来讲解，sequence labeling的任务是输入N个vector并且输出N个label。\n\n典型的例子有输入一个句子，分析每个词汇的词性是什么，比如句子“I saw a saw”，这个句子里saw和saw的词性分别是verb和nonu，如果我们用fully-connected（FC）层来做的话，那么面对同样的输入saw，我们无法得出不同的结果。\n\n![Pasted image 20230315195403](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/1.png)\n\n我们的做法可以是对输入加窗，考虑周边邻近的词汇信息，这与信号处理常用的方法类似，但是窗的长度是有限且固定的，而seq的长度是变化的，因此我们在面对这种任务的时候，我们可以借助**self-attention**层。\n\n## Detail\n\n![Pasted image 20230315195603](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230315195603.png)\n\n对于Self-attention层，生成的$b^i$向量是考虑到所有输入$\\sum_i\\alpha^i$向量\n\n### Vector Relevance\n\n![250](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230315200009.png)\n\n\n* *Step 1.* 使用Dot-product 去计算 vector relevance\n\n![400](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230315201906.png)\n\n* *Step 2.* Normalizing计算出来的vector relevance\n![400](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230315202047.png)\n\n* *Step 3.*  根据vector relevance，也就是attention scores计算最后的输出。这是一个Reweighting Process，一个extract information based on attention scores\n\n![400](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230315202314.png)\n\n\u003e [!hint] \n\u003e  从上面的过程中，可以看出，$b^i$互相之间的计算没有关系，具有很好的并行性\n\n### Matrix Detail\n\n$$\nq^i = W^q \\alpha^i\n$$\n\n\n$$\nQ = [q^1 \\quad q^2 \\quad \\cdots \\quad q^N],\\ \\  I = [\\alpha^1 \\quad \\alpha^2 \\quad \\cdots \\quad \\alpha^N]\n$$\n\n\n\nSo,\n\n$$\nQ = W^q I\n$$\n\nAs same,\n$$\nK = W^k I,\\quad V = W^v I\n$$\nCalculate attention score $\\alpha$,\n$$\n\\begin{bmatrix}\n\\alpha_{1,1} \\\\\n\\alpha_{1,2} \\\\\n\\cdots \\\\\n\\alpha_{1,N}\n\\end{bmatrix} =\n\\begin{bmatrix}\nk^1 \\\\\nk^2 \\\\\n\\cdots \\\\\nk_N\n\\end{bmatrix} q^1\n$$\n\nSo,\n$$\nA=\\begin{bmatrix}\n\\alpha_{1,1} \u0026 \\alpha_{2,1} \u0026 \\cdots \u0026 \\alpha_{N,1} \\\\\n\\alpha_{1,2} \u0026 \\alpha_{2,2} \u0026 \\cdots \u0026 \\alpha_{N,2} \\\\\n\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots\\\\\n\\alpha_{1,N} \u0026 \\alpha_{2,N} \u0026 \\cdots \u0026 \\alpha_{N,N}\n\\end{bmatrix} =\n\\begin{bmatrix}\nk^1 \\\\\nk^2 \\\\\n\\cdots \\\\\nk_N\n\\end{bmatrix} [q^1 \\quad q^2 \\quad \\cdots \\quad q^N] = K^TQ\n$$\n\n$$\nA' = \\text{Softmax}(A)\n$$\n\nFinally, calculate output $b$\n\n$$\nO = [b^1 \\quad b^2 \\quad \\cdots \\quad b^N] = [v^1 \\quad v^2 \\quad \\cdots \\quad v^N] = VA'\n$$\n\n![600](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230315205148.png)\n\n### Positional Encoding\n![250](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230315205727.png)\n* Each position has a unique positional vector $e^i$\n\t* hand-crafted\n\t* learned from data\n\n## Fun Facts\n\n### Self-attention vs. CNN\n\n![Pasted image 20230315205918](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230315205918.png)\n\n因为transformer有着更大的function set，所以需求更多的数据; ![Pasted image 20230315210032](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230315210032.png)\n\n### Self-attention vs. RNN\n\n目前，RNN的角色正在被self-attention替代，RNN在long seq的情况下，前面的信息会被逐渐遗忘；同时**RNN没有并行性**\n同样，Self attention有着比RNN更大的function set，在某些情况下，self-attention可以变成RNN\n\n# Multi-head Self-attention\nMulti-head self attention就是由不同的self attention layer在一起，有不同的$W^q$,$W^k$来负责不同种类的relevance\n\n![600](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230315210631.png)\n![300](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230315210704.png) ","lastmodified":"2023-09-18T02:45:48.357255874Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Decision_Tree":{"title":"Decision Tree","content":"\nOnly vedio here:\n\n* [Decision and Classification Trees, Clearly Explained!!!](https://www.youtube.com/watch?v=_L39rN6gz7Y\u0026t=229s \"Decision and Classification Trees, Clearly Explained!!!\")\n* [Regression Trees, Clearly Explained!!!](https://www.youtube.com/watch?v=g9c66TUylZ4\u0026t=789s \"Regression Trees, Clearly Explained!!!\")\n\n","lastmodified":"2023-09-18T02:45:48.297255181Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Deep_Neural_Decision_Forests":{"title":"Deep Neural Decision Forests","content":"\n# Background\n\n* [Decision Tree](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Decision_Tree.md)\n* [Random Forest](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Random_Forest.md)\n\n# What is Deep Neural Decision Forests\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230413112822.png)\n\nDeep Neural Decision Forests(dNDFs)是Neural Networks和Random Forest的结合，但是它更倾向于Neural Networks。它本质上是Nerual Networks incorporate Random Forest来提高NN的效率和准确度，训练方法和NN一致。\n\ndNDFs与NN的不同在output layer层发生变化，不单纯使用FC层输出，而是使用随机森林作为最后一层的分类器，相当于通过前面系统输出的data representation用随机森林作为分类器分类。**同时，通过将传统随机森林的local optimize改造成通过back propagation进行global optimize,随机森林的参数训练可以与前端的深度学习网络进行无缝衔接。**\n\n\u003e [!attention] \n\u003e  The method is different from random forest in the sense that it uses a principled, joint and global optimization of split and leaf node parameters and from conventional deep networks because a decision forest provides the final predictions\n\n# Math in Neural Decision Forests\n\nDecision Tree model要是stochastic的，为了让它differentiable，让后面可以通过back-propagation训练。在传统的decision tree模型中，从node到leaf的路径是由decision function确定的，而在这个模型中，我们将用two sets of probabilities去决定final output。\n\n1. Probability of an observation reaching to a leaf . These basically are associated with decision node/split node which decides whether an observation goes left or right\n2. Once an observation reaches a leaf node, probability that it takes a specific class\n\n \n\n# Reference\n\n* [Deep Neural Decision Forests - YouTube Vedio by  Venkatesh Bingi](https://www.youtube.com/watch?v=Uaimgqv75dY)\n* [Deep Neural Decision Forests - Medium by Gurparkash Singh Sohi](https://blog.goodaudience.com/deep-neural-decision-forests-b1dd39c4c6ce)\n* [Deep neural decision forest in keras - Medium by Kushal Mukherjee](https://kushalmukherjee.medium.com/deep-neural-decision-forest-in-keras-60134d270bfe)\n\n","lastmodified":"2023-09-18T02:45:48.297255181Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/GRU":{"title":"Gated Recurrent Unit","content":"\n","lastmodified":"2023-09-18T02:45:48.297255181Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/LSTM":{"title":"Long Short-Term Memory Networks","content":"\n\u003e [!quote] \n\u003e When I was learning LSTM, the new deep learning block *Transformers* dominate the NLP field. However, *Transformers* don't decisively outperform LSTMS in time-series-related tasks. The main reason is that LSTMs are more adept at handling **local temporal data**. \n\n\nLSTM的设计目标是解决传统RNN面临的长期依赖问题。传统RNN在处理长序列时，难以记住远距离的信息，因为随着时间的推移，梯度在传播过程中逐渐消失或爆炸。这使得传统RNN难以捕捉长期依赖关系，例如在自然语言处理中理解长句子的语义。\n\nLSTM通过使用一种称为门控机制的技术，有效地解决了这个问题。它包含一个称为记忆单元的重要组件，这个单元可以选择性地存储、读取和删除信息。LSTM的关键在于其三个门控单元：输入门、遗忘门和输出门。\n\n1.  输入门（Input Gate）：决定哪些信息将被更新到记忆单元中。它使用一个Sigmoid激活函数来控制输入的重要性。\n    \n2.  遗忘门（Forget Gate）：决定哪些信息将被从记忆单元中删除。通过使用另一个Sigmoid激活函数和一个逐元素的乘法操作，它决定了上一个记忆状态中的哪些信息保留下来。\n    \n3.  输出门（Output Gate）：决定将哪些信息从记忆单元输出到下一个时间步。这个输出经过一个Sigmoid激活函数和一个Tanh激活函数来进行处理。\n    \n\n这些门控单元允许LSTM选择性地记住或忘记特定的信息，从而使其能够有效地处理长序列。LSTM的网络结构使得信息可以在时间上流动，同时保留对过去信息的长期记忆。\n\n# Arch\n\n可以通过比较传统RNN模块和LSTM模块来加深记忆\n\n传统RNN网络：\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230522161052.png)\n\n\nLSTM模块：\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230522161520.png)\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230522161546.png)\n\n## Core idea\n\nLSTM的core idea是cell state, cell state可以被视为一个横贯整个LSTM网络的内部记忆。它类似于传统RNN中的隐藏状态，但相比之下，cell state的设计更加精细，使得LSTM能够更好地捕捉长期依赖关系。\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230522162225.png)\n\ncell state的更新是通过门控单元来控制的。在LSTM中，输入门、遗忘门和输出门共同决定了如何更新细胞状态。\n\n\n## Step-by-Step LSTM Walk Through\n\n### Step 1 - Throw away information\n\nLSTM第一步是throw away information，通过遗忘门(forget gate layer)。\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230522162536.png)\n\nforget gate layer 通过输入$x_t$和$h_{t-1}$，计算出$f_t$，$f_t$范围在（0，1），这个$f_t$会去乘以cell state $C_{t-1}$。1代表着“completely keep”，0代表着“completely get rid of this”\n\n一个好的例子，在nlp中，cell state可能包括当前主体的性别，以便可以使用正确的代词。 当我们看到一个新主题时，我们想忘记旧主题的性别。\n\n### Step 2  - Decide What information we're going to store\n\nLSTM第二步在于决定哪些信息要被store在cell state里，这里有两个部分，第一个部分是通过\"input gate layer\"（输入门），计算$i_t$。第二个部分通过一个tanh layer来计算新候选值的向量 $\\tilde{C}_t$。这两个部分将会用来update information in cell state\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230522163353.png)\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230522164237.png)\n\n### Step 3 - Decide output\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230522164609.png)\n\n最终的输出回事一个filtered version of cell state，计算如上图。\n\n# Variants on LSTM\n\nLSTM有很多变种，这里有列出来一些\n\n## Adding \"peephole connections\"  \n\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230522165117.png)\n\n在gate layer的输入中加入cell state，你可以选择在这三个门里的某些加入“peephole connection”（窥视孔连接），某些不加入。\n\n加入窥视孔连接的目的是增强LSTM对细胞状态的建模能力，并更好地捕捉序列中的长期依赖关系。\n\n## Use coupled forget and input gates\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230522170059.png)\n\n\n## GRU (Gated Recurrent Unit) ⭐⭐⭐\n\n* [GRU](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/GRU.md)\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230522170214.png)\n\nGRU是著名的LSTM变种，值得另起炉灶介绍\n\n\n# Demo code \u0026 Pytorch version LSTM graph explain\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230523164806.png)\n\n```python\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass LSTM(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size, num_layers):\n        super(LSTM, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n        \n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_seq):\n        # input_seq: (seq_len, batch, input_size)\n        # lstm_out: (seq_len, batch, hidden_size)\n\n        lstm_out, (hidden_state, cell_state) = self.lstm(input_seq)\n\n        lstm_out = self.fc(lstm_out)\n\n        return lstm_out, hidden_state, cell_state\n    \n\nif __name__ == '__main__':\n    seq = np.linspace(0, 3801, 3801)\n    h = torch.randn(1, 1, 64)\n    c = torch.randn(1, 1, 64)\n\n    lstm = LSTM(1, 1, 64, 1)\n\n    input = torch.Tensor(seq).view(len(seq), 1, -1)\n\n    lstm_out, hidden_state, cell_state = lstm(input)\n    lstm_out = torch.squeeze(lstm_out)\n\n    print(lstm_out.shape)\n    print(hidden_state.shape)\n    print(cell_state.shape)\n```\n\n# Reference\n\n* _Understanding LSTM Networks -- Colah’s Blog_. https://colah.github.io/posts/2015-08-Understanding-LSTMs/. Accessed 22 May 2023.\n* Hochreiter, Sepp, and Jürgen Schmidhuber. “Long Short-Term Memory.” _Neural Computation_, vol. 9, no. 8, Nov. 1997, pp. 1735–80. _DOI.org (Crossref)_, https://doi.org/10.1162/neco.1997.9.8.1735.\n* _Recurrent Nets That Time and Count_. https://ieeexplore.ieee.org/document/861302/. Accessed 22 May 2023.\n* ","lastmodified":"2023-09-18T02:45:48.297255181Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Random_Forest":{"title":"Random Forest","content":"\n# Background\n\n* [Decision Tree](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Decision_Tree.md)\n\n# Detail\n\nonly vedio here:\n\n* [StatQuest: Random Forests Part 1 - Building, Using and Evaluating](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ\u0026t=32s \"StatQuest: Random Forests Part 1 - Building, Using and Evaluating\")\n\n","lastmodified":"2023-09-18T02:45:48.297255181Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Transformer":{"title":"Transformer","content":"\n\u003e [!info] \n\u003e 在学习Transformer前，你需要学习 [⭐Attention](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/⭐Attention.md)\n\n\n\nTransformer 是Seq2Seq model，由Encoder和Decoder组成\n![300](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230316160103.png)\n\n# Encoder\n这里贴的是原文Encoder的架构\n![Pasted image 20230316162635](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230316162635.png)\n\n![Pasted image 20230316162642](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Pasted%20image%2020230316162642.png)","lastmodified":"2023-09-18T02:45:48.297255181Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/XGBoost":{"title":"XGBoost","content":"\n\nXGBoost is an open-source software library that implements optimized distributed gradient boosting machine learning algorithms under the **Gradient Boosting** framework.\n\n# What you need to know first\n\n* [🚧🚧AdaBoost](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/AdaBoost.md)\n\n# What is XGBoost\n\n**XGBoost**, which stands for Extreme Gradient Boosting, is a scalable, distributed **gradient-boosted** decision tree (GBDT) machine learning library. It provides parallel tree boosting and is the leading machine learning library for regression, classification, and ranking problems.\n\nIt’s vital to an understanding of XGBoost to first grasp the machine learning concepts and algorithms that XGBoost builds upon: **supervised machine learning**, **decision trees**, **ensemble learning**, and **gradient boosting**.\n\nHere, we need to know **ensemble learning** and **gradient boosting,** this two thing I don’t konw before.\n\n## What is Ensemble Learning(集成学习)\n\n**Ensemble learning** is a general meta approach to machine learning that **seeks better predictive performance by combining the predictions from multiple models**.\n\nThe three main classes of ensemble learning methods are **bagging**, **stacking**, and **boosting.**\n\n### Bagging\n\nBagging means **Bootstrap aggregation.** It’s an ****ensemble learning method that seeks a diverse group of ensemble members by **varying the training data**.\n\nThis typically involves using a single machine learning algorithm, almost always an unpruned decision tree, and **training each model on a different sample of the same training dataset.** The predictions made by the ensemble members are then **combined using simple statistics, such as voting or averaging.**\n\nKey to the method is the manner in which each sample of the dataset is prepared to train ensemble members. Each model gets its own unique sample of the dataset.\n\nBagging adopts the **bootstrap distribution** for generating **different base learners**. In other words, it applies **bootstrap sampling** to obtain the data subsets for training the base learners.\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Untitled.png)\n\n\u003caside\u003e\n💡 **Bootstrap Sampling\nSelect a sample(a row of data), then reture the sample to dataset and re-select another sample to aggregate a data sample dataset. It means a sample can be selected zero, one, or mulitple times for a given dataset.**\nBootstrap sampling ****is often used in statistics with **small dataset**. geive a better overall estimate of the desired quantity than simply estimating from the whole dataset directly.\n\n\u003c/aside\u003e\n\nKey word of bagging method:\n\n- **Bootstrap Sampling**\n- **Voting or averaging of predictions**\n- **Unpruned decision tree**\n\n\u003e Random forest is the typical example based on the bagging method.\n\u003e \n\n### Stacking\n\nStacking means **Stacked Generalization**. It is an ensemble method that seeks a diverse group of members by **varying the model types** fit on the training data and using a model to combine predictions.\n\n\u003e *Stacking is a general procedure where a learner is trained to combine the individual learners. Here, the individual learners are called the first-level learners, while the combiner is called the second-level learner, or meta-learner.*\n\u003e \n\nStacking has its own nomenclature where ensemble members are referred to as **level-0 models** and the model that is used to combine the predictions is referred to as a **level-1 model**.\n\nThe two-level hierarchy of models is the most common approach, although more layers of models can be used. For example, instead of a single level-1 model, we might have 3 or 5 level-1 models and a single level-2 model that combines the predictions of level-1 models in order to make a prediction.\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Untitled%201.png)\n\nKey words of stacknig method:\n\n- **Unchanged training dataset**\n- **Different machine learning algorithms for each ensemble member**\n- **Machine learning model to learn how to best combine predictions**\n\n### Boosting\n\n**Boosting** is an ensemble method that seeks to change the training data to focus attention on examples that previous fit models on the training dataset have gotten wrong.\n\n\u003e *In boosting, […] the training dataset for each subsequent classifier increasingly focuses on instances misclassified by previously generated classifiers.*\n\u003e \n\nThe key property of boosting ensembles is the idea of **correcting prediction errors**. The models are fit and added to the ensemble sequentially such that the second model attempts to correct the predictions of the first model, the third corrects the second model, and so on.\n\nThis typically involves the use of very simple decision trees that only make a single or a few decisions, referred to in boosting as weak learners. The predictions of the weak learners are combined using simple voting or averaging, although **the contributions are weighed proportional to their performance or capability**. The objective is to develop a so-called “***strong-learner***” from many purpose-built “***weak-learners***”.\n\nTypically, the training **dataset is left unchanged** and instead, the learning algorithm is modified to **pay more or less attention to specific samples based on whether they have been predicted correctly or incorrectly** by previously added ensemble members. \n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Untitled%202.png)\n\nKey words to boosting method:\n\n- **Bias training data** toward those examples that are hard to predict\n- **Iteratively add ensemble members to correct predictions of prior models**\n- Combine predictions **using a weighted average** of models\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Untitled%203.png)\n\nType of boosting:\n\n- Adaptive boosting\n- Gradient boosting\n- Extreme gradient boosting\n\n# Introduction to three main type of boosting method\n\n## [Adaptive boosting](https://www.notion.so/AdaBoost-8e7009e35aee4334b31d46bfd7e3dbba)\n\nAdaptive Boosting (AdaBoost) was one of **the earliest boosting models** developed. It adapts and tries to **self-correct** in every iteration of the boosting process.\n\nAdaBoost initially gives the same weight to each dataset. Then, it automatically adjusts the weights of the data points after every decision tree. It **gives more weight to incorrectly classified items** to correct them for the next round. It repeats the process until the residual error, or the difference between actual and predicted values, falls below an acceptable threshold.\n\nYou can use AdaBoost with many predictors, and it is typically not as sensitive as other boosting algorithms. This approach does not work well when there is a correlation among features or high data dimensionality. Overall, **AdaBoost is a suitable type of boosting for classification problems**.\n\n**Must check Learning material below to know more detail of this algorithm. 🚧🚧🚧**\n\n## Gradient boosting\n\nGradient Boosting (GB) is similar to AdaBoost in that it, too, is a **sequential training technique**. The difference between AdaBoost and GB is that GB does not give incorrectly classified items more weight. Instead, GB software **optimizes the loss function by generating base learners sequentially** so that **the present base learner is always more effective than the previous one**. This method **attempts to generate accurate results initially instead of correcting errors throughout the process**, like AdaBoost. For this reason, GB software can lead to more accurate results. Gradient Boosting can help with both classification and regression-based problems.\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/attachments/Untitled%204.png)\n\n## Extreme gradient boosting\n\nExtreme Gradient Boosting (XGBoost) improves gradient boosting for **computational speed and scale** in several ways. XGBoost uses multiple cores on the CPU so that learning can occur in parallel during training. It is a boosting algorithm that can handle extensive datasets, making it attractive for big data applications. The key features of XGBoost are parallelization, distributed computing, cache optimization, and out-of-core processing.\n\n# Reference\n\n## XGBoost\n\n* [What is XGBoost?](https://www.nvidia.com/en-us/glossary/data-science/xgboost/)\n\n* [XGBoost Part 1 (of 4): Regression](https://www.youtube.com/watch?v=OtD8wVaFm6E)\n\n## Ensemble Learning\n\n* [A Gentle Introduction to Ensemble Learning Algorithms - MachineLearningMastery.com](https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/)\n\n* [集成学习(ensemble learning)原理详解_Soyoger的博客-CSDN博客_ensemble l](https://blog.csdn.net/qq_36330643/article/details/77621232)\n\n* [What is Boosting? Guide to Boosting in Machine Learning - AWS](https://aws.amazon.com/what-is/boosting/)\n\n* [Regression Trees, Clearly Explained!!!](https://www.youtube.com/watch?v=g9c66TUylZ4\u0026list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF\u0026index=45)\n\n* [AdaBoost, Clearly Explained](https://www.youtube.com/watch?v=LsK-xG1cLYA)\n\n* [Gradient Boost Part 1 (of 4): Regression Main Ideas](https://www.youtube.com/watch?v=3CC4N4z3GJc)\n","lastmodified":"2023-09-18T02:45:48.297255181Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/deep_learning_MOC":{"title":"Deep Learning MOC","content":"\n\n# Attention is all you need\n\n* [[computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/⭐Attention|Attention Blocker]]\n* [[computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Transformer|Transformer]]\n\n\n# Tree-like architecture\n\n* [Decision Tree](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Decision_Tree.md)\n* [Random Forest](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Random_Forest.md)\n* [Deep Neural Decision Forests](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Deep_Neural_Decision_Forests.md)\n* [XGBoost](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/XGBoost.md)\n\n\n# Ensemble Learning\n\n* [AdaBoost](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/AdaBoost.md)\n* [XGBoost](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/XGBoost.md)\n\n\n# Time-series dealing block\n\n* [LSTM](computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/LSTM.md)\n\n# Clustering Algorithm\n\n\n* [K-means Clustering Algorithm](computer_sci/Deep_Learning_And_Machine_Learning/clustering/k-means/k_means.md)","lastmodified":"2023-09-18T02:45:48.357255874Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/machine_learning/MOC":{"title":"Machine Learning MOC","content":"* [SVM](computer_sci/Deep_Learning_And_Machine_Learning/machine_learning/SVM.md)","lastmodified":"2023-09-18T02:45:48.357255874Z","tags":null},"/computer_sci/Deep_Learning_And_Machine_Learning/machine_learning/SVM":{"title":"Support Vector Machine","content":"\n# Overview\n\n![](computer_sci/Deep_Learning_And_Machine_Learning/machine_learning/attachments/Pasted%20image%2020230904225904.png)\n\n# Hyper Parameters\n\n## Kernel Function\n\n* Linear\n* Polynomial\n* RBF\n\t* $\\gamma$: The gamma parameter **defines the influence of each training example on the decision boundary**. A higher gamma value gives more weight to the closer points, while a lower value allows points further away to have a significant impact. Higher values of gamma can lead to overfitting, especially in datasets with noise.\n## C Parameter\n\nThe C parameter, also known as the regularization parameter, controls the trade-off between maximizing the margin and minimizing the classification error. **A smaller C value allows for a larger margin but may lead to misclassification of some training examples, while a larger C value focuses on classifying all training examples correctly but might result in a narrower margin**\n## [Training Method](https://wadhwatanya1234.medium.com/multi-class-classification-one-vs-all-one-vs-one-993dd23ae7ca)\n\n* One-vs-All\n* One-vs-One\n# Detail\n\n## Score Function\n\n$$\nf(x) = \\sum_i \\alpha_i y_i G(x, x_i) + bias\n$$\n* $\\alpha_i$ is corresponding support vector weight\n* $y_i$ is corresponding support vector tags\n* $G(x,x_i)$ is kernel function about input sample $x$ and support vector $x_i$\n* $bias$ is bias\n## Decision Function \n\n$$\nDecision \\ Function = sign(f(x))\n$$\nWe determine the sample's category by checking its decision function's sign.\n# Reference\n\n* [“华为开发者论坛.” _Huawei_, https://developer.huawei.com/consumer/cn/forum/topic/41598169. Accessed 4 Sept. 2023.](https://developer.huawei.com/consumer/cn/forum/topic/41598169)\n* [Multi-class Classification — One-vs-All \u0026 One-vs-One](https://wadhwatanya1234.medium.com/multi-class-classification-one-vs-all-one-vs-one-993dd23ae7ca)\n* [Saini, Anshul. “Guide on Support Vector Machine (SVM) Algorithm.” _Analytics Vidhya_, 12 Oct. 2021, https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/.](https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/)","lastmodified":"2023-09-18T02:45:48.357255874Z","tags":null},"/computer_sci/Hardware/Hardware_MOC":{"title":"Hardware - MOC","content":"\n# Microcontroller unit (MCU)\n\n## Basic concepts\n\n* [Different programming interfaces](computer_sci/Hardware/MCU/Different%20programming%20interfaces.md)\n","lastmodified":"2023-09-18T02:45:48.357255874Z","tags":null},"/computer_sci/Hardware/MCU/Different-programming-interfaces":{"title":"Different programming interfaces","content":"# What is programming interfaces in MCU\n\nA **programming interface** is a device that allows a programmer to connect to a microcontroller (MCU) and program it. The programming interface is used to load the program into the MCU’s memory and debug it.\n\n# Different types of programming interfaces in MCU\nChipmakers have different names for programming interfaces that all basically do the same thing:\n-   ISP - programming interface for Atmel (now Microchip) AVRs. SPI-like (MISO, MOSI, SCK, reset). It can be used for flash programming and debugging.\n-   PDI - newer programming interface for Atmel AVRs (eg. Xmega). Uses two wires (data and clock). Can do the same as ISP.\n-   DebugWire - yet another interface from Atmel (this one uses only a single wire)\n-   ICSP - programming interface for Microchip PIC line of MCUs\n-   SWD - Serial Wire Debug - programming interface for MCUs with ARM Cortex-M cores (uses two wires - data and clock)\n-   JTAG - very generic term, SPI-like interface used for [boundary scan](https://en.wikipedia.org/wiki/Boundary_scan), can also be used for programming/debugging MCUs (almost every vendor has its own protocol, so Cortex-M JTAG is not the same as AVR JTAG or Blackfin JTAG)\n-   Spy-Bi-Wire - yet another two wire programming interface, this one is for TI's MSP430 MCUs\n\n## SWD 和 JTAG的区别\n\n目前在使用的st link可以使用SWD和JTAG这两种debugger去调试stm32，所以这两种方式的区别令人比较在意；\n* JTAG（Joint Test Action Group，联合测试行动小组）是一种国际标准测试协议，主要用于芯片内部测试。现在多数的高级器件都支持JTAG协议，如ARM、DSP、FPGA器件等。JTAG调试接口必须使用VCC、GND电源信号，以及TMS、TCK、TDI、TDO四根调试信号，可选TRST、RESET复位信号和RTCK（同步时钟）信号。\n\t* TMS(Test Mode Select)：模式选择，TMS用来设置JTAG接口处于某种特定的测试模式；\n\t* TCK(Test Clock)：时钟输入；\n\t* TDI(Test Data Input)：数据输入，数据通过TDI引脚输入JTAG接口；\n\t* TDO(Test Data Output)：数据输出，数据通过TDO引脚从JTAG接口输出；\n* 串行调试（Serial Wire Debug），是一种和JTAG不同的调试模式，使用的调试协议也不一样，所以最直接的体现在调试接口上，与JTAG的20个引脚相比，SWD只需要4个（或者5个）引脚，结构简单，但是使用范围没有JTAG广泛，主流调试器上也是后来才加的SWD调试模式。\n\t* SWDIO：串行数据输入输出，作为仿真信号的双向数据信号线，建议上拉；\n\t* SWCLK：串行时钟输入，作为仿真信号的时钟信号线，建议下拉；\n\t* SWO：串行数据输出引脚，CPU调试接口可通过SWO引脚输出一些调试信息。该引脚是可选的；\n\t* RESET：仿真器输出至目标CPU的系统复位信号，该引脚也为可选\n\n* SWD模式比JTAG在高速模式下面更加可靠。在大数据量的情况下面JTAG下载程序会失败，但是SWD发生的几率会小很多。*基本使用JTAG仿真模式的情况下是可以直接使用SWD模式的，只要你的仿真器支持。*\n* 在GPIO刚好缺一个的时候，可以使用SWD仿真，这种模式支持更少的引脚。\n\n\n* 同时JTAG调试版本不同的情况下：\n\t* JTAGV6 需要的硬件接口为: GND, RST, SWDIO, SWDCLK；\n\t* JTAGV7 需要的硬件接口为: GND, RST, SWDIO, SWDCLK，相对V6， 其速度有了明显的提高，速度是 JTAGV6 的 6 倍。 \n\t* JTAGV8 需要的硬件接口为: VCC, GND, RST, SWDIO, SWDCLK，速度可以到 10M。\n\n\n\n# Reference\n\n[JTAG, SWD, EDBG, ICSP, ISP terms - Electrical Engineering Stack Exchange](https://electronics.stackexchange.com/questions/412029/jtag-swd-edbg-icsp-isp-terms)\n\n[jtag和swd的区别_jtag和swd区别_耶稣赞我萌的博客-CSDN博客](https://blog.csdn.net/yym6789/article/details/88721409)\n\n[STM32的JTAG和SWD模式_学术马的博客-CSDN博客](https://blog.csdn.net/w1050321758/article/details/108663603)\n","lastmodified":"2023-09-18T02:45:48.357255874Z","tags":null},"/computer_sci/coding_knowledge/coding_lang_MOC":{"title":"About coding language design detail","content":"\n# Python\n\n[Why python doesn't need pointer?](computer_sci/coding_knowledge/python/python_doesnt_need_pointer.md)\n\n# C\n\n# MATLAB\n\n# JavaScript\n\n","lastmodified":"2023-09-18T02:45:48.357255874Z","tags":null},"/computer_sci/coding_knowledge/python/matplotlib_backend":{"title":"Matplotlib Backend Review","content":"\n","lastmodified":"2023-09-18T02:45:48.357255874Z","tags":null},"/computer_sci/coding_knowledge/python/python_doesnt_need_pointer":{"title":"Why python doesn't need pointer?","content":"\n\nPython doesn't require the explicit use of pointers like C because of its **underlying memory management** and **object model**.\n\n# Design Concept\n\n## Underlying memory management\n\nIn Python, variables are *references to objects rather than memory addresses* like pointers in C. When you assign a value to a variable in Python, you are actually creating a reference to an object in memory. This reference allows you to access and manipulate the object, but you don't need to manage memory explicitly.\n\nPython uses automatic memory management through a mechanism called **garbage collection**. It keeps track of objects in memory and automatically deallocates memory for objects that are no longer referenced or used. **This automatic memory management frees developers from the responsibility of explicitly allocating and deallocating memory using pointers.**\n\n## Object model\n\n\nFor python, everything is object, objects contains at least three pieces of data:\n\n* Reference count\n* Type\n* Value\n\nReference count is an interesting concept in python, it's designed for memory management. \n\n\u003e [!help] \n\u003e Reference count refers to the number of references an object. Each object in Python contains a reference count, which is a count of how many references or variables are currently pointing to that object. \n\u003e \n\u003e The reference count mechanism is part of Python's memory management system. When an object is created or assigned to a variable, the reference count of that object is incremented. When a reference to an object is deleted or reassigned, the reference count is decremented. When the reference count of an object reaches zero, it means that there are no more references to that object, and the memory occupied by the object can be deallocated.\n\u003e \n\u003e The reference count mechanism help python build its **garbage collection** memory management.\n\nPython's object model also provides additional benefits. For example, *objects in Python can have different types, and variables can be dynamically reassigned to different objects of different types without any explicit type declarations or memory management.*\n\n# Example\n\nGenerating by [ChatGPT](https://chat.openai.com/):\n\n**Example about swap**\n\n--- \nCode:\n\n```python\ndef swap_variables(a, b):\n    return b, a\n\n# Example usage\nx = 10\ny = 20\n\nx, y = swap_variables(x, y)\nprint(\"x =\", x)\nprint(\"y =\", y)\n```\n\n\nCertainly! Let's go through the example step by step to understand how memory management and the reference system work in Python when swapping two variables.\n\nStep 1: Variable assignment\n```python\nx = 10\ny = 20\n```\nIn this step, two variables `x` and `y` are created and assigned the values `10` and `20`, respectively. At this point, **the reference count for the objects `10` and `20` is 1** each because both `x` and `y` reference these objects.\n\nStep 2: Calling the `swap_variables` function\n```python\nx, y = swap_variables(x, y)\n```\nHere, the `swap_variables` function is called with the current values of `x` and `y` as arguments. The function returns a tuple containing the swapped values, which are then assigned to `x` and `y`. At this stage, the memory and reference count changes as follows:\n\n- The `swap_variables` function receives the values of `x` and `y` (references to objects `10` and `20`) as arguments. *The function does not create any new objects or change the reference count*.\n- The `swap_variables` function returns a tuple with the swapped values `(20, 10)`. *This creates a new tuple object in memory and increments the reference count of objects `20` and `10` to 2 each* (one reference from the tuple, one reference from `x` and `y`).\n\nStep 3: Swapping the values\n```python\nx, y = 20, 10\n```\nThe parallel assignment swaps the values of `x` and `y`. At this point, `x` refers to the object `20`, and `y` refers to the object `10`. The reference count of objects `10` and `20` remains at 2 each (one reference from the tuple, one reference from `x` and `y`).\n\nStep 4: Printing the swapped values\n```python\nprint(\"x =\", x)\nprint(\"y =\", y)\n```\nThis step simply prints the values of `x` and `y`. The output will be:\n```\nx = 20\ny = 10\n```\n\nStep 5: Memory deallocation\nAt the end of the program, Python's garbage collector runs periodically to identify and deallocate objects that are no longer referenced. In this example, the tuple created in the `swap_variables` function (`(20, 10)`) becomes eligible for garbage collection because it is no longer referenced. When the garbage collector runs, it will decrement the reference count of objects `10` and `20` to 1 each, and since the reference count reaches zero, the memory occupied by the tuple and the objects will be deallocated.\n\nIn summary, the Python memory management and reference system handles the creation, reference counting, and deallocation of objects automatically. The variables `x` and `y` act as references to the objects `10` and `20`, and swapping their values using the parallel assignment does not involve explicit memory management or pointer manipulation.\n\n--- \n\n## Trick\n\n\u003e [!quote] \n\u003eYou can use  `getrefcount` to get reference count of the object. *The count returned is generally one higher than you might expect, because it includes the (temporary) reference as an argument to* `getrefcount()`.\n\n# Conclusion\n\nOverall, Python's design philosophy aims to prioritize simplicity and readability while abstracting away low-level memory management concerns, making it easier and more convenient to work with compared to languages like C that require explicit pointer manipulation.\n\n# Reference\n\n* ChatGPT - Prompt: Why python don't need pointer？","lastmodified":"2023-09-18T02:45:48.357255874Z","tags":null},"/computer_sci/computational_geometry/MOC":{"title":"Computational Geometry MOC","content":"\n# 3D Geometry Algorithm\n\n* [Delaunay Triangulation](computer_sci/computational_geometry/delaunay_triangulation.md)","lastmodified":"2023-09-18T02:45:48.357255874Z","tags":null},"/computer_sci/computational_geometry/delaunay_triangulation":{"title":"Delaunay Triangulation","content":"# What is Delaunay Triangulation?\n\n\n\n# Reference\n\n* [_Delaunay Triangulation (1/5) | Computational Geometry - Lecture 08_. _www.youtube.com_, https://www.youtube.com/watch?v=6UsdvbiJx54. Accessed 4 Sept. 2023.](https://www.youtube.com/watch?v=6UsdvbiJx54)\n* [_Delaunay Triangulation_. _www.youtube.com_, https://www.youtube.com/watch?v=GctAunEuHt4. Accessed 4 Sept. 2023.](https://www.youtube.com/watch?v=GctAunEuHt4)","lastmodified":"2023-09-18T02:45:48.357255874Z","tags":null},"/computer_sci/data_structure_and_algorithm/MOC":{"title":"Data Structure and Algorithm MOC","content":"\n# Tree-like Structure\n\n* [Fenwick Tree](computer_sci/data_structure_and_algorithm/tree/fenwick_tree.md)\n* [Segment Tree](computer_sci/data_structure_and_algorithm/tree/segment_tree.md)\n\n# Graph\n\n## Algorithm\n\n* [BFS](computer_sci/data_structure_and_algorithm/graph/BFS.md)\n* [Topological Sorting](computer_sci/data_structure_and_algorithm/graph/topological_sorting.md)\n* [Minimum Spanning Tree](computer_sci/data_structure_and_algorithm/graph/MST.md)\n\n## Type of graph\n\n* [Spanning Tree](computer_sci/data_structure_and_algorithm/graph/spanning_tree.md)","lastmodified":"2023-09-18T02:45:48.357255874Z","tags":null},"/computer_sci/data_structure_and_algorithm/graph/BFS":{"title":"Breadth First Search in Python","content":"\n# Basic Concept\n\n\n\n# Code Implementation\n\n\n\n# Reference\n\n* [_Breadth First Search Algorithm Explained (With Example and Code)_. _www.youtube.com_, https://www.youtube.com/watch?v=YtD2KGRdn3s. Accessed 19 July 2023.](https://www.youtube.com/watch?v=YtD2KGRdn3s\u0026t=2s)","lastmodified":"2023-09-18T02:45:48.357255874Z","tags":null},"/computer_sci/data_structure_and_algorithm/graph/MST":{"title":"Minimum Spanning Tree","content":"Not now...","lastmodified":"2023-09-18T02:45:48.357255874Z","tags":null},"/computer_sci/data_structure_and_algorithm/graph/spanning_tree":{"title":"Spanning Tree","content":"\n# What is Spanning Tree?\n\n树上再加一条边使之存在环，就称为**基环树**\n\nExample:\n\n![](computer_sci/data_structure_and_algorithm/graph/attachments/Pasted%20image%2020230915111826.png)\n\n\n# Why do we need Spanning Tree\n\n* **Network design**: Spanning trees are used to create efficient and redundant networks, such as in Ethernet networks or telecommunications.\n* **Routing protocols**: Spanning trees are employed in protocols like Spanning Tree Protocol (STP) and Rapid Spanning Tree Protocol (RSTP) for loop prevention and redundancy in network switches.\n* [Minimum Spanning Tree (MST)](computer_sci/data_structure_and_algorithm/graph/MST.md): Spanning trees can be used to find the minimum-weighted spanning tree in a weighted graph. This is particularly useful in **optimizing costs in transportation networks or electrical power distribution grids**.\n* **Broadcast algorithms**: Spanning trees are used in broadcasting messages or data packets efficiently within a network, ensuring that each node receives the message exactly once.\n\n\u003e [!summary] \n\u003e  Spanning trees provide **a simplified view of the graph**, which **eliminates unnecessary edges** while **preserving connectivity**. This simplification helps in various graph-related algorithms, network design, and optimization problems.\n\n# More about Spanning Tree\n\n## Inward Spanning Tree, 内向基环树\n\n一个基环树的拓展概念，没有在英文资料里查到很多相关资料，但是中文资料和chatgpt明白这个词，表达的概念一致\n\n![](computer_sci/data_structure_and_algorithm/graph/attachments/Pasted%20image%2020230915114049.png)\n\n内向基环树类似于基环树的结构，在有向图中，每个点有且只有一条出边，即every node out-degree = 1。$k$个点有$k$条边，那么必然存在有且只有一个环，其它点均指向这个环。\n# Reference\n\n* [_$Note$-内向基环树 - AcWing_. https://www.acwing.com/blog/content/23513/. Accessed 15 Sept. 2023.](https://www.acwing.com/blog/content/23513/)","lastmodified":"2023-09-18T02:45:48.36125592Z","tags":null},"/computer_sci/data_structure_and_algorithm/graph/topological_sorting":{"title":"Topological Sorting","content":"# What is Topological Sorting\n\n**Topological Sorting**(拓扑排序) is designed for Directed Acyclic Graph(**DAG, 有向无环图**). Topological Sorting is a linear ordering of vertices such that for every directed edge u v, vertex u comes before v in the ordering.\n\n## Example\n\n![](computer_sci/data_structure_and_algorithm/graph/attachments/Pasted%20image%2020230914104155.png)\n\nTopological sorting can be more than one result. For this graph, \"5 4 2 3 1 0\" is one of the result. **The first vertex in topological sorting is always a vertex with an *in-degree of 0*.**\n\n# Algorithm to do Topological Sorting\n\n## DFS\n\n深度优先搜索以任意顺序循环遍历图中的每个节点，若搜索进行中碰到之前已经遇到的节点，或碰到叶节点，则中止算法。\n\n```fake\nL ← Empty list that will contain the sorted nodes\nwhile exists nodes without a permanent mark do\n\tselect an unmarked node n\n\tvisit(n)\n\nfunction visit(node n)\n\tif n has a permanent mark then\n\t\treturn\n\tif n has a temporary mark then\n\t\treturn error (graph not DAG)\n\n\tmark n with a temporary mark\n\t\n\tfor each node  m with a edge from n to m do\n\t\tvisit(m)\n\t\n\tremove temporary mark from n\n\tmark n with a permanent mark\n\tadd n to head of L\n```\n\n\n## Kahn's Algorithm\n\n First, find a list of \"start nodes\" which have no incoming edges and insert them into a set S; *at least one such node must exist in a non-empty acyclic graph*\n\n```fake\nL ← Empty list that will contain the sorted elements\nS ← Set of all nodes with no incoming edge\n\nwhile S is not empty do\n\n\tremove a node n from S\n\tadd n to L\n\t\n\tfor each node m with an edge e from n to m do\n\t\tremove edge e from graph\n\t\t\n\t\tif m has no other incoming edge then\n\t\t\tinsert m into S\n\t\n\tif graph has edges then\n\t\treturn error (graph no DAG)\n\telse\n\t\treturn L\n```\n\n\n![](computer_sci/data_structure_and_algorithm/graph/attachments/algo.gif)\n\n\n# Topological Sorting Application\n\n* Task Priorities\n\n# Reference\n\n* [“Topological Sorting.” _GeeksforGeeks_, 12 May 2013, https://www.geeksforgeeks.org/topological-sorting/.](https://www.geeksforgeeks.org/topological-sorting/)\n* [“拓撲排序.” 维基百科，自由的百科全书, 22 May 2022. _Wikipedia_, https://zh.wikipedia.org/w/index.php?title=%E6%8B%93%E6%92%B2%E6%8E%92%E5%BA%8F\u0026oldid=71758255.](https://zh.wikipedia.org/wiki/%E6%8B%93%E6%92%B2%E6%8E%92%E5%BA%8F)\n* [“算法 - 拓扑排序.” _Earth Guardian_, 22 Aug. 2018, http://redspider110.github.io/2018/08/22/0092-algorithms-topological-sorting/index.html.](https://redspider110.github.io/2018/08/22/0092-algorithms-topological-sorting/)","lastmodified":"2023-09-18T02:45:48.36125592Z","tags":null},"/computer_sci/data_structure_and_algorithm/tree/fenwick_tree":{"title":"fenwick_tree","content":"\n  \n![](computer_sci/data_structure_and_algorithm/tree/attachments/Pasted%20image%2020230710160348.png)\n\n**树状数组（Fenwick Tree）**，也被称为**二叉索引树（Binary Indexed Tree，BIT）**，其初衷是解决数据压缩里的累积频率（Cumulative Frequency）的计算问题，现多用于*高效计算数列的[前缀和](tmp_script/prefix_sum.md)， 区间和*。它可以以$O(\\log{n})$的时间得到任意前缀和，并同时支持在$O(\\log{n})$时间内支持动态单点值的修改，空间复杂度为$O(n)$\n\n我们希望BIT可以完成的操作是：\n1. 更改存储在索引I处的值。(这称为**点更新**操作)\n2. 查找长度为k的前缀之和。(这称为**前缀**和**查询**)\n\n\n# Origin\n\n按照Peter M.Fenwick的说法，正如所有的整数都可以表示成2的幂和，我们也可以把一串序列表示成一系列子序列的的和。采用这个想法，我们可*将一个前缀和划分成多个子序列的和*，而划分的方法与数的2的幂和具有极其相似的方式。*一方面，子序列的个数是其二进制表示中1的个数，另一方面，子序列代表的f[i]的个数也是2的幂。*\n\n# Step by Step\n\n\n## `lowbit(x:int) -\u003e int`\n\n该函数返回参数转为二进制后,最后一个1的位置所代表的数值，例如：\n\n```\nlowbit(34) -\u003e 2\nlowbit(12) -\u003e 4\nlowbit(8) -\u003e 8\n```\n\n在coding时，可以使用位运算`(~i + 1) \u0026 i`来计算最后一位1的值，它的原理在于使得最小位数上的1在`~i + 1`和`i`上都为1，而其它位置上则不会同时为1，因此使用`and`运算可以得到最后一位1的值\n\n同时，具有trick的点在于，实际coding的时候，`lowbit`函数写为：\n\n```python\ndef lowbit(x):\n    return x \u0026 (-x)\n```\n\n并不需要做+1操作，这是因为这是因为当我们对整数 `i` 取负数 `-i` 时，其二进制表示中只有最右边的 1 保持不变，而其余位都会取反。然后我们再将 `i` 与 `-i` 进行按位与操作 `\u0026`，结果会保留 `i` 中最右边的 1，而将其他位都变为 0。这个技巧的原理基于补码表示法，在补码表示法中，*正整数的补码和其本身相同，负整数的补码是将其绝对值的二进制表示取反后加 1*。所以很多语言在coding的时候，`-i`所做操作就是`~i+1`\n\n`lowbit()` \n\n## Build Array `BIT` (**Binary Indexed Tree**)\n\n二叉索引树一般由数组实现\n\n在Fenwick Tree结构中，需要一个数组`BIT`来维护数组$A$的前缀和，有：\n$$\n{BIT}_i = \\sum_{j=i-lowbit(i)+1}^{i} A_j\n$$\ncode实现：\n\n```python\n\n```\n\n\n# Reference\n\n* [二叉索引树 | 三点水. https://lotabout.me/2018/binary-indexed-tree/. Accessed 11 July 2023.](https://lotabout.me/2018/binary-indexed-tree/)","lastmodified":"2023-09-18T02:45:48.36125592Z","tags":null},"/computer_sci/data_structure_and_algorithm/tree/segment_tree":{"title":"Segment Tree","content":"# Overview\n\nSegment Tree（**线段树**）是一种用于解决区间查询问题的数据结构。它可以**有效地处理包含大量区间操作的问题**，如*查询*区间最大值、最小值、*求和*、*更新*等。\n\nSegment Tree将给定的区间划分为若干个较小的子区间，并使用树进行表示。**每个节点表示一个子区间，树的根节点表示整个区间**。每个节点记录了对应子区间的一些统计信息，如该区间的最大值、最小值、总和等。\n\n构建Segment Tree的过程中，首先将问题规模不断缩小，将大的区间划分为两个较小的子区间，并依次递归构建每个子区间的节点。当区间缩小到长度为1时，即叶子节点，将问题的原始数据作为叶子节点的值。\n\nSegment Tree的构建完成后，可以高效地进行查询和更新操作。查询操作通过递归遍历树的节点，在给定的区间范围内查找所需的统计信息。更新操作通过递归更新树的节点，更新目标区间内的值，并更新父节点的统计信息。\n\n**由于Segment Tree的每个节点代表的区间是互不重叠的，因此在进行统计信息的查询和更新时，可以利用区间的性质进行剪枝操作，从而提高效率**。\n\n# Detail\n\n## Basic \n\n![](computer_sci/data_structure_and_algorithm/tree/attachments/Pasted%20image%2020230907145346.png)\n\n*Segment Tree* is a basically binary tree, we can represent segment tree in a simple linear array. We can learn segment tree by knowing some key points. We consider an array $A$ of size $N$ and a corresponding Segment Tree $T$.\n\n1. The root of $T$ will represent the whole array $A[0:N-1]$\n2.  In each step, the segment is divided into half and the two children represent those two halves. $A[0:N-1]$ will be divided into $A[0, (N-1)/2]$ \u0026 $A[(N-1)/2 + 1, N-1]$\n3. **Height** of the segment tree will be $log_2{N}$. The **internal nodes** is $N-1$ and **leaves** are $N$. So a **total number of nodes** are $2 \\times N - 1$.\n\n## Operations\n\nOnce the Segment Tree is built, its structure cannot be changed. We can update the values of nodes but we cannot change its structure. Segment tree provides two operations:\n1. **Update**: To update the element of the array $A$ and reflect the corresponding change in the Segment tree.\n2. **Query**: In this operation we can **query on an interval or segment and return the answer to the problem** (say minimum/maximum/summation in the particular segment).\n\n## Time Complexity and Code Implementation Demo\n\n### Build\n\n![](computer_sci/data_structure_and_algorithm/tree/attachments/Pasted%20image%2020230907170533.png)\n\n\n```c\nvoid build(int node, int start, int end)\n{\n    if(start == end)\n    {\n        // Leaf node will have a single element\n        tree[node] = A[start];\n    }\n    else\n    {\n        int mid = (start + end) / 2;\n        // Recurse on the left child\n        build(2*node, start, mid);\n        // Recurse on the right child\n        build(2*node+1, mid+1, end);\n        // Internal node will have the sum of both of its children\n        tree[node] = tree[2*node] + tree[2*node+1];\n    }\n}\n```\n\n\n```python\ndef segment_tree_build(nums):\n    n = len(nums)\n    tree = np.zeros(2 * n)\n    \n    tree[n:2 * n] = nums\n    \n    for i in range(n-1, 0, -1):\n        tree[i] = tree[2 * i] + tree[2 * i + 1]\n        \n    return tree\n```\n\n\n**Every nodes means a sum of an interval**. Build Complexity is $O(N)$\n\n### Update\n\n```c\nvoid update(int node, int start, int end, int idx, int val)\n{\n    if(start == end)\n    {\n        // Leaf node\n        A[idx] += val;\n        tree[node] += val;\n    }\n    else\n    {\n        int mid = (start + end) / 2;\n        if(start \u003c= idx and idx \u003c= mid)\n        {\n            // If idx is in the left child, recurse on the left child\n            update(2*node, start, mid, idx, val);\n        }\n        else\n        {\n            // if idx is in the right child, recurse on the right child\n            update(2*node+1, mid+1, end, idx, val);\n        }\n        // Internal node will have the sum of both of its children\n        tree[node] = tree[2*node] + tree[2*node+1];\n    }\n}\n```\n\n\nTo update an element, **look at the interval in which the element is present and recurse accordingly on the left or the right child**.\n\nComplexity of update will be $O(logN)$\n\n\n### Query\n\n```c\nint query(int node, int start, int end, int l, int r)\n{\n    if(r \u003c start or end \u003c l)\n    {\n        // range represented by a node is completely outside the given range\n        return 0;\n    }\n    if(l \u003c= start and end \u003c= r)\n    {\n        // range represented by a node is completely inside the given range\n        return tree[node];\n    }\n    // range represented by a node is partially inside and partially outside the given range\n    int mid = (start + end) / 2;\n    int p1 = query(2*node, start, mid, l, r);\n    int p2 = query(2*node+1, mid+1, end, l, r);\n    return (p1 + p2);\n}\n```\n\n将查询区间切割成多个区间在不同节点查找并合并\n## LazyTag Trick\n\n\nLazy Tag的设计目的是为了[l, r]区间所有数增加k的情况，做多次update时间复杂度浪费过多，利用lazy tag降低时间复杂度。\n\nlazy tag的设计原理是，被打上lazy tag的seg node是已经更新完了的seg node，而lazy tag之下的seg node是没有更新的。只有要访问lazy tag之下的seg node的时候才去做更新，来节省更新。\n\n### Lazy Tag Propagation\n\nlazy propagation is a optimize technique in segment tree to **minimize** tons of operations.\n\nlazy propagation is hard to explain, so watch this tutorial vedio is a best way to learn and review.\n\npls watch vedio in reference 3:  [_Lazy Propagation Segment Tree_. _www.youtube.com_, https://www.youtube.com/watch?v=xuoQdt5pHj0. Accessed 12 Sept. 2023.](https://www.youtube.com/watch?v=xuoQdt5pHj0)\n\n\n\n# Reference\n\n* [“Segment Trees Tutorials \u0026 Notes | Data Structures.” _HackerEarth_, https://www.hackerearth.com/practice/data-structures/advanced-data-structures/segment-trees/tutorial/. Accessed 7 Sept. 2023.](https://www.hackerearth.com/practice/data-structures/advanced-data-structures/segment-trees/tutorial/)\n* [“力扣（LeetCode）官网 - 全球极客挚爱的技术成长平台.” _力扣 LeetCode_, https://leetcode.cn/problems/handling-sum-queries-after-update/solutions/2356392/geng-xin-shu-zu-hou-chu-li-qiu-he-cha-xu-kv6u/. Accessed 11 Sept. 2023.](https://leetcode.cn/problems/handling-sum-queries-after-update/solutions/2356392/geng-xin-shu-zu-hou-chu-li-qiu-he-cha-xu-kv6u/)\n* [_Lazy Propagation Segment Tree_. _www.youtube.com_, https://www.youtube.com/watch?v=xuoQdt5pHj0. Accessed 12 Sept. 2023.](https://www.youtube.com/watch?v=xuoQdt5pHj0)","lastmodified":"2023-09-18T02:45:48.36125592Z","tags":null},"/data_sci/data_sci_MOC":{"title":"Data science MOC","content":"\n","lastmodified":"2023-09-18T02:45:48.365255967Z","tags":null},"/data_sci/visual_information_theory":{"title":"Visual Information Theory","content":"\n# Reference\n\n[https://colah.github.io/posts/2015-09-Visual-Information/](https://colah.github.io/posts/2015-09-Visual-Information/)","lastmodified":"2023-09-18T02:45:48.365255967Z","tags":null},"/equipment_research/equip_res_MOC":{"title":"Equipment Research MOC","content":"\n","lastmodified":"2023-09-18T02:45:48.365255967Z","tags":null},"/equipment_research/spectrum_analyzer":{"title":"Spectrum Analyzer","content":"\n# What is spectrum analyzer?\n\n\n\n# Spectrum Analyzer for UWB\n\n## Papers\n\n### Measurements of UWB through-the-wall propagation using spectrum analyzer and the Hilbert transform\n\n\n[*pdf* - Measurements of UWB through-the-wall propagation using spectrum analyzer and the Hilbert transform](equipment_research/attachments/mop.23107.pdf)\n\n![Architect](equipment_research/attachments/Pasted%20image%2020230918104114.png)\n\n\n# Reference\n\n* [_Understanding Basic Spectrum Analyzer Operation_. _www.youtube.com_, https://www.youtube.com/watch?v=P5gxNGckjLc. Accessed 13 Sept. 2023.](https://pinktalk.online/%E6%96%87%E5%AD%A6/%E5%8F%A5%E5%AD%90/Feeling/)\n* https://www.bilibili.com/video/BV1kG4y1q72V/?spm_id_from=333.337.search-card.all.click\u0026vd_source=c47136abc78922800b17d6ce79d6e19f","lastmodified":"2023-09-18T02:45:48.365255967Z","tags":null},"/log/2023/7/log_01072023":{"title":"Log 2023.07.01 - 云朵也是会动滴☁️","content":"\n晚上去食堂吃饭的路上，看到了今天最棒的云\n\n![](log/2023/7/attachments/cloud.png)\n\n就想着吃完饭再来好好拍；\n\n吃完饭，再回来的时候，云朵就已经漂远了 \n云朵是会移动的，其他事情又何尝不是这样呢\n\n--- \n\n\n还有一张蒋学姐拍的云☁️：\n\n![](log/2023/7/attachments/Pasted%20image%2020230701220633.png)","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/log/2023/7/log_03072023":{"title":"Log 2023.07.03 - K-means clustering algorithm for Pixel art style","content":"\n今天用k-means聚类算法做了一个像素化的效果，还蛮好玩的\n\n![](log/2023/7/attachments/3ed5fee41bd566be093bebd62a33d12.jpg)","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/log/2023/7/log_06072023":{"title":"Log 2023.07.06 - 路过人间，谁有意见","content":"\n![](文学/log/2023/7/attachments/7JEC(63A65[8JFI[G6O`IIK_tmb.jpg)","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/log/2023/9/log_11092023":{"title":"Log 2023.09.11 - Get some interesting blog here","content":"* [_Building a Frontend Framework; Reactivity and Composability With Zero Dependencies_. https://18alan.space/posts/how-hard-is-it-to-build-a-frontend-framework.html. Accessed 11 Sept. 2023.](https://18alan.space/posts/how-hard-is-it-to-build-a-frontend-framework.html)","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/log/log_MOC":{"title":"log_MOC","content":"\n* [Log 2023.07.01 - 云朵也是会动滴☁️](log/2023/7/log_01072023.md)\n* [Log 2023.07.03 - K-means clustering algorithm for Pixel art style](log/2023/7/log_03072023.md)\n* []","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/recent":{"title":"Recent note","content":"\n```dataview\ntable WITHOUT ID file.link AS \"Title\",file.mtime as \"Edit Time\"\nfrom \"\"\nsort file.mtime desc\nlimit 10\n```","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/resume":{"title":"Resume","content":"\n\u003cdiv style=\"margin:auto;width: 50%; transform: translate(50%, 0);\"\u003e\n\u003ctr\u003e\n            \u003ctd style=\"text-align:\n                center;\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/50662650?v=4\"\n                    role=\"presentation\" width=\"114\" \n                    style=\"display: inline-block; max-width: 180px; border-radius: 25px;text-align: center;\"\u003e\n                \u003ch2 color=\"#000000\"  style=\"margin: 0px; font-size: 20px; color: #3A4E48; font-weight: 1000;\"\u003e\u003cspan\u003eJude\u003c/span\u003e\u003cspan\u003e\u0026nbsp;\u003c/span\u003e\u003cspan\u003eWang\u003c/span\u003e\u003c/h2\u003e\n        \u003c/tr\u003e\n\u003c/div\u003e\n\n# 📐 Education\n\n**Zhejiang University (ZJU)**,\u0026nbsp;\u0026nbsp;Zhejiang,\u0026nbsp;\u0026nbsp;China \u003cspan style=\"float: right; \"\u003e2022.09 - Now\u003c/span\u003e \u003cbr\u003e\n*M.Sc.* Major in Biomedical Engineering (BME)\n\n**Exchange to National University of Singapore (NUS)**  \u003cspan style=\"float: right; \"\u003e2021.08-2022.05\u003c/span\u003e \u003cbr\u003e\n*Final Year Project* \u0026nbsp;instructed by [Dan Wu](https://person.zju.edu.cn/en/danwu) and [Zhiwei Huang](https://cde.nus.edu.sg/bme/staff/dr-huang-zhiwei/)\n\n**Zhejiang University**, \u0026nbsp;\u0026nbsp;Zhejiang, China \u003cspan style=\"float: right; \"\u003e2018.08-2022.06\u003c/span\u003e\u003cbr\u003e\n*B.S.* Major in Biomedical Engineering (BME), *The first Lv Weixue laboratory class in ZJU*\u003cbr\u003e\n*B.S.* Minor in *Intensive Training Program of Innovation and Entrepreneurship (ITP)*\n\n**Summer exchange to City University of HongKong (CityU)** \u003cspan style=\"float: right; \"\u003eAug. 2019\u003c/span\u003e\n\n# 🔥 Projects \u0026 Research Experience\n\n**Master's thesis** \u003cspan style=\"float: right; \"\u003e2022 - now\u003c/span\u003e \u003cbr\u003e\n*SAR image reconstruction to detect burn skin based on UWB echo signal*\u003cbr\u003e\nFor now, my master's research direction is about echo signal processing. Ultra-wide band(UWB) signal has a good ability to across skin surface to get information under skin. We want use UWB signal's feature to detect skin burn level in non-invasive way. This project involves *back projection(BP)* algorithm to reconstruct 3D image from echo signal, image artifact removal based on amplitude coherence factor(ACF) and correlation weighted(CW), burn level evaluation method and so on. \nhttps://github.com/PinkR1ver/UWB-Imagination-Using-SAR\n\n**FYP** \u003cspan style=\"float: right; \"\u003e2021-2022\u003c/span\u003e \u003cbr\u003e\n*Radiogenomics Analysis of Glioblastoma with Deep learning Techniques*\u003cbr\u003e\nI finish this FYP instructed by [Zhiwei Huang](https://cde.nus.edu.sg/bme/staff/dr-huang-zhiwei/) in NUS. This project contains three part. MRI image segmentation by *U-Net*, radiomics features extraction by *pyradiomics*, and feature vector classification by machine learning such as *random forest*, *MLP*, *SVM* and so on. [https://github.com/PinkR1ver/Radiogenemics--on-Ivy-Gap](https://github.com/PinkR1ver/Radiogenemics--on-Ivy-Gap)\n\n**SRTP** \u003cspan style=\"float: right; \"\u003e2020-2021\u003c/span\u003e \u003cbr\u003e\n*3D tooth segmentation based on deep learning*\u003cbr\u003e\nThis project targets at instance segmentation given images on teeth . We have implemented a semantic segmentation to detect teeth and gingiva using PointNet, and then utilize bounding boxes to do instance segmentation on every single teeth using 3D-BoNet.\n\n**Smooth Boy** \u003cspan style=\"float: right; \"\u003eAug. 2020\u003c/span\u003e \u003cbr\u003e\n*A skin evaluation and product recommendation WeChat app*\u003cbr\u003e\nSmooth Boy is a WeChat app that can evaluate a person’s skin quality and recommend skin care products and it was especially developed for young male teenage who purchase for beauty. It was a real self-learning project as well. In this project I design the UI and code the core part of the app to achieve the function of detecting the person’s face. [https://github.com/PinkR1ver/Smooth-Boy](https://github.com/PinkR1ver/Smooth-Boy)\n\n\n**Sketchpad** \u003cspan style=\"float: right; \"\u003eApr – Jun. 2019\u003c/span\u003e \u003cbr\u003e\n*Polynomial function visualization in 1995 style* \u003cbr\u003e\nIt was my first class project in college. It was a simple program based on an old graphics library which created in 1995, can draw image of polynomial function. In this project, I built the whole structure of the code and organize my team to code different part to make it done. It was really exciting to stay up all night to code and when it was finished, I almost cried that time. [https://github.com/PinkR1ver/SketchPad](https://github.com/PinkR1ver/SketchPad)\n\n\n# 🤹🏽Skills \u0026 Knowledge\n\n## Proficient\n\u003cbr\u003e\n\u003cdiv style=\"display: flex; white-space:nowrap; overflow:auto; padding: 15px\"\u003e\n\t\u003cimg align=\"left\" alt=\"python\" height=\"35px\" style=\"margin:0px 4px\" src=\"https://github.com/PinkR1ver/Jude.W-s-Knowledge-Brain/blob/master/warehouse/img/skills/python.png?raw=true\" /\u003e\n\t\u003cimg align=\"left\" alt=\"matlab\" height=\"35px\" style=\"margin:0px 4px\" src=\"https://github.com/PinkR1ver/Jude.W-s-Knowledge-Brain/blob/master/warehouse/img/skills/matlab.png?raw=true\" /\u003e\n\t\u003cimg align=\"left\" alt=\"numpy\" height=\"35px\" style=\"margin:0px 4px\" src=\"https://github.com/PinkR1ver/Jude.W-s-Knowledge-Brain/blob/master/warehouse/img/skills/numpy.png?raw=true\" /\u003e\n\t\u003cimg align=\"left\" alt=\"pandas\" height=\"35px\" style=\"margin:0px 4px\" src=\"https://github.com/PinkR1ver/Jude.W-s-Knowledge-Brain/blob/master/warehouse/img/skills/pandas.png?raw=true\" /\u003e\n\t\u003cimg align=\"left\" alt=\"pytorch\" height=\"35px\" style=\"margin:0px 4px\" src=\"https://github.com/PinkR1ver/Jude.W-s-Knowledge-Brain/blob/master/warehouse/img/skills/pytorch.png?raw=true\" /\u003e\n\t\u003cimg align=\"left\" alt=\"anaconda\" height=\"35px\" style=\"margin:0px 4px\" src=\"https://github.com/PinkR1ver/Jude.W-s-Knowledge-Brain/blob/master/warehouse/img/skills/anaconda.png?raw=true\" /\u003e\n\u003c/div\u003e\n\n## Detail\n\n* Program Language: Python \u003e= MATLAB \u003e\u003e C == HTML/CSS/JavaScript\n* Deep Learning Associated:\n\t* Proficient in PyTorch deep learning frameworks\n\t* Familiar with the common techniques and algorithms of deep neural network,.\n\t* Familiar with the common CV tasks and NLP tasks.\n\t* Familiar with some famous backbone of DL model -  U-Net, Vit...\n\t* Learning LLM knowledge recently\n* Core lessons: \n\t* NUS - EE4305 - Fuzzy/Neural System for Intelligent Robotics - Grade: A\n\t* NUS - EE4309 - Robot Perception - Grade: A-\n\t* ZJU - Signal and System - Grade: A-\n\t* ZJU - Modern Medical Imaging Technology - Grade: A\n\t* ZJU - Data Structure - Grade: B+\n\t* ...\n* Toolkit: Git, VS code\n\n## Others\n\n*  Jekyll, RStudio and some other tools to build personal blog: [https://pinkr1ver.com](https://pinkr1ver.com) (🚧 obsolete...)\n* HTML+CSS+JS to create my photo slide show web - [https://pinkr1ver.com/PhotoGallery/](https://pinkr1ver.com/PhotoGallery/)\n* SHAP analysis for model interpretability https://github.com/PinkR1ver/SHAP_Tutorial\n* $\\LaTeX$ for my FYP thesis, contributed to 1.9k star repository [zjuthesis](https://github.com/TheNetAdmin/zjuthesis)\n\n# 🏆 Honors\n\n* Excellent Graduate of Zhejiang University\n* Third Class Scholarship of Zhejiang University\n\n# 🎈 Clubs \u0026 Social Activities\n\n* Support Education in Jiande Town, Changsha, Hunan \u003cspan style=\"float: right; \"\u003e2019.7-2019.8\u003c/span\u003e\n* DanYang \u0026 QingXi Community Student Union New Media Department deputy director \u003cspan style=\"float: right; \"\u003e2018-2020\u003c/span\u003e\n\n# 🌺 Other Fun Facts\n\n* Outdoor fans - cycling, hiking... - [My Strava Profile](https://www.strava.com/athletes/109116948)\n* Photography fans - [My Photo Gallery](https://pinkr1ver.notion.site/3cfdd332b9a94b20bca041f2aa2bdcd2?v=24e696e6ab754386a710bc8e83976357)\n* Loving films, dramas, books... - [My Watching List](https://pinkr1ver.notion.site/5e136466f3664ff1aaaa75b85446e5b4?v=a41efbce52a84f7aa89d8f649f4620f6)\n* PC Game fans, especially CS - [My Steam profile](https://steamcommunity.com/id/PinkCred1t)\n* Chess fans - [Rank in chess.com](https://www.chess.com/member/yichongwang)\n\n# 📟 Contacts\n\n* 🏢: Dept. Biomedical Engineering Lab 511 | Zhejiang University\n* ☎: +86 177-6826-6860\n* 📬: pinkr1veroops@gmail.com\n\n","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/signal_processing/basic_concepts_in_signal_processing":{"title":"Basic Concepts in Signal Processing","content":"\n* [what_is_dB](signal_processing/what_is_dB.md)","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/signal_processing/signal_processing_MOC":{"title":"Signal Processing - MOC","content":"\n* [basic_concepts_in_signal_processing](signal_processing/basic_concepts_in_signal_processing.md)\n","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/signal_processing/what_is_dB":{"title":"What is dB","content":"dB is short for decibel, which is a unit that indicates ratio or gain. It is often used to measure *sound intensity*, *signal strength*, *attenuation* and other quantities. \n\nFor example, if a sound has a power of 10 W and another sound has a power of 1 W, then the difference in decibels is 10 dB = 10 log (10/1) = 10 log 10 = 10.\n\n**Signal Noise Ratio** is also measured by dB\n\n## Signal Noise Ratio\n$$\n{SNR}_{power}=\\frac{\\text{Average Signal Power}}{\\text{Average Noise Power}}\n$$\n\n$$\n{SNR}_{voltage}=\\frac{\\text{RMS Signal Voltage}}{\\text{RMS Noise Voltage}}\n$$\n\n$$\n{SNR}_{power}={{SNR}_{voltage}}^2\n$$\n\n$$\n{SNR}_{dB}=10\\log_{10}{{SNR}_{power}}=20\\log_{10}{{SNR}_{voltage}}\n$$\n","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/synthetic_aperture_radar_imaging/Antenna":{"title":"Antenna","content":"\n# Theorem you need know\n\n* [🧷Resonant circuit](Physics/Electromagnetism/Resonant_circuit.md)\n\n# What is antenna\n\nA usually metallic device for radiating or receiving radio waves\n\n## A simple model representing antenna\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230404163712.png)\n\n* $R_L$ 损耗电阻 - 介质与结构导致的损耗\n* $R_r$ 辐射电阻 - 与天线产生的辐射的能量关系密切\n* $X_A$ 电抗 - 描述天线近场电磁能转换的现象 (一般情况下$X_A$ = 0)\n\n\u003e [!warning] \n\u003e 天线还有一个很重要的损耗来源，**mismatch loss**, 天线跟前端的阻抗不匹配，导致能量打不进天线，这点可以通过设计和材质来解决 \n\n# Types of antennas\n\n## Wire antennas\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230404165239.png)\n\n## Aperture antennas\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230410105310.png)\n\n## Microstrip antennas\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230410105548.png)\n\n## Array antennas\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230410111719.png)\n\n\u003e [!hint] \n\u003e 天线的目的简单来说，就是为了将能量尽可能辐射出去，同时按照你希望的方向和区间辐射。\n\n## Reflector antennas \u0026 Lens antennas\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230410112252.png)\n\n\n# Radiation mechanism\n\n## Ideal antenna\n\nRadiate all the power delivered to it from the transmitter in a desired direction or directions.\n\n## How is radiation accomplished?\n\n* How are EM fields generated by the source?\n* How are EM fields contained and guided within the transmission line \u0026 antenna?\n* How are EM fields finally detached from the antenna to form a free-space wave?\n\n### How are EM fields generated by the source?\n \n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230410113039.png)\n\n* $q_v$电荷密度，$C/m^3$\n* $v_Z$电荷移动速度，$m/s$\n* $J_Z$电流密度，$A/m^2$\n$$\nA/m^2 = C/m^3 * m/s = \\frac{C}{m^2 * s}\n$$\n\n导线由PEC所做时，或者高频情况，电流变成面电流\n* $J_S$变成面电流密度，$A/m^2$\n* $q_S$也变成面电荷密度，$C/m^2$\n\n但wire非常thin，当然面最终被认为为线\n\n$$\nI_Z = q_l v_Z\n$$\n我们用thin wire case来讨论\n\n对这个式子做时间微分\n$$\n\\frac{dI_z}{dt} = q_l\\frac{v_Z}{dt}=q_l a_z\n$$\n$$\nl\\frac{dI_Z}{dt}=lq_la_Z=Qa_z\n$$\n\u003e [!hint] \n\u003e To create radiation, there must be **a time-varying current** or **an acceleration (or deceleration) of charge** \n\u003e \n\u003e -\u003e The wire must be curved, bent, discontinuous, terminated, or truncated\n\n###  How are EM fields contained and guided within the transmission line \u0026 antenna?\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230411105457.png)\n\nradiation要考虑两个方面，一方面激发电场那边提供的电子的加速，另一方面时end部分的pause造成的电子的减速，这两边会有最主要的辐射；\n\n如果加速和减速之间的距离很短，形成一个pulse，会发出一个很宽频的信号；\n\n如果加减速达到间歇运动状态，会发出一个单频的辐射\n\n\u003e [!hint] \n\u003e 用水波去理解辐射\n\u003e \n\u003e 在池塘里要产生水波，你可以丟一颗石头\n\u003e \n\u003e Source可以产生pulse或者弦波，引起电磁振荡，induce电荷做加减速，产生时变电流，在导线里产生导波，也就是在传输线中引导的电磁波，电磁波最后会走到天线端，被辐射出去；\n\u003e \n\u003e pulse就像你丢了一颗石头下去，弦波就像你按照周期去丢\n \n\u003e [!hint] \n\u003e 根据[Maxwell's equations](Physics/Electromagnetism/Maxwells_equation.md)\n\u003e \n\u003e 当电磁波在导线中存在的时候，它是需要时变的电流或者说是加减速的电荷来support。在传输线里，需要source才能有场；\n\u003e \n\u003e 但是在解[Maxwell's equations](Physics/Electromagnetism/Maxwells_equation.md)的时候，是有一组homogeneous的解，这组解指的是，你不需要source的存在的场，这个场指的是free-space wave；\n\u003e \n\u003e 所以，天线本质上就是一个interface，将导线内需要source的场，变成不需要source的场，也就是free-space wave\n\n### How are EM fields finally detached from the antenna to form a free-space wave?\n\n# Radar key Parameters\n\n\n\n# Reference\n\n* [知乎 - 天线与电波传播基础知识](https://zhuanlan.zhihu.com/p/497482699)\n* [天线 in wiki](https://zh.wikipedia.org/wiki/%E5%A4%A9%E7%BA%BF)\n* [⭐⭐⭐陈士元 - 天线原理与基本参数](https://www.youtube.com/watch?v=JsVGW3z81wc\u0026list=PLQdXflQNtKfLaGnvPLW_XVal-RaHxFN5j\u0026index=1)\n* [天线8个核心参数解析 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/375911768)","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/synthetic_aperture_radar_imaging/Chirp":{"title":"Chirp - 啁啾","content":"\n啁啾（Chirp）是指频率随时间而改变（增加或减少）的信号。其名称来源于这种信号听起来类似鸟鸣的啾声。\n\n![](synthetic_aperture_radar_imaging/attachments/Linear-chirp.svg)\n\nChirp常常被用在sonar, radar, laser systems里。其中，为了能够测量长距离又保留时间的分辨率，雷达需要短时间的派冲波但是又要持续的发射信号，啁啾信号可以同时保留连续信号和脉冲的特性，因此被应用在雷达和声纳探测上。\n\n# Definition\n\n## 瞬时频率 (instantaneous angular frequency)\n\n\n有一信号，$x(t)=A\\sin{(\\phi(t))}$，其瞬时角频率为\n$$\n\\omega(t)=\\frac{d\\phi(t)}{dt}\n$$\n经适当归一化后得到瞬时频率\n$$\nf(t)=\\frac{1}{2\\pi}\\frac{d\\phi(t)}{dt}\n$$\n\n## 啁啾度\n\n对前两式再求导，得到瞬时角频率的变化速率为**瞬时角啁啾度**(instantaneous angular chirpyness)\n\n$$\n\\gamma(t)=\\frac{d^2\\phi(t)}{dt^2}\n$$\n类似有**瞬时（普通）啁啾度**(instantaneous ordinary chirpyness)\n\n$$\nc(t)=\\frac{1}{2\\pi}\\gamma(t)=\\frac{1}{2\\pi}\\frac{d^2\\phi(t)}{dt^2}\n$$\n# Types\n\n## Linear\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230418110700.png)\n\n啁啾的瞬时频率$f(t)$呈线性变化\n\n$$f(t)=f_0 + ct$$\n$$\nc = \\frac{f_1-f_0}{T}\n$$\n\nc是一个常值\n\nAlso，\n\n$$\n\\phi(t)=\\phi_0 + 2\\pi \\int_{0}^t f(\\tau)d\\tau =\\phi_0 = 2\\pi(\\frac{c}{2}t^2 + f_0 t)\n$$\n\n相位为t的二次函数，从而可以继续推导出信号在time domain：\n\n$$\nx(t)=A \\cos{(\\phi_0 + 2\\pi (\\frac{c}{2}t^2 + f_0 t))}\n$$\n\n这种Linear Chirp信号也被称为二次相位讯号(**quadratic-phase signal**)\n\n## Exponential\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230418111708.png)\n\nExponential chirp，也叫geometric chirp，瞬时频率以指数变化，即$f(t_2)/f(t_1)$会是常数\n\nsignal frequency:\n\n$$\nf(t)=f_0 k^t\n$$\n\n$$\nk = (\\frac{f(T)}{f_0})^{\\frac{1}{T}} = \\text{constant}\n$$\n\n相位:\n\n$$\n\\phi(t)=\\phi_0 + 2\\pi\\int_0^t f(\\tau)d\\tau = \\phi_0 + 2\\pi f_0 (\\frac{k^t - 1}{\\ln(k)})\n$$\n\ntime-domain:\n\n$$\nx(t) = \\sin{[\\phi_0 + 2\\pi f_0(\\frac{k^t - 1}{\\ln(k)})]}\n$$\n\n## Hyperbolic\n\n双曲线线性调频用于雷达应用，因为它们在被多普勒效应([Doppler Effect](Physics/Wave/Doppler_Effect.md))扭曲后显示出最大的匹配滤波器([Matched filter](https://en.wikipedia.org/wiki/Matched_filter))响应。\n\nsignal frequency:\n\n$$\nf(t) = \\frac{f_0 f_1 T}{(f_0 - f_1)t + f_1T}\n$$\n\nPhase:\n\n$$\n\\phi(t) = \\phi_0 + 2\\pi \\int_0^t f(\\tau)d\\tau = \\phi_0 + 2\\pi \\frac{-f_0f_1 T}{f_1 - f_0}\\ln(1 - \\frac{f_1-f_0}{f_1 T}t)\n$$\n\n\ntime-domain:\n\n$$\nx(t) = \\sin{[\\phi_0 + 2\\pi \\frac{-f_0f_1 T}{f_1 - f_0}\\ln(1 - \\frac{f_1-f_0}{f_1 T}t)]}\n$$\n\n","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/synthetic_aperture_radar_imaging/Radiometric_Calibration":{"title":"Radiometric Calibration - 辐射校准","content":"\n# Overview\nSAR 校准旨在提供其像素值可与场景中的雷达反向散射直接相关的影像。虽然未校准的 SAR 影像足以用于定性用途，但校准后的 SAR 影像对于定量使用 SAR 数据而言仍至关重要。\n\n生成级别 1 影像的典型 SAR 数据处理不包括辐射校正，且仍然存在明显的辐射偏差。因此有必要对 SAR 影像应用辐射校正，*使影像的像素值真正能够反映反射表面的雷达反向散射情况*。在比较由不同的传感器采集的 SAR 影像时，或比较由同一传感器在不同时间、不同模式下采集的（或由不同处理器处理的）SAR 影像时，都需要进行辐射校正。\n\n## Types\n* **Sigma nought** - 用于校准地面上单位面积内返回到天线的反向散射，并与地面范围相关。影像经过校准，因此可以直接与相同或不同传感器收集的不同雷达影像进行比较。科学家倾向于使用 sigma naught 来解释表面散射、表面反射以及表面属性。\n\t* *Scattering coefficient*, or the conventional measure of the strength of radar signals reflected by a distributed scatterer, usually expressed in dB. It is a *normalised dimensionless number*, comparing the strength observed to that expected from an area of one square meter. Sigma nought is defined with respect to the nominally horizontal plane, and in general has a significant variation with **incidence angle**, **wavelength**, and **polarisation**, as well as with **properties of the scattering surface itself**.\n* **Beta nought** - 可生成包含雷达亮度系数的数据集（雷达亮度系数是天线发射功率与接收功率之比）。它与倾斜范围有关，且无维度。\n* **Gamma** - 通常在校准天线时使用。因为每个范围像元与卫星的距离均相等，所以近距范围和远距范围的亮度均相等，这有助于确定输出数据集中的天线方向图。\n* **None** - 不做校正\n\n\n\n# Reference\n\n* [Sentinel-1 Radiometric Calibration—ArcMap | Documentation (arcgis.com)](https://desktop.arcgis.com/en/arcmap/latest/manage-data/raster-and-images/sentinel-1-radiometric-calibration.htm)\n\n* [Urban objects detection from C-band synthetic aperture radar (SAR) satellite images through simulating filter properties | Scientific Reports (nature.com)](https://www.nature.com/articles/s41598-021-85121-9)\n\n* [✨✨✨User Guides - Sentinel-1 SAR - Definitions - Sentinel Online - Sentinel Online (esa.int)](https://sentinel.esa.int/web/sentinel/user-guides/sentinel-1-sar/definitions)\n\n","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/synthetic_aperture_radar_imaging/SAR_Explained":{"title":"Synthetic Aperture Radar (SAR) Explained","content":"\n# Radar Basic Concepts\n\n## Down Looking vs. Side Looking\n\n![Pasted image 20230320150424](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230320150424.png)\n\nDown Looking不能区分距离一样的a，b点，一般只用于monitoring of air and naval traffic\n\n## Simplified explanation of Radar working \u0026 What is SAR\nThe radar consists fundamentally of *a transmitter*, *a receiver*, *an antenna* and *an electronic system* to process and record the data.\n\nThe transmitter generates successive short bursts or pulses of microwave at regular intervals which are focused by the antenna into a beam. Radar beam illuminates the surface **obliquely** at a right angle to the motion of the platform. The antenna receives a portion of the transmitted energy reflected or it's known as backscattered from various objects within the illuminated beam by  measuring this time delay between the transmission of a pulse and the reception of the backscattered echo from different  targets. Their distance from the radar and therefore their location can be determined as the sensor platform *moves forward* recording and processing of the backscattered signals builds up a 2-dimensional image of the surface.\n\n\n\u003e [!important] \n\u003e Important\u003cbr\u003e\n\u003e The along track **resolution** is determined by the beam width which is *inversely proportional to the antenna length*, also known as the **aperture**, which means that longer antenna or a longer aperture will produce a narrow beam and a finer resolution. \u003cbr\u003e\n\u003e Long antenna $\\leftrightarrow$ Small beam $\\leftrightarrow$ Long aperture $\\leftrightarrow$ Better image resolution\n\n\n\n### Why SAR\n介于实际情况下的物理空间中，雷达天线的大小是限的，可以通过雷达的移动去模拟长天线情况下的雷达，也就是活得更大的aperture，这项被叫做SAR。目的是在于使用*comparatively small physical antennas*去获得*high resolution images*\n\n--- \n\n![660](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230320163240.png)\n\n* Radar can measure *amplitude* and *phase*\n* Radar can only measure part of echoes.\n* The strength of the reflected echo is the backscattering coefficient ([sigma nought](synthetic_aperture_radar_imaging/Radiometric_Calibration.md)）and is expressed in [decibels(dB)](signal_processing/what_is_dB.md)\n\n## Radar Resolution\n\n### Detail geometry\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230330153450.png)\n\u003cfont size=1\u003e**Fig** *Geometry of a side-looking real aperture radar. (SLAR)*\u003c/font\u003e\n\nside-looking的雷达被分为two types —— real aperture radar(*SLAR or SLR*, SL for side-looking)和synthetic aperture radar(SAR)\n\n如上图所示，雷达发出的pulse被[antenna聚焦](synthetic_aperture_radar_imaging/Antenna.md)在一个narrow的area里，然后scatter后在不同和的时间再被receiver接收\n\n### Resolution\n\n当我们谈SAR的分辨率时，我们要知道有四种operating modes对于SAR而言。\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230418103211.png)\n\n* Stripmap SAR\n* Spotlight SAR\n* Circular SAR\n* Scan SAR\n\n其中Stripmap SAR, Spotlight SAR,  Circular SAR这三种最为常用\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230414105501.png)\n\nStripmap SAR是将antenna固定在platform，以straight line方式移动并连续接发pulse，它的优势是可以cover large area。\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230414105703.png)\n\nSpotlight SAR天线不断移动以照射同一区域，它的特点是high-resolution image，因为它从不同的角度收集同一区域的data\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230414110025.png)\n\nCircular SAR通过circular trajectory窥探同一片area，它跟spotlight SAR很像，区别在于Spotlight mode里antenna是不动的，只有平台在移动，而在circular mode里，antenna也在移动，来收集$360^\\circ$信息，circular SAR的分辨率计算时，认为反射是$360^\\circ$各向同性反射，所以是理论分辨率。\n\n我在UWB radar探测烧伤的技术中将采用Spotlight SAR\n\n\n#### Range Resolution \u0026 Azimuth Resolution\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230414111329.png)\n\n这是一张可以快速check概念的图\n\nTable. *Range and azimuth resolution*\n|               | Range Resolution                             | Azimuth Resolution                                     |\n| ------------- | -------------------------------------------- | ------------------------------------------------------ |\n| Stripmap SAR  | $\\Delta_r = \\frac{c\\pi}{2\\omega_0}$          | $\\Delta_a = \\frac{D_y}{2}$                             |\n| Spotlight SAR | $\\Delta_r = \\frac{c\\pi}{2\\omega_0}$          | $\\Delta_a=\\frac{r_n\\lambda_c}{4L \\cos \\theta_n(0)}$    |\n| Circular SAR  | $\\Delta_r = \\frac{\\pi}{\\rho_max - \\rho_min}$ | $\\Delta_a=\\frac{\\pi}{2k_c \\cos{\\theta_z}\\sin{\\phi_0}}$ |\n\n* $\\omega_0$ radar signal half-bandwidth in radians\n* $D_y$ the diameter of the radar in azimuth domain\n* $r_n$ the target radical distance from the center of aperture\n* $\\lambda_c = \\frac{2c\\pi}{\\omega_c}$ the wavelength at carrier fast-time frequency\n* $\\omega_c$ the central frequency\n* $L$ half-size of the aperture\n* $\\theta_n(0)$ the aspect angle of the $n$th target when radar is at (0, 0)\n* $\\rho_{max}$ and $\\rho_{min}$ the maximum and minimum polar radius in spatial frequency domain for the support of a target at the center of the spotlight area\n* $k_c$ the wavenumber at carrier frequency\n* $\\theta_z$ the average depression angle of the target area\n* $\\phi_0$ the polar angle in spatial frequency domain \n\n## Radar Image Format\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230509140819.png)\n\n## Radar Key Parameters\n* Wave Length\n* Polarization\n* Incidence Angle\n\n### Wave Length\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230330153007.png)\n\n雷达数据的空间分辨率与传感器波长与传感器天线长度之比直接相关。 对于给定的波长，天线越长，空间分辨率越高。 对于以大约 5 cm 波长运行的太空卫星（C 波段雷达），为了获得 10 m 的空间分辨率，您需要一个大约 4,250 m 长的雷达天线。 （超过 47 个足球场！）\n\n\n\n# Reference\n\n* [Theory of Synthetic Aperture Radar (uzh.ch)](https://www.geo.uzh.ch/~fpaul/sar_theory.html)\n* ***Sentinel-1** is a famous SAR, you can find almost every definitions* of SAR in this page:\n[User Guides - Sentinel-1 SAR - Definitions - Sentinel Online - Sentinel Online (esa.int)](https://sentinel.esa.int/web/sentinel/user-guides/sentinel-1-sar/definitions)\n* [SAR(Synthetic Aperture Radar)基础(一) - 知乎 (评论区说这个有错)](https://zhuanlan.zhihu.com/p/98053986)\n* [A Review of Synthetic-Aperture Radar Image Formation Algorithms and Implementations: A Computational Perspective]([Remote Sensing | Free Full-Text | A Review of Synthetic-Aperture Radar Image Formation Algorithms and Implementations: A Computational Perspective (mdpi.com)](https://www.mdpi.com/2072-4292/14/5/1258))\n","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/synthetic_aperture_radar_imaging/SAR_Imaging_Algorithm":{"title":"SAR Imaging Algorithm review in 2022","content":"\n\n# Overview\n\n* Backprojection\n* Matched-filter\n* Polar format\n* Range-Doppler\n* Chirp scaling algorithms\n\n\n# What is SAR processing?\n\n\n## Born approximation\n\nSAR 处理算法将场景建模为一组离散的点目标，其分散的 EM 场不会相互影响。\n\n* 无多次反弹\n* 目标处的电场仅来自入射波，而不来自周围的散射体\n* 目标模型是线性的，因为点目标 P1 和点目标 P2 的散射响应被建模为点目标 P1 本身的响应 + 点目标 P2 本身的响应\n* 可以应用**叠加原理(principle of superposition)**\n\n\u003c!--SAR 处理是对图像中每个像素应用匹配滤波器，其中匹配滤波器系数是来自单个孤立点目标的响应\n\n* SAR processing is a correlation filter between a single isolated point target response and the raw data\n* SAR processing is an inner product between our model of a single isolated point target and the raw data\n--\u003e\n\n## 信号建模\n\n\nSAR成像是对一个区域的散射特性进行成像，这个区域的地形一般比较复杂，区域内不同位置处的物体散射特性各不相同，最后SAR接收的是探测区域内所有物体后向散射信号的叠加，整个探测区域散射的回波信号模型非常复杂。直接构造整个探测区域的散射信号模型十分困难，也没有必要。为了简化信号模型，信号模型的建立运用了两个离散化：\n\n* 探测区域的离散化；\n* 平台飞行的离散化;\n\n### 探测区域离散化\n\n**将探测区域认为是若干散射点的集合**，由此对区域回波信号模型的建立转化为对这些散射点回波信号模型的建立。这样只需构建任意散射点的回波信号模型即可表示整个探测区域的回波信号模型。该离散化的准则是：离散间隔内的物体散射特性基本不变。\n\n### 平台飞行离散化\n\n**将平台的飞行过程认为是一个“走停”模式**，即在一个脉冲时间（脉冲重复周期）内，平台是“停”（静止）的状态，平台发射一个脉冲信号，并在该位置处接收该脉冲照射目标的回波信号；在下一个脉冲时间内，平台“走”（瞬移）到另一个位置（按照原来匀速运动应该走到的位置处），并在下一个位置重复上一个脉冲时间内平台的操作。该离散化的准则是：电磁波传播速度远大于平台速度，即SAR一次发射、接收过程中，雷达的位置基本不变。\n\n--- \n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230419111635.png)\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230418165114.png)\n\n如图，针对红点目标，SAR从A点开始照射到P点最接近目标，直到B点离开红点离开。\n\n假设平台$t$时刻飞行到红点位置，雷达发射脉冲信号$s(\\tau)$，此时接收的回波信号信息为：\n\n\n$$\nr(\\tau,t) = \\sigma(R_0, A_0) s(\\tau - \\frac{2R(t)}{c})\\omega_a(\\frac{t - t_p}{T_{syn}})\n$$\n\n\n* $\\sigma(R_0, A_0)$表示$(R_0, A_0)$处目标的散射面积\n* $T_{syn}$表示合成孔径的时长\n* $\\omega_a(\\cdot)$理想情况可以认为是矩性窗，实际上是由实孔径天线的方向图构成；考虑到信号往返，$\\omega_a(\\cdot)$函数为天线方向图的平方。\n\n同时，有：\n\n$$\nR(t) = \\sqrt{R_0^2 + v^2(t-t_p)^2}\n$$\n\n从图示不难发现，与红点目标相比，距离向等距的黑点目标多普勒历程一致，只是对应的方位向时延不一样，反映在表达式上，即距离目标最短的时刻$t_p$不同。对接收的回波信号进一步化简可得：\n\n$$\nr(\\tau, t) = \\{s(\\tau)w_a(\\frac{t}{T_{syn}})\\} \\bigotimes h(\\tau, t)\n$$\n\n$$\nh(\\tau, t) = \\sigma(R_0, A_0)\\delta(\\tau-\\frac{2R(t)}{c}, t-t_p)\n$$\n\n将SAR（信号发射到接收的过程）看成一个系统，$h(\\tau, t)$为对应的系统函数，系统函数包含目标位置处散射面积$\\sigma(R_0, A_0)$和重建函数$\\delta(\\tau-\\frac{2R(t)}{c}, t-t_p)$。\n\nSAR成像问题等效为：根据发射信号从回波信号中反卷积出系统函数$h(\\tau, t)$\n\n同时，系统函数$h(\\tau, t)$中的重建函数$\\delta(\\tau-\\frac{2R(t)}{c}, t-t_p)$的快时间维存在慢时间维的耦合项，为此SAR成像算法一个关键的步骤是去除这个耦合项，称为距离徙动校正，将重建函数矫正为$\\delta(\\tau-\\frac{2R}{c}, t-t_p)$，这时候可以分别对快时间维$\\tau$和慢时间维$t$的信号做脉冲压缩处理，得到SAR图像\n\n上述是SAR回波信号模型的建立过程以及对所得回波信号模型的简单分析，在建立信号模型的过程中，运用了雷达领域经常用到了两个概念，*慢时间是对脉冲间时间的标记*，即慢时间表示发射的是第几个脉冲信号，所以慢时间本身是离散的，离散间隔为脉冲重复周期；*快时间是对脉冲内时间的标记*，即快时间显示的是任意一个脉冲内的时刻，相比慢时间，快时间是连续的，需要通过信号的采样来离散。\n\n\n### 信号模型的四域表示\n\n\n\n# Range-Doppler Algorithm (RDA)\n\nRange-Doppler Algorithm是SAR成像的第一个算法，在1970年代被developed出来，用来生成stripmap的SAR。Range-Doppler Algorithm利用block-processing处理，在距离和方位角中使用频域运算。\n\n步骤如下：\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230417110036.png)\n\n## Range Compression\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230418102226.png)\n\n距离参考函数是一系列复数，表示天线发射的原始啁啾信号(original [chirp](synthetic_aperture_radar_imaging/Chirp.md))。\n\n天线发射的原始线性调频信号（**linear-frequency chirp**）是一种线性调频连续波信号，它的频率随着时间线性变化，形成一种锯齿状的波形。这种信号可以用数学公式表示为：\n\n$$ s(t) = \\cos\\left(2\\pi\\left(f_c t + \\frac{B}{T} t^2\\right)\\right) $$\n\n其中，$f_c$是信号的中心频率，$B$是信号的带宽，$T$是信号的持续时间。这种信号可以用一个本地振荡器（LO）来生成，然后通过一个功率放大器来放大，并从天线发射出去。\n\n## Azimuth Compression\n\n![](synthetic_aperture_radar_imaging/attachments/Pasted%20image%2020230418162216.png)\n\n\n\n# Reference\n\n* [A Review of Synthetic-Aperture Radar Image Formation Algorithms and Implementations: A Computational Perspective]([Remote Sensing | Free Full-Text | A Review of Synthetic-Aperture Radar Image Formation Algorithms and Implementations: A Computational Perspective (mdpi.com)](https://www.mdpi.com/2072-4292/14/5/1258))\n* [Range Doppler Algorithm - University of Kansas](https://people.eecs.ku.edu/~callen58/826/826_SAR_Processing_Algorithms_Overview-F15.pptx)\n* [距离多普勒算法（RDA）-SAR成像算法系列（三）-【杨（_\u003e \u003c_)】的博客-CSDN博客 🚧这个人的博客讲的真不错🚧](https://blog.csdn.net/yjh_2019/article/details/123772486?spm=1001.2014.3001.5502)\n\n","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/synthetic_aperture_radar_imaging/SAR_MOC":{"title":"Synthetic Aperture Radar (SAR) Imaging - MOC","content":"\n\n# Antenna\n\n* [Antenna](synthetic_aperture_radar_imaging/Antenna.md)\n\n# SAR\n\n* [[synthetic_aperture_radar_imaging/SAR_Explained|SAR Explained]]\n* [SAR Imaging Algorithm review in 2022](synthetic_aperture_radar_imaging/SAR_Imaging_Algorithm.md)","lastmodified":"2023-09-18T02:45:48.389256244Z","tags":null},"/tmp_script/interview_31_ans":{"title":"2023三位一体 - 生医工面试真题","content":"\n\n[三一面试.pdf](tmp_script/attachments/三一面试.pdf)","lastmodified":"2023-09-18T02:45:48.433256752Z","tags":null},"/tmp_script/prefix_sum":{"title":"Prefix Sum","content":"\n假设我们有一个长度为n的数组arr，**前缀和**数组prefixSum的定义如下：\n\n```python\nprefixSum[0] = arr[0] \nprefixSum[1] = arr[0] + arr[1] \nprefixSum[2] = arr[0] + arr[1] + arr[2] \n... \nprefixSum[i] = arr[0] + arr[1] + ... + arr[i]\n```\n","lastmodified":"2023-09-18T02:45:48.433256752Z","tags":null},"/toolkit/git/git_MOC":{"title":"Git MOC","content":"\n* [GitHub Actions](toolkit/git/github_actions.md)","lastmodified":"2023-09-18T02:45:48.433256752Z","tags":null},"/toolkit/git/github_actions":{"title":"GitHub Actions","content":"\n# Reference\n\n* [GitHub Actions by Example](https://www.actionsbyexample.com/)","lastmodified":"2023-09-18T02:45:48.433256752Z","tags":null},"/warehouse/dampers_keeping_a_door_from_slamming-shut":{"title":"Dampers keeping a door from slamming shut","content":"\n![](warehouse/attachments/Pasted%20image%2020230404150745.png)","lastmodified":"2023-09-18T02:45:48.437256798Z","tags":null}}